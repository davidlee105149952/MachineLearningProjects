{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digits Recognition Using Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data comes from the \"MNIST\" data set, you can download it from [Kaggle](https://www.kaggle.com/c/digit-recognizer/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1, Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"train.csv\", delimiter=',', skip_header=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "[8. 3. 6. 8. 1. 4. 7. 8. 9. 9.]\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "print(data.shape)\n",
    "print(data[:10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 42000)\n",
      "(1, 42000)\n"
     ]
    }
   ],
   "source": [
    "features = data[:, 1:].T\n",
    "labels = data[:, 0]\n",
    "labels = np.reshape(labels, (1, -1))\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = features.shape[1]\n",
    "nx = features.shape[0]\n",
    "ny = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_Y(labels):\n",
    "    Y = np.zeros((ny, m))\n",
    "    for i in range(m):\n",
    "        Y[int(labels[0, i]), i] = 1\n",
    "    return Y\n",
    "\n",
    "def Y_to_labels(Y):\n",
    "    labels = np.argmax(Y, axis=0).astype(float)\n",
    "    labels = np.reshape(labels, (1, -1))\n",
    "    return labels\n",
    "\n",
    "def X_to_images(X):\n",
    "    images = [np.reshape(X[:, i], (28, 28)) for i in range(X.shape[1])]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 42000)\n",
      "8.0\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = features / 255\n",
    "Y = labels_to_Y(labels)\n",
    "images = X_to_images(X)\n",
    "print(Y.shape)\n",
    "plt.imshow(images[0])\n",
    "print(labels[0, 0])\n",
    "print(Y[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 37800)\n",
      "(10, 37800)\n",
      "(784, 4200)\n",
      "(10, 4200)\n",
      "(1, 37800)\n"
     ]
    }
   ],
   "source": [
    "test_ratio = 1 - train_ratio\n",
    "train_m = int(m * train_ratio)\n",
    "test_m = m - train_m\n",
    "train_X = X[:, :train_m]\n",
    "test_X = X[:, train_m:]\n",
    "train_Y = Y[:, :train_m]\n",
    "test_Y = Y[:, train_m:]\n",
    "train_labels = Y_to_labels(train_Y)\n",
    "test_labels = Y_to_labels(test_Y)\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2, Design Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    a = 1/ (1 + np.exp(-z))\n",
    "    return a\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    p1 = np.exp(-z)\n",
    "    a = p1/((1+p1)**2)\n",
    "    return a\n",
    "\n",
    "def relu(z):\n",
    "    a = np.maximum(z, 0.01*z)\n",
    "    return a\n",
    "\n",
    "def relu_prime(z):\n",
    "    a = np.where(z > 0, 1, 0.01)\n",
    "    return a\n",
    "\n",
    "def tanh(z):\n",
    "    p1 = np.exp(z)\n",
    "    p2 = np.exp(-z)\n",
    "    a = (p1-p2)/(p1+p2)\n",
    "    return a\n",
    "    \n",
    "def tanh_prime(z):\n",
    "    p1 = tanh(z)\n",
    "    a = 1-p1**2\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_for__config_multi_layer_classifier_without_regularization(Y_hat, Y):\n",
    "    delta = 1e-10\n",
    "    l = -((Y+delta)*np.log(Y_hat+delta) + (1-Y+delta)*np.log(1-Y_hat+delta))\n",
    "    return l\n",
    "\n",
    "def forward_propagation_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    X = model['X']\n",
    "    Y = model['Y']\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f = model['f']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    costs = model['costs']\n",
    "    loss_function = model['loss_function']\n",
    "    A[0] = X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    costs.append(np.sum(loss_function(A[L], Y)))\n",
    "\n",
    "def back_propagation_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f_prime = model['f_prime']\n",
    "    m = model['m']\n",
    "    W = model['W']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    Y = model['Y']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    dA = model['dA']\n",
    "    dZ = model['dZ']\n",
    "    dZ[L] = A[L] - Y\n",
    "    dW[L] = np.matmul(dZ[L], A[L-1].T)/m\n",
    "    db[L] = np.sum(dZ[L], axis=1, keepdims=True)/m\n",
    "    for i in reversed(range(1, L)):\n",
    "        dZ[i] = np.matmul(W[i+1].T, dZ[i+1]) * f_prime[i](Z[i])\n",
    "        dW[i] = np.matmul(dZ[i], A[i-1].T)/m\n",
    "        db[i] = np.sum(dZ[i], axis=1, keepdims=True)/m\n",
    "        \n",
    "def update_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    L = model['L']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    learning_rate = model['learning_rate']\n",
    "    for i in range(1, L+1):\n",
    "        W[i] -= learning_rate*dW[i]\n",
    "        b[i] -= learning_rate*db[i]\n",
    "    \n",
    "    \n",
    "def predict_for_config_multi_layer_classifier_without_regularization(model, test_X):\n",
    "    L = model['L']\n",
    "    A = model['A']\n",
    "    Z = model['Z']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    f = model['f']\n",
    "    A[0] = test_X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    return A[L]\n",
    "    \n",
    "    \n",
    "def config_multi_layer_classifier_without_regularization(X, Y, neuron_of_hidden_layer, learning_rate):\n",
    "    n_input_layer = X.shape[0]\n",
    "    n_output_layer = Y.shape[0]\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = [n_input_layer] + neuron_of_hidden_layer + [n_output_layer]\n",
    "    L = len(n)-1\n",
    "    f = [tanh if i != 0 else None for i in range(L)]\n",
    "    f.append(sigmoid)\n",
    "    f_prime = [tanh_prime if i != 0 else None for i in range(L)]\n",
    "    f_prime.append(sigmoid_prime)\n",
    "    W = [np.random.normal(0, 1, (n[i], n[i-1])) if i != 0 else None for i in range(L+1)]\n",
    "    b = [np.random.normal(0, 1, (n[i], 1)) if i != 0 else None for i in range(L+1)]\n",
    "    Z = [None for i in range(L+1)]\n",
    "    A = [None for i in range(L+1)]\n",
    "    dW = [None for i in range(L+1)]\n",
    "    db = [None for i in range(L+1)]\n",
    "    dA = [None for i in range(L+1)]\n",
    "    dZ = [None for i in range(L+1)]\n",
    "    costs = []\n",
    "    \n",
    "    model = dict()\n",
    "    model['X'] = X\n",
    "    model['Y'] = Y\n",
    "    model['m'] = m\n",
    "    model['n'] = n\n",
    "    model['L'] = L\n",
    "    model['f'] = f\n",
    "    model['f_prime'] = f_prime\n",
    "    model['W'] = W\n",
    "    model['b'] = b\n",
    "    model['Z'] = Z\n",
    "    model['A'] = A\n",
    "    model['dW'] = dW\n",
    "    model['db'] = db\n",
    "    model['dA'] = dA\n",
    "    model['dZ'] = dZ\n",
    "    model['loss_function'] = loss_function_for__config_multi_layer_classifier_without_regularization\n",
    "    model['costs'] = costs\n",
    "    model['learning_rate'] = learning_rate\n",
    "    model['forwardprop'] = forward_propagation_for_config_multi_layer_classifier_without_regularization\n",
    "    model['backprop'] = back_propagation_for_config_multi_layer_classifier_without_regularization\n",
    "    model['update'] = update_for_config_multi_layer_classifier_without_regularization\n",
    "    model['predict'] = predict_for_config_multi_layer_classifier_without_regularization\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_for__config_multi_layer_classifier_with_dropout_regularization(Y_hat, Y):\n",
    "    delta = 1e-10\n",
    "    l = -((Y+delta)*np.log(Y_hat+delta) + (1-Y+delta)*np.log(1-Y_hat+delta))\n",
    "    return l\n",
    "\n",
    "def forward_propagation_for_config_multi_layer_classifier_with_dropout_regularization(model):\n",
    "    X = model['X']\n",
    "    Y = model['Y']\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f = model['f']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    keep_prob = model['keep_prob']\n",
    "    costs = model['costs']\n",
    "    loss_function = model['loss_function']\n",
    "    A[0] = X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "        dropout = np.random.uniform(0.0, 1.0, A[i].shape)\n",
    "        dropout = dropout < keep_prob[i]\n",
    "        A[i] = A[i] * dropout\n",
    "        A[i] = A[i] / keep_prob[i]\n",
    "    costs.append(np.sum(loss_function(A[L], Y)))\n",
    "\n",
    "def back_propagation_for_config_multi_layer_classifier_with_dropout_regularization(model):\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f_prime = model['f_prime']\n",
    "    m = model['m']\n",
    "    W = model['W']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    Y = model['Y']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    dA = model['dA']\n",
    "    dZ = model['dZ']\n",
    "    dZ[L] = A[L] - Y\n",
    "    dW[L] = np.matmul(dZ[L], A[L-1].T)/m\n",
    "    db[L] = np.sum(dZ[L], axis=1, keepdims=True)/m\n",
    "    for i in reversed(range(1, L)):\n",
    "        dZ[i] = np.matmul(W[i+1].T, dZ[i+1]) * f_prime[i](Z[i])\n",
    "        dW[i] = np.matmul(dZ[i], A[i-1].T)/m\n",
    "        db[i] = np.sum(dZ[i], axis=1, keepdims=True)/m\n",
    "        \n",
    "def update_for_config_multi_layer_classifier_with_dropout_regularization(model):\n",
    "    L = model['L']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    learning_rate = model['learning_rate']\n",
    "    for i in range(1, L+1):\n",
    "        W[i] -= learning_rate*dW[i]\n",
    "        b[i] -= learning_rate*db[i]\n",
    "    \n",
    "    \n",
    "def predict_for_config_multi_layer_classifier_with_dropout_regularization(model, test_X):\n",
    "    L = model['L']\n",
    "    A = model['A']\n",
    "    Z = model['Z']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    f = model['f']\n",
    "    A[0] = test_X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    return A[L]\n",
    "    \n",
    "    \n",
    "def config_multi_layer_classifier_with_dropout_regularization(X, Y, neuron_of_hidden_layer, learning_rate):\n",
    "    n_input_layer = X.shape[0]\n",
    "    n_output_layer = Y.shape[0]\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = [n_input_layer] + neuron_of_hidden_layer + [n_output_layer]\n",
    "    L = len(n)-1\n",
    "    keep_prob = [1] + [0.8 for i in range(L-1)] + [1]\n",
    "    f = [tanh if i != 0 else None for i in range(L)]\n",
    "    f.append(sigmoid)\n",
    "    f_prime = [tanh_prime if i != 0 else None for i in range(L)]\n",
    "    f_prime.append(sigmoid_prime)\n",
    "    W = [np.random.normal(0, 1, (n[i], n[i-1])) if i != 0 else None for i in range(L+1)]\n",
    "    b = [np.random.normal(0, 1, (n[i], 1)) if i != 0 else None for i in range(L+1)]\n",
    "    Z = [None for i in range(L+1)]\n",
    "    A = [None for i in range(L+1)]\n",
    "    dW = [None for i in range(L+1)]\n",
    "    db = [None for i in range(L+1)]\n",
    "    dA = [None for i in range(L+1)]\n",
    "    dZ = [None for i in range(L+1)]\n",
    "    costs = []\n",
    "    \n",
    "    model = dict()\n",
    "    model['X'] = X\n",
    "    model['Y'] = Y\n",
    "    model['m'] = m\n",
    "    model['n'] = n\n",
    "    model['L'] = L\n",
    "    model['f'] = f\n",
    "    model['f_prime'] = f_prime\n",
    "    model['W'] = W\n",
    "    model['b'] = b\n",
    "    model['Z'] = Z\n",
    "    model['A'] = A\n",
    "    model['dW'] = dW\n",
    "    model['db'] = db\n",
    "    model['dA'] = dA\n",
    "    model['dZ'] = dZ\n",
    "    model['loss_function'] = loss_function_for__config_multi_layer_classifier_with_dropout_regularization\n",
    "    model['costs'] = costs\n",
    "    model['learning_rate'] = learning_rate\n",
    "    model['keep_prob'] = keep_prob\n",
    "    model['forwardprop'] = forward_propagation_for_config_multi_layer_classifier_with_dropout_regularization\n",
    "    model['backprop'] = back_propagation_for_config_multi_layer_classifier_with_dropout_regularization\n",
    "    model['update'] = update_for_config_multi_layer_classifier_with_dropout_regularization\n",
    "    model['predict'] = predict_for_config_multi_layer_classifier_with_dropout_regularization\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(iteration_times, model):\n",
    "    forwardprop = model['forwardprop']\n",
    "    backprop = model['backprop']\n",
    "    update = model['update']\n",
    "    costs = model['costs']\n",
    "    for i in range(iteration_times):\n",
    "        forwardprop(model)\n",
    "        backprop(model)\n",
    "        update(model)\n",
    "        print(\"iteration %d, current loss: %f\" % (i, costs[len(costs)-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = config_multi_layer_classifier_without_regularization(train_X, train_Y, [16, 16, 16, 16, 16, 16], 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, current loss: 91544.496673\n",
      "iteration 1, current loss: 91296.704484\n",
      "iteration 2, current loss: 91020.932822\n",
      "iteration 3, current loss: 90927.957583\n",
      "iteration 4, current loss: 90729.650064\n",
      "iteration 5, current loss: 90722.282231\n",
      "iteration 6, current loss: 90556.873750\n",
      "iteration 7, current loss: 90505.421145\n",
      "iteration 8, current loss: 90403.065171\n",
      "iteration 9, current loss: 90230.575289\n",
      "iteration 10, current loss: 90010.241845\n",
      "iteration 11, current loss: 89906.795778\n",
      "iteration 12, current loss: 89636.080388\n",
      "iteration 13, current loss: 89490.276575\n",
      "iteration 14, current loss: 89320.723401\n",
      "iteration 15, current loss: 89190.481045\n",
      "iteration 16, current loss: 88934.252129\n",
      "iteration 17, current loss: 88803.649181\n",
      "iteration 18, current loss: 88765.730666\n",
      "iteration 19, current loss: 88590.698533\n",
      "iteration 20, current loss: 88674.496893\n",
      "iteration 21, current loss: 88459.792393\n",
      "iteration 22, current loss: 88487.413214\n",
      "iteration 23, current loss: 88102.730481\n",
      "iteration 24, current loss: 87979.380157\n",
      "iteration 25, current loss: 87637.423518\n",
      "iteration 26, current loss: 87500.138766\n",
      "iteration 27, current loss: 87260.906989\n",
      "iteration 28, current loss: 87171.448573\n",
      "iteration 29, current loss: 87025.766028\n",
      "iteration 30, current loss: 86974.736383\n",
      "iteration 31, current loss: 86833.866897\n",
      "iteration 32, current loss: 86606.337300\n",
      "iteration 33, current loss: 86448.765435\n",
      "iteration 34, current loss: 86304.264011\n",
      "iteration 35, current loss: 86174.811495\n",
      "iteration 36, current loss: 86314.006949\n",
      "iteration 37, current loss: 86106.400136\n",
      "iteration 38, current loss: 86175.669713\n",
      "iteration 39, current loss: 85730.182880\n",
      "iteration 40, current loss: 85587.178506\n",
      "iteration 41, current loss: 85222.044066\n",
      "iteration 42, current loss: 85059.632645\n",
      "iteration 43, current loss: 84830.230277\n",
      "iteration 44, current loss: 84670.941460\n",
      "iteration 45, current loss: 84620.609271\n",
      "iteration 46, current loss: 84493.833231\n",
      "iteration 47, current loss: 84607.702628\n",
      "iteration 48, current loss: 84761.654722\n",
      "iteration 49, current loss: 84757.005087\n",
      "iteration 50, current loss: 84732.154407\n",
      "iteration 51, current loss: 84669.649937\n",
      "iteration 52, current loss: 84200.441005\n",
      "iteration 53, current loss: 84102.521893\n",
      "iteration 54, current loss: 83758.400080\n",
      "iteration 55, current loss: 83477.667986\n",
      "iteration 56, current loss: 83160.147618\n",
      "iteration 57, current loss: 82875.649694\n",
      "iteration 58, current loss: 82715.686258\n",
      "iteration 59, current loss: 82514.569395\n",
      "iteration 60, current loss: 82769.523157\n",
      "iteration 61, current loss: 82401.049976\n",
      "iteration 62, current loss: 82629.081397\n",
      "iteration 63, current loss: 82516.873518\n",
      "iteration 64, current loss: 82650.346110\n",
      "iteration 65, current loss: 82379.761535\n",
      "iteration 66, current loss: 82344.695812\n",
      "iteration 67, current loss: 82035.385191\n",
      "iteration 68, current loss: 81503.411583\n",
      "iteration 69, current loss: 81128.260519\n",
      "iteration 70, current loss: 80891.902571\n",
      "iteration 71, current loss: 80587.285339\n",
      "iteration 72, current loss: 80438.269879\n",
      "iteration 73, current loss: 80292.262877\n",
      "iteration 74, current loss: 80228.068865\n",
      "iteration 75, current loss: 80230.200432\n",
      "iteration 76, current loss: 80111.829879\n",
      "iteration 77, current loss: 80084.288694\n",
      "iteration 78, current loss: 79981.934568\n",
      "iteration 79, current loss: 80072.037884\n",
      "iteration 80, current loss: 79838.055518\n",
      "iteration 81, current loss: 79933.270888\n",
      "iteration 82, current loss: 79663.700864\n",
      "iteration 83, current loss: 79811.766905\n",
      "iteration 84, current loss: 79683.915936\n",
      "iteration 85, current loss: 79590.803230\n",
      "iteration 86, current loss: 79145.694729\n",
      "iteration 87, current loss: 78794.267271\n",
      "iteration 88, current loss: 78487.627456\n",
      "iteration 89, current loss: 78288.361803\n",
      "iteration 90, current loss: 78220.581973\n",
      "iteration 91, current loss: 78108.367477\n",
      "iteration 92, current loss: 78187.144907\n",
      "iteration 93, current loss: 77936.095405\n",
      "iteration 94, current loss: 77903.975174\n",
      "iteration 95, current loss: 77847.064100\n",
      "iteration 96, current loss: 77798.714256\n",
      "iteration 97, current loss: 77570.987192\n",
      "iteration 98, current loss: 77433.489515\n",
      "iteration 99, current loss: 77297.094199\n",
      "iteration 100, current loss: 77227.359942\n",
      "iteration 101, current loss: 77033.318280\n",
      "iteration 102, current loss: 76977.787288\n",
      "iteration 103, current loss: 76913.159005\n",
      "iteration 104, current loss: 76892.734181\n",
      "iteration 105, current loss: 76884.748616\n",
      "iteration 106, current loss: 76802.761239\n",
      "iteration 107, current loss: 76687.776581\n",
      "iteration 108, current loss: 76793.345881\n",
      "iteration 109, current loss: 76727.000298\n",
      "iteration 110, current loss: 76948.631374\n",
      "iteration 111, current loss: 76785.739071\n",
      "iteration 112, current loss: 76603.101202\n",
      "iteration 113, current loss: 76574.600150\n",
      "iteration 114, current loss: 75921.001306\n",
      "iteration 115, current loss: 75783.899688\n",
      "iteration 116, current loss: 75515.514047\n",
      "iteration 117, current loss: 75380.446628\n",
      "iteration 118, current loss: 75192.702461\n",
      "iteration 119, current loss: 74907.242926\n",
      "iteration 120, current loss: 74760.291904\n",
      "iteration 121, current loss: 74504.844802\n",
      "iteration 122, current loss: 74347.415574\n",
      "iteration 123, current loss: 74192.000598\n",
      "iteration 124, current loss: 74111.224452\n",
      "iteration 125, current loss: 74240.333221\n",
      "iteration 126, current loss: 74014.323489\n",
      "iteration 127, current loss: 73923.316098\n",
      "iteration 128, current loss: 73785.437213\n",
      "iteration 129, current loss: 73860.033596\n",
      "iteration 130, current loss: 73631.484470\n",
      "iteration 131, current loss: 73705.451015\n",
      "iteration 132, current loss: 73603.818502\n",
      "iteration 133, current loss: 73628.898173\n",
      "iteration 134, current loss: 73544.876908\n",
      "iteration 135, current loss: 73457.071407\n",
      "iteration 136, current loss: 73200.745098\n",
      "iteration 137, current loss: 73181.228797\n",
      "iteration 138, current loss: 72983.676308\n",
      "iteration 139, current loss: 72835.833992\n",
      "iteration 140, current loss: 72661.271545\n",
      "iteration 141, current loss: 72574.261510\n",
      "iteration 142, current loss: 72501.420922\n",
      "iteration 143, current loss: 72465.934927\n",
      "iteration 144, current loss: 72261.944502\n",
      "iteration 145, current loss: 72115.568912\n",
      "iteration 146, current loss: 72094.505208\n",
      "iteration 147, current loss: 72077.285311\n",
      "iteration 148, current loss: 72042.403848\n",
      "iteration 149, current loss: 72100.395443\n",
      "iteration 150, current loss: 71988.685113\n",
      "iteration 151, current loss: 72032.074693\n",
      "iteration 152, current loss: 71946.576183\n",
      "iteration 153, current loss: 72072.729205\n",
      "iteration 154, current loss: 72034.678635\n",
      "iteration 155, current loss: 72593.225770\n",
      "iteration 156, current loss: 72596.436332\n",
      "iteration 157, current loss: 74553.234262\n",
      "iteration 158, current loss: 76363.322032\n",
      "iteration 159, current loss: 80003.972425\n",
      "iteration 160, current loss: 109716.868485\n",
      "iteration 161, current loss: 93789.397252\n",
      "iteration 162, current loss: 88713.332609\n",
      "iteration 163, current loss: 88844.746693\n",
      "iteration 164, current loss: 80332.031399\n",
      "iteration 165, current loss: 76759.801882\n",
      "iteration 166, current loss: 75037.274980\n",
      "iteration 167, current loss: 73527.544546\n",
      "iteration 168, current loss: 72566.751189\n",
      "iteration 169, current loss: 72021.261348\n",
      "iteration 170, current loss: 71632.340653\n",
      "iteration 171, current loss: 71153.834175\n",
      "iteration 172, current loss: 70886.340573\n",
      "iteration 173, current loss: 70561.708235\n",
      "iteration 174, current loss: 70416.465664\n",
      "iteration 175, current loss: 70225.727475\n",
      "iteration 176, current loss: 70084.754614\n",
      "iteration 177, current loss: 70037.853682\n",
      "iteration 178, current loss: 69918.072257\n",
      "iteration 179, current loss: 69848.297193\n",
      "iteration 180, current loss: 69635.100334\n",
      "iteration 181, current loss: 69434.045909\n",
      "iteration 182, current loss: 69306.463196\n",
      "iteration 183, current loss: 69288.036757\n",
      "iteration 184, current loss: 69157.907871\n",
      "iteration 185, current loss: 69143.027875\n",
      "iteration 186, current loss: 68849.527617\n",
      "iteration 187, current loss: 68750.945592\n",
      "iteration 188, current loss: 68616.007853\n",
      "iteration 189, current loss: 68490.383380\n",
      "iteration 190, current loss: 68379.179761\n",
      "iteration 191, current loss: 68341.715383\n",
      "iteration 192, current loss: 68230.249245\n",
      "iteration 193, current loss: 68292.835161\n",
      "iteration 194, current loss: 68126.421392\n",
      "iteration 195, current loss: 68102.638149\n",
      "iteration 196, current loss: 67884.649629\n",
      "iteration 197, current loss: 67739.862400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 198, current loss: 67504.542776\n",
      "iteration 199, current loss: 67459.271671\n",
      "iteration 200, current loss: 67436.816846\n",
      "iteration 201, current loss: 67578.126630\n",
      "iteration 202, current loss: 67270.989927\n",
      "iteration 203, current loss: 67254.252171\n",
      "iteration 204, current loss: 67065.760867\n",
      "iteration 205, current loss: 67007.462162\n",
      "iteration 206, current loss: 66894.520982\n",
      "iteration 207, current loss: 66934.462982\n",
      "iteration 208, current loss: 66713.570107\n",
      "iteration 209, current loss: 66687.558798\n",
      "iteration 210, current loss: 66582.184313\n",
      "iteration 211, current loss: 66598.302103\n",
      "iteration 212, current loss: 66396.270675\n",
      "iteration 213, current loss: 66289.663287\n",
      "iteration 214, current loss: 66277.057165\n",
      "iteration 215, current loss: 66352.946381\n",
      "iteration 216, current loss: 66114.559007\n",
      "iteration 217, current loss: 66079.318033\n",
      "iteration 218, current loss: 66075.514828\n",
      "iteration 219, current loss: 66210.456770\n",
      "iteration 220, current loss: 66012.964579\n",
      "iteration 221, current loss: 66251.579499\n",
      "iteration 222, current loss: 65832.875306\n",
      "iteration 223, current loss: 65970.986717\n",
      "iteration 224, current loss: 65609.443600\n",
      "iteration 225, current loss: 65399.598311\n",
      "iteration 226, current loss: 65243.378389\n",
      "iteration 227, current loss: 65358.190610\n",
      "iteration 228, current loss: 65166.852531\n",
      "iteration 229, current loss: 65275.058929\n",
      "iteration 230, current loss: 65115.118683\n",
      "iteration 231, current loss: 65286.188722\n",
      "iteration 232, current loss: 65049.691301\n",
      "iteration 233, current loss: 65092.491377\n",
      "iteration 234, current loss: 64865.014363\n",
      "iteration 235, current loss: 64837.900268\n",
      "iteration 236, current loss: 64628.698704\n",
      "iteration 237, current loss: 64588.031747\n",
      "iteration 238, current loss: 64520.279110\n",
      "iteration 239, current loss: 64575.875689\n",
      "iteration 240, current loss: 64280.920021\n",
      "iteration 241, current loss: 64254.022619\n",
      "iteration 242, current loss: 64231.881590\n",
      "iteration 243, current loss: 64163.736760\n",
      "iteration 244, current loss: 64122.735116\n",
      "iteration 245, current loss: 64211.231613\n",
      "iteration 246, current loss: 64149.077636\n",
      "iteration 247, current loss: 64411.544959\n",
      "iteration 248, current loss: 64048.759829\n",
      "iteration 249, current loss: 64056.798344\n",
      "iteration 250, current loss: 63697.364217\n",
      "iteration 251, current loss: 63540.396893\n",
      "iteration 252, current loss: 63483.557214\n",
      "iteration 253, current loss: 63483.450254\n",
      "iteration 254, current loss: 63501.943304\n",
      "iteration 255, current loss: 63371.126479\n",
      "iteration 256, current loss: 63259.165353\n",
      "iteration 257, current loss: 63251.924723\n",
      "iteration 258, current loss: 63210.504642\n",
      "iteration 259, current loss: 63137.004456\n",
      "iteration 260, current loss: 63151.350672\n",
      "iteration 261, current loss: 63099.826989\n",
      "iteration 262, current loss: 62828.351170\n",
      "iteration 263, current loss: 62829.397051\n",
      "iteration 264, current loss: 62954.649344\n",
      "iteration 265, current loss: 63050.898044\n",
      "iteration 266, current loss: 63007.696771\n",
      "iteration 267, current loss: 63020.969705\n",
      "iteration 268, current loss: 62713.398107\n",
      "iteration 269, current loss: 62786.056911\n",
      "iteration 270, current loss: 62523.947792\n",
      "iteration 271, current loss: 62479.644060\n",
      "iteration 272, current loss: 62405.731289\n",
      "iteration 273, current loss: 62236.363768\n",
      "iteration 274, current loss: 62081.826137\n",
      "iteration 275, current loss: 61873.347380\n",
      "iteration 276, current loss: 61823.431057\n",
      "iteration 277, current loss: 61709.702461\n",
      "iteration 278, current loss: 61629.390970\n",
      "iteration 279, current loss: 61824.037051\n",
      "iteration 280, current loss: 61657.000977\n",
      "iteration 281, current loss: 61962.259340\n",
      "iteration 282, current loss: 62095.179444\n",
      "iteration 283, current loss: 62491.713690\n",
      "iteration 284, current loss: 62294.824058\n",
      "iteration 285, current loss: 62485.905402\n",
      "iteration 286, current loss: 61703.163190\n",
      "iteration 287, current loss: 61627.553001\n",
      "iteration 288, current loss: 61305.951715\n",
      "iteration 289, current loss: 61255.095495\n",
      "iteration 290, current loss: 61305.794615\n",
      "iteration 291, current loss: 61264.312781\n",
      "iteration 292, current loss: 61140.521802\n",
      "iteration 293, current loss: 61125.233187\n",
      "iteration 294, current loss: 61061.088037\n",
      "iteration 295, current loss: 61022.585731\n",
      "iteration 296, current loss: 60921.520466\n",
      "iteration 297, current loss: 60972.152825\n",
      "iteration 298, current loss: 60932.994994\n",
      "iteration 299, current loss: 61108.592696\n",
      "iteration 300, current loss: 60993.903065\n",
      "iteration 301, current loss: 61165.283035\n",
      "iteration 302, current loss: 60810.848282\n",
      "iteration 303, current loss: 60960.658732\n",
      "iteration 304, current loss: 60672.478489\n",
      "iteration 305, current loss: 60636.138925\n",
      "iteration 306, current loss: 60441.776242\n",
      "iteration 307, current loss: 60432.487051\n",
      "iteration 308, current loss: 60334.254923\n",
      "iteration 309, current loss: 60763.434978\n",
      "iteration 310, current loss: 60437.071376\n",
      "iteration 311, current loss: 60842.782904\n",
      "iteration 312, current loss: 60864.365559\n",
      "iteration 313, current loss: 61105.347950\n",
      "iteration 314, current loss: 60503.187481\n",
      "iteration 315, current loss: 60639.330134\n",
      "iteration 316, current loss: 60266.186039\n",
      "iteration 317, current loss: 60214.543806\n",
      "iteration 318, current loss: 59722.038144\n",
      "iteration 319, current loss: 59629.008490\n",
      "iteration 320, current loss: 59493.509286\n",
      "iteration 321, current loss: 59733.638581\n",
      "iteration 322, current loss: 59737.844404\n",
      "iteration 323, current loss: 60216.935523\n",
      "iteration 324, current loss: 59746.859750\n",
      "iteration 325, current loss: 59907.656938\n",
      "iteration 326, current loss: 59442.121309\n",
      "iteration 327, current loss: 59356.354105\n",
      "iteration 328, current loss: 59215.386077\n",
      "iteration 329, current loss: 59365.077707\n",
      "iteration 330, current loss: 59111.763592\n",
      "iteration 331, current loss: 59684.550086\n",
      "iteration 332, current loss: 59037.020592\n",
      "iteration 333, current loss: 59354.818674\n",
      "iteration 334, current loss: 59250.423346\n",
      "iteration 335, current loss: 60129.708144\n",
      "iteration 336, current loss: 60262.124068\n",
      "iteration 337, current loss: 60626.363076\n",
      "iteration 338, current loss: 60394.886014\n",
      "iteration 339, current loss: 60340.167041\n",
      "iteration 340, current loss: 59889.918134\n",
      "iteration 341, current loss: 59058.585588\n",
      "iteration 342, current loss: 58693.169391\n",
      "iteration 343, current loss: 58455.061397\n",
      "iteration 344, current loss: 58392.722590\n",
      "iteration 345, current loss: 58605.976787\n",
      "iteration 346, current loss: 58635.605276\n",
      "iteration 347, current loss: 58862.405539\n",
      "iteration 348, current loss: 58643.467460\n",
      "iteration 349, current loss: 58880.563078\n",
      "iteration 350, current loss: 58575.598488\n",
      "iteration 351, current loss: 58779.500450\n",
      "iteration 352, current loss: 58501.196460\n",
      "iteration 353, current loss: 58788.083940\n",
      "iteration 354, current loss: 58449.577270\n",
      "iteration 355, current loss: 58433.922657\n",
      "iteration 356, current loss: 58258.275071\n",
      "iteration 357, current loss: 58276.024640\n",
      "iteration 358, current loss: 58048.886092\n",
      "iteration 359, current loss: 57793.660207\n",
      "iteration 360, current loss: 57678.825464\n",
      "iteration 361, current loss: 57747.316261\n",
      "iteration 362, current loss: 57941.480554\n",
      "iteration 363, current loss: 58338.964655\n",
      "iteration 364, current loss: 58222.371575\n",
      "iteration 365, current loss: 59002.801972\n",
      "iteration 366, current loss: 58511.542032\n",
      "iteration 367, current loss: 58787.449226\n",
      "iteration 368, current loss: 57948.278366\n",
      "iteration 369, current loss: 58123.683736\n",
      "iteration 370, current loss: 57972.139142\n",
      "iteration 371, current loss: 58033.781002\n",
      "iteration 372, current loss: 57795.247334\n",
      "iteration 373, current loss: 57924.061143\n",
      "iteration 374, current loss: 57548.101967\n",
      "iteration 375, current loss: 57636.837706\n",
      "iteration 376, current loss: 57482.258002\n",
      "iteration 377, current loss: 57428.055110\n",
      "iteration 378, current loss: 57213.994507\n",
      "iteration 379, current loss: 57415.601997\n",
      "iteration 380, current loss: 57070.009123\n",
      "iteration 381, current loss: 57278.755907\n",
      "iteration 382, current loss: 56962.743619\n",
      "iteration 383, current loss: 57159.085429\n",
      "iteration 384, current loss: 57014.247431\n",
      "iteration 385, current loss: 57296.819380\n",
      "iteration 386, current loss: 57688.858508\n",
      "iteration 387, current loss: 58141.424409\n",
      "iteration 388, current loss: 57622.007940\n",
      "iteration 389, current loss: 57504.173335\n",
      "iteration 390, current loss: 57016.149548\n",
      "iteration 391, current loss: 56740.386836\n",
      "iteration 392, current loss: 56536.535465\n",
      "iteration 393, current loss: 56399.249502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 394, current loss: 56356.066583\n",
      "iteration 395, current loss: 56460.789681\n",
      "iteration 396, current loss: 56351.277225\n",
      "iteration 397, current loss: 56566.799271\n",
      "iteration 398, current loss: 56475.300603\n",
      "iteration 399, current loss: 56749.853830\n",
      "iteration 400, current loss: 56489.592448\n",
      "iteration 401, current loss: 56597.292337\n",
      "iteration 402, current loss: 56399.157180\n",
      "iteration 403, current loss: 56244.891858\n",
      "iteration 404, current loss: 56281.507447\n",
      "iteration 405, current loss: 56106.954492\n",
      "iteration 406, current loss: 56164.174086\n",
      "iteration 407, current loss: 56294.915663\n",
      "iteration 408, current loss: 56361.365719\n",
      "iteration 409, current loss: 56462.596985\n",
      "iteration 410, current loss: 56407.774297\n",
      "iteration 411, current loss: 55908.802712\n",
      "iteration 412, current loss: 55790.317320\n",
      "iteration 413, current loss: 55671.489751\n",
      "iteration 414, current loss: 56016.963448\n",
      "iteration 415, current loss: 56067.000404\n",
      "iteration 416, current loss: 56519.474306\n",
      "iteration 417, current loss: 56490.549282\n",
      "iteration 418, current loss: 56778.949480\n",
      "iteration 419, current loss: 56544.185416\n",
      "iteration 420, current loss: 56231.060666\n",
      "iteration 421, current loss: 55906.329002\n",
      "iteration 422, current loss: 55688.432203\n",
      "iteration 423, current loss: 55618.435884\n",
      "iteration 424, current loss: 55670.246520\n",
      "iteration 425, current loss: 55467.199410\n",
      "iteration 426, current loss: 55685.494227\n",
      "iteration 427, current loss: 55797.664860\n",
      "iteration 428, current loss: 56184.113926\n",
      "iteration 429, current loss: 55838.021007\n",
      "iteration 430, current loss: 55792.151038\n",
      "iteration 431, current loss: 55176.816636\n",
      "iteration 432, current loss: 55378.706649\n",
      "iteration 433, current loss: 54991.716634\n",
      "iteration 434, current loss: 55161.271815\n",
      "iteration 435, current loss: 54939.983635\n",
      "iteration 436, current loss: 55056.912206\n",
      "iteration 437, current loss: 55002.820881\n",
      "iteration 438, current loss: 55369.137979\n",
      "iteration 439, current loss: 55080.569696\n",
      "iteration 440, current loss: 55348.441717\n",
      "iteration 441, current loss: 55281.021300\n",
      "iteration 442, current loss: 55534.770458\n",
      "iteration 443, current loss: 55623.772873\n",
      "iteration 444, current loss: 55664.392090\n",
      "iteration 445, current loss: 55540.419127\n",
      "iteration 446, current loss: 55692.104341\n",
      "iteration 447, current loss: 55092.947178\n",
      "iteration 448, current loss: 55180.220239\n",
      "iteration 449, current loss: 54730.896732\n",
      "iteration 450, current loss: 54704.466434\n",
      "iteration 451, current loss: 54362.288088\n",
      "iteration 452, current loss: 54225.259881\n",
      "iteration 453, current loss: 54115.897431\n",
      "iteration 454, current loss: 54117.434042\n",
      "iteration 455, current loss: 54083.538190\n",
      "iteration 456, current loss: 54180.582513\n",
      "iteration 457, current loss: 54276.379508\n",
      "iteration 458, current loss: 54231.979890\n",
      "iteration 459, current loss: 54450.800297\n",
      "iteration 460, current loss: 54270.362071\n",
      "iteration 461, current loss: 54624.917889\n",
      "iteration 462, current loss: 54641.364477\n",
      "iteration 463, current loss: 54626.448834\n",
      "iteration 464, current loss: 54428.655844\n",
      "iteration 465, current loss: 54070.397252\n",
      "iteration 466, current loss: 53864.013184\n",
      "iteration 467, current loss: 53790.189007\n",
      "iteration 468, current loss: 53763.565741\n",
      "iteration 469, current loss: 53973.870390\n",
      "iteration 470, current loss: 53989.397280\n",
      "iteration 471, current loss: 54236.079959\n",
      "iteration 472, current loss: 53892.358106\n",
      "iteration 473, current loss: 53913.118772\n",
      "iteration 474, current loss: 53797.979743\n",
      "iteration 475, current loss: 53884.553063\n",
      "iteration 476, current loss: 53843.848020\n",
      "iteration 477, current loss: 53669.791765\n",
      "iteration 478, current loss: 53549.088607\n",
      "iteration 479, current loss: 53508.997988\n",
      "iteration 480, current loss: 53396.532088\n",
      "iteration 481, current loss: 53597.985222\n",
      "iteration 482, current loss: 53250.247907\n",
      "iteration 483, current loss: 53034.651045\n",
      "iteration 484, current loss: 53126.991044\n",
      "iteration 485, current loss: 53265.150156\n",
      "iteration 486, current loss: 52935.161250\n",
      "iteration 487, current loss: 53044.811915\n",
      "iteration 488, current loss: 52844.501617\n",
      "iteration 489, current loss: 52909.705628\n",
      "iteration 490, current loss: 53006.673939\n",
      "iteration 491, current loss: 53295.388675\n",
      "iteration 492, current loss: 53449.124256\n",
      "iteration 493, current loss: 53635.951162\n",
      "iteration 494, current loss: 53696.850095\n",
      "iteration 495, current loss: 54510.841831\n",
      "iteration 496, current loss: 55210.894824\n",
      "iteration 497, current loss: 54448.115352\n",
      "iteration 498, current loss: 54044.211214\n",
      "iteration 499, current loss: 53157.691245\n",
      "iteration 500, current loss: 52928.038733\n",
      "iteration 501, current loss: 52671.560390\n",
      "iteration 502, current loss: 52799.857441\n",
      "iteration 503, current loss: 52883.448543\n",
      "iteration 504, current loss: 53168.984456\n",
      "iteration 505, current loss: 53097.920083\n",
      "iteration 506, current loss: 53367.154546\n",
      "iteration 507, current loss: 53052.006711\n",
      "iteration 508, current loss: 52779.076841\n",
      "iteration 509, current loss: 52471.489150\n",
      "iteration 510, current loss: 52304.235806\n",
      "iteration 511, current loss: 52289.172962\n",
      "iteration 512, current loss: 52322.411854\n",
      "iteration 513, current loss: 52366.505949\n",
      "iteration 514, current loss: 52448.092708\n",
      "iteration 515, current loss: 52696.910126\n",
      "iteration 516, current loss: 52554.668729\n",
      "iteration 517, current loss: 52695.589392\n",
      "iteration 518, current loss: 53023.797325\n",
      "iteration 519, current loss: 52979.336363\n",
      "iteration 520, current loss: 52671.644369\n",
      "iteration 521, current loss: 52016.814988\n",
      "iteration 522, current loss: 51888.025064\n",
      "iteration 523, current loss: 51617.919514\n",
      "iteration 524, current loss: 51617.534066\n",
      "iteration 525, current loss: 51613.539021\n",
      "iteration 526, current loss: 51936.147710\n",
      "iteration 527, current loss: 51793.610966\n",
      "iteration 528, current loss: 52007.664019\n",
      "iteration 529, current loss: 52237.984129\n",
      "iteration 530, current loss: 52659.000562\n",
      "iteration 531, current loss: 52562.097019\n",
      "iteration 532, current loss: 52161.734286\n",
      "iteration 533, current loss: 51990.794846\n",
      "iteration 534, current loss: 51660.859837\n",
      "iteration 535, current loss: 51386.436828\n",
      "iteration 536, current loss: 51389.179807\n",
      "iteration 537, current loss: 51718.142267\n",
      "iteration 538, current loss: 51937.753140\n",
      "iteration 539, current loss: 52260.005254\n",
      "iteration 540, current loss: 52474.841584\n",
      "iteration 541, current loss: 51946.725765\n",
      "iteration 542, current loss: 51786.615535\n",
      "iteration 543, current loss: 51265.762374\n",
      "iteration 544, current loss: 51019.485301\n",
      "iteration 545, current loss: 50714.178744\n",
      "iteration 546, current loss: 50817.068882\n",
      "iteration 547, current loss: 50814.852331\n",
      "iteration 548, current loss: 51163.048336\n",
      "iteration 549, current loss: 51272.303907\n",
      "iteration 550, current loss: 51678.840596\n",
      "iteration 551, current loss: 52235.757796\n",
      "iteration 552, current loss: 52232.184660\n",
      "iteration 553, current loss: 51964.414957\n",
      "iteration 554, current loss: 51246.335327\n",
      "iteration 555, current loss: 51007.076104\n",
      "iteration 556, current loss: 50923.949510\n",
      "iteration 557, current loss: 50620.503209\n",
      "iteration 558, current loss: 50467.781617\n",
      "iteration 559, current loss: 50517.790261\n",
      "iteration 560, current loss: 50638.216225\n",
      "iteration 561, current loss: 50782.481732\n",
      "iteration 562, current loss: 50920.639905\n",
      "iteration 563, current loss: 50981.551344\n",
      "iteration 564, current loss: 50910.398069\n",
      "iteration 565, current loss: 51057.514311\n",
      "iteration 566, current loss: 50583.775978\n",
      "iteration 567, current loss: 50654.687079\n",
      "iteration 568, current loss: 50217.715938\n",
      "iteration 569, current loss: 50391.400516\n",
      "iteration 570, current loss: 50695.221920\n",
      "iteration 571, current loss: 51068.602655\n",
      "iteration 572, current loss: 51051.346699\n",
      "iteration 573, current loss: 51429.076734\n",
      "iteration 574, current loss: 51870.964252\n",
      "iteration 575, current loss: 51977.036563\n",
      "iteration 576, current loss: 50955.259341\n",
      "iteration 577, current loss: 50461.019652\n",
      "iteration 578, current loss: 50194.267408\n",
      "iteration 579, current loss: 49867.804247\n",
      "iteration 580, current loss: 49741.785605\n",
      "iteration 581, current loss: 49715.482385\n",
      "iteration 582, current loss: 49604.177018\n",
      "iteration 583, current loss: 49786.381130\n",
      "iteration 584, current loss: 49740.800185\n",
      "iteration 585, current loss: 49913.487718\n",
      "iteration 586, current loss: 49785.766535\n",
      "iteration 587, current loss: 49907.972627\n",
      "iteration 588, current loss: 49905.890537\n",
      "iteration 589, current loss: 49608.946378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 590, current loss: 49573.471163\n",
      "iteration 591, current loss: 49751.610370\n",
      "iteration 592, current loss: 49515.357538\n",
      "iteration 593, current loss: 49462.800068\n",
      "iteration 594, current loss: 49419.268389\n",
      "iteration 595, current loss: 49597.476862\n",
      "iteration 596, current loss: 50211.559479\n",
      "iteration 597, current loss: 50628.487432\n",
      "iteration 598, current loss: 51264.261767\n",
      "iteration 599, current loss: 51426.926177\n",
      "iteration 600, current loss: 50602.034021\n",
      "iteration 601, current loss: 50198.839786\n",
      "iteration 602, current loss: 49609.657919\n",
      "iteration 603, current loss: 49355.964079\n",
      "iteration 604, current loss: 49286.925354\n",
      "iteration 605, current loss: 49769.529112\n",
      "iteration 606, current loss: 50123.260502\n",
      "iteration 607, current loss: 50473.618769\n",
      "iteration 608, current loss: 50450.957637\n",
      "iteration 609, current loss: 50184.757908\n",
      "iteration 610, current loss: 49543.572720\n",
      "iteration 611, current loss: 49444.154841\n",
      "iteration 612, current loss: 49587.812896\n",
      "iteration 613, current loss: 49593.413833\n",
      "iteration 614, current loss: 49676.294795\n",
      "iteration 615, current loss: 49208.453092\n",
      "iteration 616, current loss: 49325.158770\n",
      "iteration 617, current loss: 48731.545406\n",
      "iteration 618, current loss: 48650.870773\n",
      "iteration 619, current loss: 48481.987102\n",
      "iteration 620, current loss: 48398.642325\n",
      "iteration 621, current loss: 48370.511228\n",
      "iteration 622, current loss: 48531.318215\n",
      "iteration 623, current loss: 48676.782492\n",
      "iteration 624, current loss: 49082.544511\n",
      "iteration 625, current loss: 49712.354292\n",
      "iteration 626, current loss: 49699.992339\n",
      "iteration 627, current loss: 49240.452582\n",
      "iteration 628, current loss: 48969.777305\n",
      "iteration 629, current loss: 48926.721368\n",
      "iteration 630, current loss: 48707.159300\n",
      "iteration 631, current loss: 49103.312762\n",
      "iteration 632, current loss: 48991.639668\n",
      "iteration 633, current loss: 49141.545933\n",
      "iteration 634, current loss: 48824.726005\n",
      "iteration 635, current loss: 48907.928910\n",
      "iteration 636, current loss: 48922.638629\n",
      "iteration 637, current loss: 48964.706304\n",
      "iteration 638, current loss: 48923.046486\n",
      "iteration 639, current loss: 49137.482544\n",
      "iteration 640, current loss: 48800.445077\n",
      "iteration 641, current loss: 48834.404913\n",
      "iteration 642, current loss: 48860.469380\n",
      "iteration 643, current loss: 48862.393007\n",
      "iteration 644, current loss: 48674.803290\n",
      "iteration 645, current loss: 48315.942064\n",
      "iteration 646, current loss: 48095.128752\n",
      "iteration 647, current loss: 47858.608158\n",
      "iteration 648, current loss: 47840.680531\n",
      "iteration 649, current loss: 47742.811453\n",
      "iteration 650, current loss: 47751.808879\n",
      "iteration 651, current loss: 47717.317005\n",
      "iteration 652, current loss: 48040.805502\n",
      "iteration 653, current loss: 48688.681371\n",
      "iteration 654, current loss: 49573.988178\n",
      "iteration 655, current loss: 48992.931556\n",
      "iteration 656, current loss: 49087.224447\n",
      "iteration 657, current loss: 48149.028538\n",
      "iteration 658, current loss: 47714.297252\n",
      "iteration 659, current loss: 47646.177633\n",
      "iteration 660, current loss: 47584.492774\n",
      "iteration 661, current loss: 47274.677004\n",
      "iteration 662, current loss: 47235.812523\n",
      "iteration 663, current loss: 47458.263306\n",
      "iteration 664, current loss: 48094.015511\n",
      "iteration 665, current loss: 48980.332187\n",
      "iteration 666, current loss: 49646.095267\n",
      "iteration 667, current loss: 49285.928728\n",
      "iteration 668, current loss: 48475.065891\n",
      "iteration 669, current loss: 48199.340784\n",
      "iteration 670, current loss: 48144.829981\n",
      "iteration 671, current loss: 47972.493466\n",
      "iteration 672, current loss: 47457.717327\n",
      "iteration 673, current loss: 47220.223526\n",
      "iteration 674, current loss: 47378.638459\n",
      "iteration 675, current loss: 47743.327992\n",
      "iteration 676, current loss: 48012.415745\n",
      "iteration 677, current loss: 48574.679135\n",
      "iteration 678, current loss: 48415.768496\n",
      "iteration 679, current loss: 48236.754659\n",
      "iteration 680, current loss: 48017.997878\n",
      "iteration 681, current loss: 47611.531224\n",
      "iteration 682, current loss: 47253.976891\n",
      "iteration 683, current loss: 46905.245261\n",
      "iteration 684, current loss: 46674.226161\n",
      "iteration 685, current loss: 46579.044782\n",
      "iteration 686, current loss: 46573.294321\n",
      "iteration 687, current loss: 46642.716996\n",
      "iteration 688, current loss: 46761.567614\n",
      "iteration 689, current loss: 46908.559636\n",
      "iteration 690, current loss: 47140.022985\n",
      "iteration 691, current loss: 47718.361181\n",
      "iteration 692, current loss: 47882.910463\n",
      "iteration 693, current loss: 48249.345509\n",
      "iteration 694, current loss: 47241.070617\n",
      "iteration 695, current loss: 46872.872417\n",
      "iteration 696, current loss: 46549.964419\n",
      "iteration 697, current loss: 46242.360796\n",
      "iteration 698, current loss: 46306.194845\n",
      "iteration 699, current loss: 46346.844765\n",
      "iteration 700, current loss: 46840.002981\n",
      "iteration 701, current loss: 47570.777208\n",
      "iteration 702, current loss: 47717.451363\n",
      "iteration 703, current loss: 47743.528477\n",
      "iteration 704, current loss: 47459.231029\n",
      "iteration 705, current loss: 46900.394133\n",
      "iteration 706, current loss: 46902.610088\n",
      "iteration 707, current loss: 46718.293222\n",
      "iteration 708, current loss: 46388.599177\n",
      "iteration 709, current loss: 46241.481565\n",
      "iteration 710, current loss: 46128.542504\n",
      "iteration 711, current loss: 46179.757764\n",
      "iteration 712, current loss: 46692.876843\n",
      "iteration 713, current loss: 46804.267436\n",
      "iteration 714, current loss: 47026.825074\n",
      "iteration 715, current loss: 46817.390829\n",
      "iteration 716, current loss: 47124.630657\n",
      "iteration 717, current loss: 46739.195929\n",
      "iteration 718, current loss: 46635.846055\n",
      "iteration 719, current loss: 46211.930439\n",
      "iteration 720, current loss: 46741.255725\n",
      "iteration 721, current loss: 46723.488252\n",
      "iteration 722, current loss: 47350.871490\n",
      "iteration 723, current loss: 46985.837979\n",
      "iteration 724, current loss: 46483.913156\n",
      "iteration 725, current loss: 46283.454914\n",
      "iteration 726, current loss: 46175.621258\n",
      "iteration 727, current loss: 46324.274721\n",
      "iteration 728, current loss: 46861.334343\n",
      "iteration 729, current loss: 46969.340813\n",
      "iteration 730, current loss: 47112.027321\n",
      "iteration 731, current loss: 47855.023315\n",
      "iteration 732, current loss: 47551.695675\n",
      "iteration 733, current loss: 48247.326571\n",
      "iteration 734, current loss: 47156.061500\n",
      "iteration 735, current loss: 46195.640500\n",
      "iteration 736, current loss: 45839.330205\n",
      "iteration 737, current loss: 45573.146709\n",
      "iteration 738, current loss: 45636.858832\n",
      "iteration 739, current loss: 45516.263895\n",
      "iteration 740, current loss: 45694.342445\n",
      "iteration 741, current loss: 45726.295580\n",
      "iteration 742, current loss: 46213.204693\n",
      "iteration 743, current loss: 46404.329921\n",
      "iteration 744, current loss: 47218.118975\n",
      "iteration 745, current loss: 46686.591562\n",
      "iteration 746, current loss: 46505.077761\n",
      "iteration 747, current loss: 45801.606409\n",
      "iteration 748, current loss: 45588.926810\n",
      "iteration 749, current loss: 45546.430672\n",
      "iteration 750, current loss: 45822.534167\n",
      "iteration 751, current loss: 45948.335677\n",
      "iteration 752, current loss: 46279.519540\n",
      "iteration 753, current loss: 46059.870409\n",
      "iteration 754, current loss: 46406.473824\n",
      "iteration 755, current loss: 46404.115350\n",
      "iteration 756, current loss: 46226.065861\n",
      "iteration 757, current loss: 45730.393851\n",
      "iteration 758, current loss: 45598.363744\n",
      "iteration 759, current loss: 45345.547630\n",
      "iteration 760, current loss: 45237.868793\n",
      "iteration 761, current loss: 45214.949640\n",
      "iteration 762, current loss: 45579.395658\n",
      "iteration 763, current loss: 45823.328655\n",
      "iteration 764, current loss: 46373.314674\n",
      "iteration 765, current loss: 46963.455596\n",
      "iteration 766, current loss: 46492.575143\n",
      "iteration 767, current loss: 45813.621834\n",
      "iteration 768, current loss: 45621.005639\n",
      "iteration 769, current loss: 45508.184622\n",
      "iteration 770, current loss: 45492.484248\n",
      "iteration 771, current loss: 45019.902599\n",
      "iteration 772, current loss: 44843.505577\n",
      "iteration 773, current loss: 44733.930861\n",
      "iteration 774, current loss: 44991.712080\n",
      "iteration 775, current loss: 44980.293850\n",
      "iteration 776, current loss: 45536.974904\n",
      "iteration 777, current loss: 45571.326184\n",
      "iteration 778, current loss: 46477.847419\n",
      "iteration 779, current loss: 45645.195231\n",
      "iteration 780, current loss: 46027.631301\n",
      "iteration 781, current loss: 45549.564430\n",
      "iteration 782, current loss: 45881.289421\n",
      "iteration 783, current loss: 45901.739471\n",
      "iteration 784, current loss: 45671.565346\n",
      "iteration 785, current loss: 45600.980316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 786, current loss: 45014.261423\n",
      "iteration 787, current loss: 44573.622969\n",
      "iteration 788, current loss: 44347.299301\n",
      "iteration 789, current loss: 44299.639755\n",
      "iteration 790, current loss: 44563.347528\n",
      "iteration 791, current loss: 44719.506663\n",
      "iteration 792, current loss: 45405.036235\n",
      "iteration 793, current loss: 45789.086765\n",
      "iteration 794, current loss: 45620.675021\n",
      "iteration 795, current loss: 45482.050260\n",
      "iteration 796, current loss: 45032.329321\n",
      "iteration 797, current loss: 45144.962735\n",
      "iteration 798, current loss: 45545.231518\n",
      "iteration 799, current loss: 46121.681185\n",
      "iteration 800, current loss: 46022.288092\n",
      "iteration 801, current loss: 45133.291637\n",
      "iteration 802, current loss: 44510.557457\n",
      "iteration 803, current loss: 44340.676345\n",
      "iteration 804, current loss: 44136.661501\n",
      "iteration 805, current loss: 44420.734099\n",
      "iteration 806, current loss: 44807.336222\n",
      "iteration 807, current loss: 45024.488433\n",
      "iteration 808, current loss: 45231.103795\n",
      "iteration 809, current loss: 44573.099271\n",
      "iteration 810, current loss: 44466.727710\n",
      "iteration 811, current loss: 44008.042198\n",
      "iteration 812, current loss: 44133.364890\n",
      "iteration 813, current loss: 44269.599464\n",
      "iteration 814, current loss: 44900.285584\n",
      "iteration 815, current loss: 44778.790460\n",
      "iteration 816, current loss: 45338.067629\n",
      "iteration 817, current loss: 44749.145342\n",
      "iteration 818, current loss: 45205.807230\n",
      "iteration 819, current loss: 45142.417938\n",
      "iteration 820, current loss: 44935.847185\n",
      "iteration 821, current loss: 45337.419122\n",
      "iteration 822, current loss: 45031.487920\n",
      "iteration 823, current loss: 45107.942714\n",
      "iteration 824, current loss: 44531.569963\n",
      "iteration 825, current loss: 44727.776408\n",
      "iteration 826, current loss: 44925.402380\n",
      "iteration 827, current loss: 44976.714158\n",
      "iteration 828, current loss: 45176.597351\n",
      "iteration 829, current loss: 44559.652374\n",
      "iteration 830, current loss: 44153.256934\n",
      "iteration 831, current loss: 43872.096435\n",
      "iteration 832, current loss: 43610.483640\n",
      "iteration 833, current loss: 43407.530427\n",
      "iteration 834, current loss: 43491.520057\n",
      "iteration 835, current loss: 43316.559807\n",
      "iteration 836, current loss: 43440.539407\n",
      "iteration 837, current loss: 44072.481992\n",
      "iteration 838, current loss: 45187.591601\n",
      "iteration 839, current loss: 45977.069594\n",
      "iteration 840, current loss: 47862.791659\n",
      "iteration 841, current loss: 46566.287316\n",
      "iteration 842, current loss: 46643.377692\n",
      "iteration 843, current loss: 44555.926735\n",
      "iteration 844, current loss: 44041.087287\n",
      "iteration 845, current loss: 43271.770160\n",
      "iteration 846, current loss: 43083.371293\n",
      "iteration 847, current loss: 43275.763655\n",
      "iteration 848, current loss: 43303.371473\n",
      "iteration 849, current loss: 43794.288506\n",
      "iteration 850, current loss: 43825.796000\n",
      "iteration 851, current loss: 43772.375150\n",
      "iteration 852, current loss: 43414.468781\n",
      "iteration 853, current loss: 43271.690007\n",
      "iteration 854, current loss: 43056.638453\n",
      "iteration 855, current loss: 43195.033716\n",
      "iteration 856, current loss: 43281.619613\n",
      "iteration 857, current loss: 43693.781612\n",
      "iteration 858, current loss: 44078.766854\n",
      "iteration 859, current loss: 45084.086126\n",
      "iteration 860, current loss: 44725.693364\n",
      "iteration 861, current loss: 44354.353542\n",
      "iteration 862, current loss: 43988.000534\n",
      "iteration 863, current loss: 43693.729293\n",
      "iteration 864, current loss: 43414.240987\n",
      "iteration 865, current loss: 43290.636322\n",
      "iteration 866, current loss: 43156.190927\n",
      "iteration 867, current loss: 43472.144607\n",
      "iteration 868, current loss: 43762.034965\n",
      "iteration 869, current loss: 44162.890547\n",
      "iteration 870, current loss: 44794.036672\n",
      "iteration 871, current loss: 44793.515578\n",
      "iteration 872, current loss: 44330.147551\n",
      "iteration 873, current loss: 43733.882879\n",
      "iteration 874, current loss: 43009.767095\n",
      "iteration 875, current loss: 42987.417395\n",
      "iteration 876, current loss: 43045.786372\n",
      "iteration 877, current loss: 43020.585508\n",
      "iteration 878, current loss: 43122.556643\n",
      "iteration 879, current loss: 43392.158351\n",
      "iteration 880, current loss: 43474.123090\n",
      "iteration 881, current loss: 43489.967433\n",
      "iteration 882, current loss: 43790.926224\n",
      "iteration 883, current loss: 43810.130101\n",
      "iteration 884, current loss: 43682.484866\n",
      "iteration 885, current loss: 42853.391583\n",
      "iteration 886, current loss: 42682.733003\n",
      "iteration 887, current loss: 42598.806030\n",
      "iteration 888, current loss: 42679.849658\n",
      "iteration 889, current loss: 42625.087585\n",
      "iteration 890, current loss: 42591.424053\n",
      "iteration 891, current loss: 42221.775771\n",
      "iteration 892, current loss: 42458.201105\n",
      "iteration 893, current loss: 42893.739942\n",
      "iteration 894, current loss: 43695.565391\n",
      "iteration 895, current loss: 43285.452406\n",
      "iteration 896, current loss: 43760.460962\n",
      "iteration 897, current loss: 44052.786373\n",
      "iteration 898, current loss: 44334.168922\n",
      "iteration 899, current loss: 44739.798580\n",
      "iteration 900, current loss: 43893.369685\n",
      "iteration 901, current loss: 43691.840395\n",
      "iteration 902, current loss: 43290.891168\n",
      "iteration 903, current loss: 42964.515369\n",
      "iteration 904, current loss: 42479.336485\n",
      "iteration 905, current loss: 42294.985571\n",
      "iteration 906, current loss: 42085.063786\n",
      "iteration 907, current loss: 42241.594931\n",
      "iteration 908, current loss: 42266.520212\n",
      "iteration 909, current loss: 42520.841551\n",
      "iteration 910, current loss: 42918.893379\n",
      "iteration 911, current loss: 43200.671581\n",
      "iteration 912, current loss: 43113.298434\n",
      "iteration 913, current loss: 43177.744281\n",
      "iteration 914, current loss: 42709.224797\n",
      "iteration 915, current loss: 43008.603403\n",
      "iteration 916, current loss: 42168.167875\n",
      "iteration 917, current loss: 42205.004993\n",
      "iteration 918, current loss: 42103.344405\n",
      "iteration 919, current loss: 42316.807238\n",
      "iteration 920, current loss: 42415.797611\n",
      "iteration 921, current loss: 42611.785065\n",
      "iteration 922, current loss: 42541.279967\n",
      "iteration 923, current loss: 42900.243929\n",
      "iteration 924, current loss: 42501.021916\n",
      "iteration 925, current loss: 42680.516828\n",
      "iteration 926, current loss: 42509.953671\n",
      "iteration 927, current loss: 42978.934317\n",
      "iteration 928, current loss: 42621.254915\n",
      "iteration 929, current loss: 42403.081596\n",
      "iteration 930, current loss: 41832.852355\n",
      "iteration 931, current loss: 41772.218119\n",
      "iteration 932, current loss: 41865.359004\n",
      "iteration 933, current loss: 42040.034875\n",
      "iteration 934, current loss: 42103.388641\n",
      "iteration 935, current loss: 43147.210953\n",
      "iteration 936, current loss: 43290.711219\n",
      "iteration 937, current loss: 44163.077640\n",
      "iteration 938, current loss: 43023.486049\n",
      "iteration 939, current loss: 43624.988518\n",
      "iteration 940, current loss: 42237.523923\n",
      "iteration 941, current loss: 41710.204096\n",
      "iteration 942, current loss: 41657.294882\n",
      "iteration 943, current loss: 42169.016054\n",
      "iteration 944, current loss: 42060.264553\n",
      "iteration 945, current loss: 43167.650326\n",
      "iteration 946, current loss: 43107.924498\n",
      "iteration 947, current loss: 42943.165277\n",
      "iteration 948, current loss: 42191.530979\n",
      "iteration 949, current loss: 42475.888369\n",
      "iteration 950, current loss: 41980.117713\n",
      "iteration 951, current loss: 41517.671289\n",
      "iteration 952, current loss: 41568.574341\n",
      "iteration 953, current loss: 41795.682808\n",
      "iteration 954, current loss: 41550.284605\n",
      "iteration 955, current loss: 41761.680344\n",
      "iteration 956, current loss: 41810.726153\n",
      "iteration 957, current loss: 42177.059670\n",
      "iteration 958, current loss: 41954.515704\n",
      "iteration 959, current loss: 41944.522874\n",
      "iteration 960, current loss: 41998.020407\n",
      "iteration 961, current loss: 42068.281547\n",
      "iteration 962, current loss: 41953.184174\n",
      "iteration 963, current loss: 42778.319677\n",
      "iteration 964, current loss: 42134.143415\n",
      "iteration 965, current loss: 41721.637805\n",
      "iteration 966, current loss: 41591.093377\n",
      "iteration 967, current loss: 41783.121703\n",
      "iteration 968, current loss: 41686.599464\n",
      "iteration 969, current loss: 42079.248717\n",
      "iteration 970, current loss: 41388.851443\n",
      "iteration 971, current loss: 41672.976254\n",
      "iteration 972, current loss: 41507.613096\n",
      "iteration 973, current loss: 41789.159485\n",
      "iteration 974, current loss: 41673.222482\n",
      "iteration 975, current loss: 42124.837210\n",
      "iteration 976, current loss: 41753.613105\n",
      "iteration 977, current loss: 41796.411333\n",
      "iteration 978, current loss: 41809.765548\n",
      "iteration 979, current loss: 41924.439140\n",
      "iteration 980, current loss: 41972.978011\n",
      "iteration 981, current loss: 41846.671520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 982, current loss: 41659.708170\n",
      "iteration 983, current loss: 41660.751486\n",
      "iteration 984, current loss: 41106.697619\n",
      "iteration 985, current loss: 41477.756675\n",
      "iteration 986, current loss: 41387.325004\n",
      "iteration 987, current loss: 42137.510235\n",
      "iteration 988, current loss: 41412.978610\n",
      "iteration 989, current loss: 41580.088438\n",
      "iteration 990, current loss: 40903.448451\n",
      "iteration 991, current loss: 41316.941407\n",
      "iteration 992, current loss: 41438.542655\n",
      "iteration 993, current loss: 42544.546731\n",
      "iteration 994, current loss: 42659.078499\n",
      "iteration 995, current loss: 42134.903563\n",
      "iteration 996, current loss: 40897.013569\n",
      "iteration 997, current loss: 40672.607628\n",
      "iteration 998, current loss: 40427.238510\n",
      "iteration 999, current loss: 40401.282521\n",
      "iteration 1000, current loss: 40217.207801\n",
      "iteration 1001, current loss: 40325.574740\n",
      "iteration 1002, current loss: 40731.312102\n",
      "iteration 1003, current loss: 41326.671801\n",
      "iteration 1004, current loss: 41995.941903\n",
      "iteration 1005, current loss: 42858.935537\n",
      "iteration 1006, current loss: 43097.209621\n",
      "iteration 1007, current loss: 43452.231049\n",
      "iteration 1008, current loss: 41897.792998\n",
      "iteration 1009, current loss: 41148.390822\n",
      "iteration 1010, current loss: 40479.022864\n",
      "iteration 1011, current loss: 40353.489015\n",
      "iteration 1012, current loss: 40118.256988\n",
      "iteration 1013, current loss: 40416.763371\n",
      "iteration 1014, current loss: 40305.568472\n",
      "iteration 1015, current loss: 40781.361007\n",
      "iteration 1016, current loss: 40879.749861\n",
      "iteration 1017, current loss: 41142.784792\n",
      "iteration 1018, current loss: 40636.606945\n",
      "iteration 1019, current loss: 41294.486405\n",
      "iteration 1020, current loss: 41212.761885\n",
      "iteration 1021, current loss: 41949.219178\n",
      "iteration 1022, current loss: 41367.644336\n",
      "iteration 1023, current loss: 40960.336991\n",
      "iteration 1024, current loss: 40575.108247\n",
      "iteration 1025, current loss: 40760.772331\n",
      "iteration 1026, current loss: 40510.416243\n",
      "iteration 1027, current loss: 40496.636941\n",
      "iteration 1028, current loss: 40449.710029\n",
      "iteration 1029, current loss: 40562.177273\n",
      "iteration 1030, current loss: 40511.008478\n",
      "iteration 1031, current loss: 40915.061034\n",
      "iteration 1032, current loss: 40903.354354\n",
      "iteration 1033, current loss: 40796.955718\n",
      "iteration 1034, current loss: 41148.930276\n",
      "iteration 1035, current loss: 41831.608171\n",
      "iteration 1036, current loss: 41895.831056\n",
      "iteration 1037, current loss: 41693.670412\n",
      "iteration 1038, current loss: 41551.726145\n",
      "iteration 1039, current loss: 40981.590925\n",
      "iteration 1040, current loss: 40206.535033\n",
      "iteration 1041, current loss: 39963.894283\n",
      "iteration 1042, current loss: 39674.394068\n",
      "iteration 1043, current loss: 40009.097229\n",
      "iteration 1044, current loss: 40246.524429\n",
      "iteration 1045, current loss: 40769.295727\n",
      "iteration 1046, current loss: 41380.516248\n",
      "iteration 1047, current loss: 42996.326664\n",
      "iteration 1048, current loss: 41908.805610\n",
      "iteration 1049, current loss: 42796.778011\n",
      "iteration 1050, current loss: 41012.287311\n",
      "iteration 1051, current loss: 40278.631771\n",
      "iteration 1052, current loss: 39715.514768\n",
      "iteration 1053, current loss: 39373.764043\n",
      "iteration 1054, current loss: 39263.363307\n",
      "iteration 1055, current loss: 39154.798327\n",
      "iteration 1056, current loss: 39171.865448\n",
      "iteration 1057, current loss: 39133.670327\n",
      "iteration 1058, current loss: 39315.166381\n",
      "iteration 1059, current loss: 39311.299545\n",
      "iteration 1060, current loss: 39769.719674\n",
      "iteration 1061, current loss: 40104.090572\n",
      "iteration 1062, current loss: 40316.812576\n",
      "iteration 1063, current loss: 40239.581733\n",
      "iteration 1064, current loss: 40625.903440\n",
      "iteration 1065, current loss: 39951.692169\n",
      "iteration 1066, current loss: 39842.940499\n",
      "iteration 1067, current loss: 39755.431525\n",
      "iteration 1068, current loss: 40176.300572\n",
      "iteration 1069, current loss: 40309.641808\n",
      "iteration 1070, current loss: 40256.134923\n",
      "iteration 1071, current loss: 40963.972073\n",
      "iteration 1072, current loss: 40996.045497\n",
      "iteration 1073, current loss: 42578.041415\n",
      "iteration 1074, current loss: 40495.505345\n",
      "iteration 1075, current loss: 40380.634866\n",
      "iteration 1076, current loss: 39573.694887\n",
      "iteration 1077, current loss: 39885.531954\n",
      "iteration 1078, current loss: 39981.907247\n",
      "iteration 1079, current loss: 40307.624381\n",
      "iteration 1080, current loss: 40225.113177\n",
      "iteration 1081, current loss: 40050.514756\n",
      "iteration 1082, current loss: 40110.367926\n",
      "iteration 1083, current loss: 40362.860820\n",
      "iteration 1084, current loss: 39537.752096\n",
      "iteration 1085, current loss: 39249.358060\n",
      "iteration 1086, current loss: 38944.354725\n",
      "iteration 1087, current loss: 38923.120315\n",
      "iteration 1088, current loss: 39148.318897\n",
      "iteration 1089, current loss: 39738.869156\n",
      "iteration 1090, current loss: 40426.595433\n",
      "iteration 1091, current loss: 41412.236610\n",
      "iteration 1092, current loss: 41274.116642\n",
      "iteration 1093, current loss: 40722.058435\n",
      "iteration 1094, current loss: 39888.281248\n",
      "iteration 1095, current loss: 39710.051956\n",
      "iteration 1096, current loss: 39592.645170\n",
      "iteration 1097, current loss: 39501.904281\n",
      "iteration 1098, current loss: 39600.761217\n",
      "iteration 1099, current loss: 40096.476135\n",
      "iteration 1100, current loss: 39887.096352\n",
      "iteration 1101, current loss: 39546.386583\n",
      "iteration 1102, current loss: 39582.723310\n",
      "iteration 1103, current loss: 39205.503682\n",
      "iteration 1104, current loss: 39437.902799\n",
      "iteration 1105, current loss: 40029.008142\n",
      "iteration 1106, current loss: 40557.738494\n",
      "iteration 1107, current loss: 42046.042403\n",
      "iteration 1108, current loss: 41180.907510\n",
      "iteration 1109, current loss: 41569.711843\n",
      "iteration 1110, current loss: 40461.851890\n",
      "iteration 1111, current loss: 40120.493159\n",
      "iteration 1112, current loss: 39273.633693\n",
      "iteration 1113, current loss: 39143.975835\n",
      "iteration 1114, current loss: 38938.033260\n",
      "iteration 1115, current loss: 38791.868805\n",
      "iteration 1116, current loss: 38592.496213\n",
      "iteration 1117, current loss: 38680.845810\n",
      "iteration 1118, current loss: 38967.062285\n",
      "iteration 1119, current loss: 39475.627020\n",
      "iteration 1120, current loss: 39423.417737\n",
      "iteration 1121, current loss: 39609.203229\n",
      "iteration 1122, current loss: 39416.646460\n",
      "iteration 1123, current loss: 39297.545784\n",
      "iteration 1124, current loss: 39435.266791\n",
      "iteration 1125, current loss: 40093.406896\n",
      "iteration 1126, current loss: 39799.964843\n",
      "iteration 1127, current loss: 39995.642497\n",
      "iteration 1128, current loss: 40166.881651\n",
      "iteration 1129, current loss: 40083.931793\n",
      "iteration 1130, current loss: 39459.739745\n",
      "iteration 1131, current loss: 39071.634892\n",
      "iteration 1132, current loss: 38584.416853\n",
      "iteration 1133, current loss: 38678.815149\n",
      "iteration 1134, current loss: 38518.912917\n",
      "iteration 1135, current loss: 38673.797232\n",
      "iteration 1136, current loss: 38444.402104\n",
      "iteration 1137, current loss: 38625.520473\n",
      "iteration 1138, current loss: 38911.063467\n",
      "iteration 1139, current loss: 39445.522471\n",
      "iteration 1140, current loss: 39659.316114\n",
      "iteration 1141, current loss: 39596.721901\n",
      "iteration 1142, current loss: 39698.028107\n",
      "iteration 1143, current loss: 38979.359875\n",
      "iteration 1144, current loss: 38673.699017\n",
      "iteration 1145, current loss: 38829.077707\n",
      "iteration 1146, current loss: 39067.911404\n",
      "iteration 1147, current loss: 39867.408694\n",
      "iteration 1148, current loss: 40316.589528\n",
      "iteration 1149, current loss: 40852.377022\n",
      "iteration 1150, current loss: 40185.506198\n",
      "iteration 1151, current loss: 39873.409970\n",
      "iteration 1152, current loss: 39386.217025\n",
      "iteration 1153, current loss: 40269.598338\n",
      "iteration 1154, current loss: 39283.870117\n",
      "iteration 1155, current loss: 39884.154158\n",
      "iteration 1156, current loss: 39193.542456\n",
      "iteration 1157, current loss: 38953.343682\n",
      "iteration 1158, current loss: 38896.322372\n",
      "iteration 1159, current loss: 38737.572278\n",
      "iteration 1160, current loss: 38728.317297\n",
      "iteration 1161, current loss: 38432.221551\n",
      "iteration 1162, current loss: 38235.765514\n",
      "iteration 1163, current loss: 38067.257650\n",
      "iteration 1164, current loss: 37838.355939\n",
      "iteration 1165, current loss: 37874.031948\n",
      "iteration 1166, current loss: 37988.839212\n",
      "iteration 1167, current loss: 38279.217559\n",
      "iteration 1168, current loss: 38523.812448\n",
      "iteration 1169, current loss: 39072.269597\n",
      "iteration 1170, current loss: 39564.903427\n",
      "iteration 1171, current loss: 39881.582730\n",
      "iteration 1172, current loss: 39239.953181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1173, current loss: 39043.584164\n",
      "iteration 1174, current loss: 38695.392494\n",
      "iteration 1175, current loss: 38627.681671\n",
      "iteration 1176, current loss: 38406.681078\n",
      "iteration 1177, current loss: 38880.615665\n",
      "iteration 1178, current loss: 39151.442518\n",
      "iteration 1179, current loss: 39205.164460\n",
      "iteration 1180, current loss: 39383.609463\n",
      "iteration 1181, current loss: 39781.385185\n",
      "iteration 1182, current loss: 39560.184666\n",
      "iteration 1183, current loss: 39720.194885\n",
      "iteration 1184, current loss: 38704.037051\n",
      "iteration 1185, current loss: 38455.855070\n",
      "iteration 1186, current loss: 38435.636140\n",
      "iteration 1187, current loss: 38770.705581\n",
      "iteration 1188, current loss: 38598.196166\n",
      "iteration 1189, current loss: 39065.217796\n",
      "iteration 1190, current loss: 38827.768370\n",
      "iteration 1191, current loss: 38693.429007\n",
      "iteration 1192, current loss: 38945.187727\n",
      "iteration 1193, current loss: 39060.634724\n",
      "iteration 1194, current loss: 39004.347873\n",
      "iteration 1195, current loss: 38558.361979\n",
      "iteration 1196, current loss: 38580.074225\n",
      "iteration 1197, current loss: 38882.161901\n",
      "iteration 1198, current loss: 39411.032371\n",
      "iteration 1199, current loss: 38873.601610\n",
      "iteration 1200, current loss: 39123.140591\n",
      "iteration 1201, current loss: 38589.748486\n",
      "iteration 1202, current loss: 38401.463355\n",
      "iteration 1203, current loss: 38273.144020\n",
      "iteration 1204, current loss: 38213.927914\n",
      "iteration 1205, current loss: 38075.333440\n",
      "iteration 1206, current loss: 37966.589816\n",
      "iteration 1207, current loss: 37844.037496\n",
      "iteration 1208, current loss: 37916.311242\n",
      "iteration 1209, current loss: 38291.142728\n",
      "iteration 1210, current loss: 38592.173722\n",
      "iteration 1211, current loss: 39073.649586\n",
      "iteration 1212, current loss: 38953.518878\n",
      "iteration 1213, current loss: 39253.521490\n",
      "iteration 1214, current loss: 39028.645249\n",
      "iteration 1215, current loss: 39370.984367\n",
      "iteration 1216, current loss: 38801.724370\n",
      "iteration 1217, current loss: 39320.884504\n",
      "iteration 1218, current loss: 38854.592419\n",
      "iteration 1219, current loss: 38961.713862\n",
      "iteration 1220, current loss: 38314.807565\n",
      "iteration 1221, current loss: 38567.425845\n",
      "iteration 1222, current loss: 37729.332143\n",
      "iteration 1223, current loss: 37758.524990\n",
      "iteration 1224, current loss: 37380.936421\n",
      "iteration 1225, current loss: 37364.593980\n",
      "iteration 1226, current loss: 37548.219592\n",
      "iteration 1227, current loss: 37705.984929\n",
      "iteration 1228, current loss: 38086.170956\n",
      "iteration 1229, current loss: 38533.709029\n",
      "iteration 1230, current loss: 39110.894866\n",
      "iteration 1231, current loss: 38980.584052\n",
      "iteration 1232, current loss: 38459.186099\n",
      "iteration 1233, current loss: 38056.841786\n",
      "iteration 1234, current loss: 38000.344863\n",
      "iteration 1235, current loss: 37973.045185\n",
      "iteration 1236, current loss: 37825.030545\n",
      "iteration 1237, current loss: 37382.801897\n",
      "iteration 1238, current loss: 37252.809283\n",
      "iteration 1239, current loss: 37225.268547\n",
      "iteration 1240, current loss: 37277.461929\n",
      "iteration 1241, current loss: 37339.101636\n",
      "iteration 1242, current loss: 37553.705898\n",
      "iteration 1243, current loss: 38232.427772\n",
      "iteration 1244, current loss: 38649.494248\n",
      "iteration 1245, current loss: 39437.673748\n",
      "iteration 1246, current loss: 39339.353404\n",
      "iteration 1247, current loss: 40010.039986\n",
      "iteration 1248, current loss: 38880.943017\n",
      "iteration 1249, current loss: 38960.140433\n",
      "iteration 1250, current loss: 37538.066116\n",
      "iteration 1251, current loss: 37108.196598\n",
      "iteration 1252, current loss: 37163.443722\n",
      "iteration 1253, current loss: 37215.159002\n",
      "iteration 1254, current loss: 37262.450315\n",
      "iteration 1255, current loss: 37502.768843\n",
      "iteration 1256, current loss: 37564.344051\n",
      "iteration 1257, current loss: 37494.548059\n",
      "iteration 1258, current loss: 38383.416667\n",
      "iteration 1259, current loss: 38839.729514\n",
      "iteration 1260, current loss: 41186.052485\n",
      "iteration 1261, current loss: 40108.636619\n",
      "iteration 1262, current loss: 40353.538607\n",
      "iteration 1263, current loss: 39144.500636\n",
      "iteration 1264, current loss: 38144.180955\n",
      "iteration 1265, current loss: 37283.869670\n",
      "iteration 1266, current loss: 37249.540237\n",
      "iteration 1267, current loss: 37245.728448\n",
      "iteration 1268, current loss: 37235.040077\n",
      "iteration 1269, current loss: 37108.942403\n",
      "iteration 1270, current loss: 37061.162682\n",
      "iteration 1271, current loss: 37046.237323\n",
      "iteration 1272, current loss: 36991.423625\n",
      "iteration 1273, current loss: 36955.098063\n",
      "iteration 1274, current loss: 37293.905081\n",
      "iteration 1275, current loss: 37487.503085\n",
      "iteration 1276, current loss: 37878.190125\n",
      "iteration 1277, current loss: 37539.627313\n",
      "iteration 1278, current loss: 38302.909744\n",
      "iteration 1279, current loss: 37972.891054\n",
      "iteration 1280, current loss: 38481.126893\n",
      "iteration 1281, current loss: 37758.392796\n",
      "iteration 1282, current loss: 37837.304604\n",
      "iteration 1283, current loss: 37380.123123\n",
      "iteration 1284, current loss: 37497.644624\n",
      "iteration 1285, current loss: 37133.584772\n",
      "iteration 1286, current loss: 37078.356331\n",
      "iteration 1287, current loss: 37182.993940\n",
      "iteration 1288, current loss: 37439.170038\n",
      "iteration 1289, current loss: 37484.768809\n",
      "iteration 1290, current loss: 37691.489695\n",
      "iteration 1291, current loss: 38085.988862\n",
      "iteration 1292, current loss: 38142.449729\n",
      "iteration 1293, current loss: 38038.901750\n",
      "iteration 1294, current loss: 37922.804239\n",
      "iteration 1295, current loss: 37101.961258\n",
      "iteration 1296, current loss: 37122.726325\n",
      "iteration 1297, current loss: 36886.721872\n",
      "iteration 1298, current loss: 36869.572193\n",
      "iteration 1299, current loss: 36728.899403\n",
      "iteration 1300, current loss: 36829.885489\n",
      "iteration 1301, current loss: 37064.558477\n",
      "iteration 1302, current loss: 37923.990834\n",
      "iteration 1303, current loss: 39136.054191\n",
      "iteration 1304, current loss: 38993.256559\n",
      "iteration 1305, current loss: 40301.296176\n",
      "iteration 1306, current loss: 38383.641254\n",
      "iteration 1307, current loss: 37844.088054\n",
      "iteration 1308, current loss: 37152.500401\n",
      "iteration 1309, current loss: 37211.149014\n",
      "iteration 1310, current loss: 37015.827709\n",
      "iteration 1311, current loss: 36687.611577\n",
      "iteration 1312, current loss: 36519.910426\n",
      "iteration 1313, current loss: 36619.410258\n",
      "iteration 1314, current loss: 37020.661864\n",
      "iteration 1315, current loss: 37373.589133\n",
      "iteration 1316, current loss: 37299.003570\n",
      "iteration 1317, current loss: 38247.911022\n",
      "iteration 1318, current loss: 38712.155607\n",
      "iteration 1319, current loss: 38935.463617\n",
      "iteration 1320, current loss: 38695.076937\n",
      "iteration 1321, current loss: 38218.467245\n",
      "iteration 1322, current loss: 37057.046949\n",
      "iteration 1323, current loss: 36529.991492\n",
      "iteration 1324, current loss: 36591.082270\n",
      "iteration 1325, current loss: 36591.274504\n",
      "iteration 1326, current loss: 36717.206855\n",
      "iteration 1327, current loss: 37400.187570\n",
      "iteration 1328, current loss: 37521.735961\n",
      "iteration 1329, current loss: 37747.693250\n",
      "iteration 1330, current loss: 37540.812076\n",
      "iteration 1331, current loss: 37244.424744\n",
      "iteration 1332, current loss: 37246.764778\n",
      "iteration 1333, current loss: 37616.033755\n",
      "iteration 1334, current loss: 38714.377979\n",
      "iteration 1335, current loss: 37817.687680\n",
      "iteration 1336, current loss: 37790.890685\n",
      "iteration 1337, current loss: 37471.677539\n",
      "iteration 1338, current loss: 37073.103732\n",
      "iteration 1339, current loss: 37344.912334\n",
      "iteration 1340, current loss: 36939.804328\n",
      "iteration 1341, current loss: 36591.340670\n",
      "iteration 1342, current loss: 36567.216650\n",
      "iteration 1343, current loss: 36350.523037\n",
      "iteration 1344, current loss: 36704.117679\n",
      "iteration 1345, current loss: 36375.165926\n",
      "iteration 1346, current loss: 36583.002309\n",
      "iteration 1347, current loss: 36692.053790\n",
      "iteration 1348, current loss: 36509.506737\n",
      "iteration 1349, current loss: 36449.654662\n",
      "iteration 1350, current loss: 36396.487885\n",
      "iteration 1351, current loss: 36560.057618\n",
      "iteration 1352, current loss: 36728.985610\n",
      "iteration 1353, current loss: 37265.365292\n",
      "iteration 1354, current loss: 37748.744270\n",
      "iteration 1355, current loss: 37822.611061\n",
      "iteration 1356, current loss: 37873.799658\n",
      "iteration 1357, current loss: 37902.229448\n",
      "iteration 1358, current loss: 37964.815926\n",
      "iteration 1359, current loss: 37525.273090\n",
      "iteration 1360, current loss: 37151.595048\n",
      "iteration 1361, current loss: 36555.845087\n",
      "iteration 1362, current loss: 36358.086474\n",
      "iteration 1363, current loss: 36269.876624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1364, current loss: 36248.753310\n",
      "iteration 1365, current loss: 36667.400177\n",
      "iteration 1366, current loss: 37036.422403\n",
      "iteration 1367, current loss: 37387.180804\n",
      "iteration 1368, current loss: 37767.911399\n",
      "iteration 1369, current loss: 37798.246899\n",
      "iteration 1370, current loss: 38257.629055\n",
      "iteration 1371, current loss: 37334.415726\n",
      "iteration 1372, current loss: 37010.582778\n",
      "iteration 1373, current loss: 36885.350457\n",
      "iteration 1374, current loss: 37155.822863\n",
      "iteration 1375, current loss: 36746.716043\n",
      "iteration 1376, current loss: 36880.993285\n",
      "iteration 1377, current loss: 36609.774815\n",
      "iteration 1378, current loss: 36862.400790\n",
      "iteration 1379, current loss: 36500.954096\n",
      "iteration 1380, current loss: 36467.862015\n",
      "iteration 1381, current loss: 35969.775303\n",
      "iteration 1382, current loss: 35960.670037\n",
      "iteration 1383, current loss: 35750.264765\n",
      "iteration 1384, current loss: 35654.540905\n",
      "iteration 1385, current loss: 35864.946381\n",
      "iteration 1386, current loss: 36012.736716\n",
      "iteration 1387, current loss: 36285.872855\n",
      "iteration 1388, current loss: 37945.795835\n",
      "iteration 1389, current loss: 38772.386552\n",
      "iteration 1390, current loss: 40350.214620\n",
      "iteration 1391, current loss: 39076.711868\n",
      "iteration 1392, current loss: 38525.110285\n",
      "iteration 1393, current loss: 37513.055138\n",
      "iteration 1394, current loss: 37125.432997\n",
      "iteration 1395, current loss: 36673.759766\n",
      "iteration 1396, current loss: 36893.171092\n",
      "iteration 1397, current loss: 36719.729996\n",
      "iteration 1398, current loss: 36204.701730\n",
      "iteration 1399, current loss: 36076.303001\n",
      "iteration 1400, current loss: 36123.412120\n",
      "iteration 1401, current loss: 36056.778848\n",
      "iteration 1402, current loss: 36230.921752\n",
      "iteration 1403, current loss: 36405.546019\n",
      "iteration 1404, current loss: 36723.326217\n",
      "iteration 1405, current loss: 36678.440703\n",
      "iteration 1406, current loss: 37563.617899\n",
      "iteration 1407, current loss: 37082.339169\n",
      "iteration 1408, current loss: 37030.115100\n",
      "iteration 1409, current loss: 36524.490718\n",
      "iteration 1410, current loss: 36210.505133\n",
      "iteration 1411, current loss: 35804.060248\n",
      "iteration 1412, current loss: 35734.047141\n",
      "iteration 1413, current loss: 35785.300497\n",
      "iteration 1414, current loss: 35673.583169\n",
      "iteration 1415, current loss: 35963.076291\n",
      "iteration 1416, current loss: 35852.968158\n",
      "iteration 1417, current loss: 36152.314172\n",
      "iteration 1418, current loss: 35763.486832\n",
      "iteration 1419, current loss: 35865.579844\n",
      "iteration 1420, current loss: 36517.381102\n",
      "iteration 1421, current loss: 37107.627980\n",
      "iteration 1422, current loss: 37421.220609\n",
      "iteration 1423, current loss: 37409.746345\n",
      "iteration 1424, current loss: 37779.883543\n",
      "iteration 1425, current loss: 36410.797015\n",
      "iteration 1426, current loss: 36013.085774\n",
      "iteration 1427, current loss: 36403.815646\n",
      "iteration 1428, current loss: 36849.499921\n",
      "iteration 1429, current loss: 36748.884989\n",
      "iteration 1430, current loss: 36527.126199\n",
      "iteration 1431, current loss: 36574.572928\n",
      "iteration 1432, current loss: 36662.964372\n",
      "iteration 1433, current loss: 36169.812091\n",
      "iteration 1434, current loss: 35869.367726\n",
      "iteration 1435, current loss: 35648.795206\n",
      "iteration 1436, current loss: 35671.961230\n",
      "iteration 1437, current loss: 35322.924805\n",
      "iteration 1438, current loss: 35465.677927\n",
      "iteration 1439, current loss: 35710.773532\n",
      "iteration 1440, current loss: 35765.499299\n",
      "iteration 1441, current loss: 36098.649514\n",
      "iteration 1442, current loss: 36418.902158\n",
      "iteration 1443, current loss: 36358.235108\n",
      "iteration 1444, current loss: 36311.983995\n",
      "iteration 1445, current loss: 35572.656571\n",
      "iteration 1446, current loss: 35240.071779\n",
      "iteration 1447, current loss: 35171.753288\n",
      "iteration 1448, current loss: 35387.390666\n",
      "iteration 1449, current loss: 35316.761107\n",
      "iteration 1450, current loss: 35956.094332\n",
      "iteration 1451, current loss: 36316.182190\n",
      "iteration 1452, current loss: 38458.969113\n",
      "iteration 1453, current loss: 37713.482260\n",
      "iteration 1454, current loss: 38447.775252\n",
      "iteration 1455, current loss: 37422.776787\n",
      "iteration 1456, current loss: 37204.039854\n",
      "iteration 1457, current loss: 35873.144021\n",
      "iteration 1458, current loss: 35802.695607\n",
      "iteration 1459, current loss: 35537.041645\n",
      "iteration 1460, current loss: 35666.761759\n",
      "iteration 1461, current loss: 35341.408564\n",
      "iteration 1462, current loss: 35325.773184\n",
      "iteration 1463, current loss: 35281.351917\n",
      "iteration 1464, current loss: 35191.802577\n",
      "iteration 1465, current loss: 35152.999595\n",
      "iteration 1466, current loss: 35449.825459\n",
      "iteration 1467, current loss: 35464.225102\n",
      "iteration 1468, current loss: 35950.366959\n",
      "iteration 1469, current loss: 36234.951244\n",
      "iteration 1470, current loss: 37858.502244\n",
      "iteration 1471, current loss: 38204.870063\n",
      "iteration 1472, current loss: 39372.805964\n",
      "iteration 1473, current loss: 37488.761318\n",
      "iteration 1474, current loss: 36808.626790\n",
      "iteration 1475, current loss: 35850.343036\n",
      "iteration 1476, current loss: 35613.791349\n",
      "iteration 1477, current loss: 35007.258277\n",
      "iteration 1478, current loss: 34963.770565\n",
      "iteration 1479, current loss: 34939.598205\n",
      "iteration 1480, current loss: 35111.305577\n",
      "iteration 1481, current loss: 35364.224219\n",
      "iteration 1482, current loss: 35637.320404\n",
      "iteration 1483, current loss: 36200.192805\n",
      "iteration 1484, current loss: 35917.940975\n",
      "iteration 1485, current loss: 36261.028684\n",
      "iteration 1486, current loss: 36241.941074\n",
      "iteration 1487, current loss: 36654.770039\n",
      "iteration 1488, current loss: 35646.571544\n",
      "iteration 1489, current loss: 35929.901988\n",
      "iteration 1490, current loss: 35915.800492\n",
      "iteration 1491, current loss: 36812.574620\n",
      "iteration 1492, current loss: 36585.766799\n",
      "iteration 1493, current loss: 36620.988676\n",
      "iteration 1494, current loss: 36150.138174\n",
      "iteration 1495, current loss: 36327.966733\n",
      "iteration 1496, current loss: 36098.033066\n",
      "iteration 1497, current loss: 35933.816260\n",
      "iteration 1498, current loss: 35940.996042\n",
      "iteration 1499, current loss: 36101.648737\n",
      "iteration 1500, current loss: 36091.361929\n",
      "iteration 1501, current loss: 36072.681397\n",
      "iteration 1502, current loss: 36191.715661\n",
      "iteration 1503, current loss: 35488.339087\n",
      "iteration 1504, current loss: 35642.544709\n",
      "iteration 1505, current loss: 35140.532075\n",
      "iteration 1506, current loss: 35344.744560\n",
      "iteration 1507, current loss: 34972.260858\n",
      "iteration 1508, current loss: 35374.823095\n",
      "iteration 1509, current loss: 35021.686735\n",
      "iteration 1510, current loss: 35114.875402\n",
      "iteration 1511, current loss: 35394.841572\n",
      "iteration 1512, current loss: 36027.188389\n",
      "iteration 1513, current loss: 36024.973444\n",
      "iteration 1514, current loss: 36677.864602\n",
      "iteration 1515, current loss: 36248.486568\n",
      "iteration 1516, current loss: 36619.614243\n",
      "iteration 1517, current loss: 35400.627325\n",
      "iteration 1518, current loss: 35040.099612\n",
      "iteration 1519, current loss: 35007.361121\n",
      "iteration 1520, current loss: 34786.163242\n",
      "iteration 1521, current loss: 34749.926088\n",
      "iteration 1522, current loss: 35442.015501\n",
      "iteration 1523, current loss: 35439.674455\n",
      "iteration 1524, current loss: 35163.774597\n",
      "iteration 1525, current loss: 35233.128969\n",
      "iteration 1526, current loss: 34904.665250\n",
      "iteration 1527, current loss: 35149.826365\n",
      "iteration 1528, current loss: 35068.375885\n",
      "iteration 1529, current loss: 35166.657391\n",
      "iteration 1530, current loss: 34983.932871\n",
      "iteration 1531, current loss: 35049.497563\n",
      "iteration 1532, current loss: 35013.811392\n",
      "iteration 1533, current loss: 35105.905981\n",
      "iteration 1534, current loss: 34725.571350\n",
      "iteration 1535, current loss: 35513.610590\n",
      "iteration 1536, current loss: 35486.264502\n",
      "iteration 1537, current loss: 36391.727720\n",
      "iteration 1538, current loss: 37121.712428\n",
      "iteration 1539, current loss: 36486.455747\n",
      "iteration 1540, current loss: 35300.423518\n",
      "iteration 1541, current loss: 35451.217781\n",
      "iteration 1542, current loss: 35520.866031\n",
      "iteration 1543, current loss: 36810.510476\n",
      "iteration 1544, current loss: 36123.857529\n",
      "iteration 1545, current loss: 36613.186074\n",
      "iteration 1546, current loss: 35881.602252\n",
      "iteration 1547, current loss: 36102.035274\n",
      "iteration 1548, current loss: 35945.252952\n",
      "iteration 1549, current loss: 35421.585810\n",
      "iteration 1550, current loss: 35158.076712\n",
      "iteration 1551, current loss: 35718.666878\n",
      "iteration 1552, current loss: 35737.194166\n",
      "iteration 1553, current loss: 35687.931780\n",
      "iteration 1554, current loss: 35735.203801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1555, current loss: 35533.647852\n",
      "iteration 1556, current loss: 35561.397471\n",
      "iteration 1557, current loss: 35435.310902\n",
      "iteration 1558, current loss: 35134.585518\n",
      "iteration 1559, current loss: 35285.799274\n",
      "iteration 1560, current loss: 34922.736495\n",
      "iteration 1561, current loss: 34663.930811\n",
      "iteration 1562, current loss: 34447.741596\n",
      "iteration 1563, current loss: 34440.146799\n",
      "iteration 1564, current loss: 34612.595506\n",
      "iteration 1565, current loss: 34782.003139\n",
      "iteration 1566, current loss: 35103.678282\n",
      "iteration 1567, current loss: 35243.755725\n",
      "iteration 1568, current loss: 35587.672434\n",
      "iteration 1569, current loss: 36322.656957\n",
      "iteration 1570, current loss: 36242.198285\n",
      "iteration 1571, current loss: 37297.979780\n",
      "iteration 1572, current loss: 37025.884956\n",
      "iteration 1573, current loss: 37366.085194\n",
      "iteration 1574, current loss: 35943.602360\n",
      "iteration 1575, current loss: 36134.167191\n",
      "iteration 1576, current loss: 34991.583341\n",
      "iteration 1577, current loss: 35022.337420\n",
      "iteration 1578, current loss: 34751.700077\n",
      "iteration 1579, current loss: 34745.921041\n",
      "iteration 1580, current loss: 34300.371819\n",
      "iteration 1581, current loss: 34332.226486\n",
      "iteration 1582, current loss: 34385.191105\n",
      "iteration 1583, current loss: 34556.813042\n",
      "iteration 1584, current loss: 34755.006095\n",
      "iteration 1585, current loss: 34348.606459\n",
      "iteration 1586, current loss: 34339.657900\n",
      "iteration 1587, current loss: 34663.886866\n",
      "iteration 1588, current loss: 35023.631299\n",
      "iteration 1589, current loss: 35490.060167\n",
      "iteration 1590, current loss: 35569.292272\n",
      "iteration 1591, current loss: 35322.970219\n",
      "iteration 1592, current loss: 35151.114716\n",
      "iteration 1593, current loss: 35050.056299\n",
      "iteration 1594, current loss: 34582.718876\n",
      "iteration 1595, current loss: 34670.608861\n",
      "iteration 1596, current loss: 35042.288386\n",
      "iteration 1597, current loss: 34875.085602\n",
      "iteration 1598, current loss: 34128.225293\n",
      "iteration 1599, current loss: 33852.887571\n",
      "iteration 1600, current loss: 34233.750253\n",
      "iteration 1601, current loss: 34322.971401\n",
      "iteration 1602, current loss: 34533.351084\n",
      "iteration 1603, current loss: 35131.935982\n",
      "iteration 1604, current loss: 35531.253210\n",
      "iteration 1605, current loss: 35972.327022\n",
      "iteration 1606, current loss: 36709.154781\n",
      "iteration 1607, current loss: 35561.527882\n",
      "iteration 1608, current loss: 35484.938924\n",
      "iteration 1609, current loss: 34762.856063\n",
      "iteration 1610, current loss: 34702.992804\n",
      "iteration 1611, current loss: 34331.569893\n",
      "iteration 1612, current loss: 34986.618994\n",
      "iteration 1613, current loss: 35717.249504\n",
      "iteration 1614, current loss: 37382.882404\n",
      "iteration 1615, current loss: 36927.777810\n",
      "iteration 1616, current loss: 36533.857210\n",
      "iteration 1617, current loss: 35420.225451\n",
      "iteration 1618, current loss: 35365.384743\n",
      "iteration 1619, current loss: 34567.298216\n",
      "iteration 1620, current loss: 33959.282338\n",
      "iteration 1621, current loss: 33924.818762\n",
      "iteration 1622, current loss: 33860.264027\n",
      "iteration 1623, current loss: 34149.678389\n",
      "iteration 1624, current loss: 34134.664534\n",
      "iteration 1625, current loss: 34326.707894\n",
      "iteration 1626, current loss: 34852.120628\n",
      "iteration 1627, current loss: 35301.783673\n",
      "iteration 1628, current loss: 36093.506571\n",
      "iteration 1629, current loss: 36105.495949\n",
      "iteration 1630, current loss: 35885.104661\n",
      "iteration 1631, current loss: 35074.536741\n",
      "iteration 1632, current loss: 34683.475741\n",
      "iteration 1633, current loss: 34302.440853\n",
      "iteration 1634, current loss: 34735.009719\n",
      "iteration 1635, current loss: 34204.350050\n",
      "iteration 1636, current loss: 34120.892120\n",
      "iteration 1637, current loss: 34318.030377\n",
      "iteration 1638, current loss: 34857.836045\n",
      "iteration 1639, current loss: 34952.000870\n",
      "iteration 1640, current loss: 34680.100949\n",
      "iteration 1641, current loss: 34129.267310\n",
      "iteration 1642, current loss: 34666.790691\n",
      "iteration 1643, current loss: 35159.784623\n",
      "iteration 1644, current loss: 35029.757286\n",
      "iteration 1645, current loss: 34613.237164\n",
      "iteration 1646, current loss: 34394.897113\n",
      "iteration 1647, current loss: 34279.787057\n",
      "iteration 1648, current loss: 34418.467825\n",
      "iteration 1649, current loss: 34052.482152\n",
      "iteration 1650, current loss: 34067.856016\n",
      "iteration 1651, current loss: 34503.186555\n",
      "iteration 1652, current loss: 34458.124306\n",
      "iteration 1653, current loss: 34330.383466\n",
      "iteration 1654, current loss: 34025.195775\n",
      "iteration 1655, current loss: 34124.597281\n",
      "iteration 1656, current loss: 34237.735687\n",
      "iteration 1657, current loss: 34397.475372\n",
      "iteration 1658, current loss: 34853.102175\n",
      "iteration 1659, current loss: 35014.650086\n",
      "iteration 1660, current loss: 35681.567120\n",
      "iteration 1661, current loss: 35651.680539\n",
      "iteration 1662, current loss: 35082.473011\n",
      "iteration 1663, current loss: 34344.517453\n",
      "iteration 1664, current loss: 34031.209083\n",
      "iteration 1665, current loss: 34015.058771\n",
      "iteration 1666, current loss: 34032.285031\n",
      "iteration 1667, current loss: 34764.194433\n",
      "iteration 1668, current loss: 34831.089476\n",
      "iteration 1669, current loss: 34986.651413\n",
      "iteration 1670, current loss: 34581.247500\n",
      "iteration 1671, current loss: 35701.514520\n",
      "iteration 1672, current loss: 35048.891292\n",
      "iteration 1673, current loss: 34317.343970\n",
      "iteration 1674, current loss: 33876.539401\n",
      "iteration 1675, current loss: 33951.587132\n",
      "iteration 1676, current loss: 33905.624911\n",
      "iteration 1677, current loss: 33964.099787\n",
      "iteration 1678, current loss: 34513.784135\n",
      "iteration 1679, current loss: 34715.216052\n",
      "iteration 1680, current loss: 35060.715606\n",
      "iteration 1681, current loss: 35538.314396\n",
      "iteration 1682, current loss: 34686.612409\n",
      "iteration 1683, current loss: 35121.407797\n",
      "iteration 1684, current loss: 34683.905639\n",
      "iteration 1685, current loss: 34721.201804\n",
      "iteration 1686, current loss: 34353.168206\n",
      "iteration 1687, current loss: 34037.828424\n",
      "iteration 1688, current loss: 33673.515352\n",
      "iteration 1689, current loss: 33743.445645\n",
      "iteration 1690, current loss: 34073.358193\n",
      "iteration 1691, current loss: 34689.193493\n",
      "iteration 1692, current loss: 35009.310242\n",
      "iteration 1693, current loss: 34705.334716\n",
      "iteration 1694, current loss: 34908.803709\n",
      "iteration 1695, current loss: 34620.090152\n",
      "iteration 1696, current loss: 34504.542149\n",
      "iteration 1697, current loss: 34268.464978\n",
      "iteration 1698, current loss: 34146.635525\n",
      "iteration 1699, current loss: 33859.635735\n",
      "iteration 1700, current loss: 34166.183460\n",
      "iteration 1701, current loss: 33612.955429\n",
      "iteration 1702, current loss: 33725.152770\n",
      "iteration 1703, current loss: 33994.687576\n",
      "iteration 1704, current loss: 34020.302162\n",
      "iteration 1705, current loss: 34520.628478\n",
      "iteration 1706, current loss: 34589.163701\n",
      "iteration 1707, current loss: 35044.318176\n",
      "iteration 1708, current loss: 34522.772475\n",
      "iteration 1709, current loss: 34240.291552\n",
      "iteration 1710, current loss: 33362.562572\n",
      "iteration 1711, current loss: 33194.100453\n",
      "iteration 1712, current loss: 33412.670255\n",
      "iteration 1713, current loss: 34014.615317\n",
      "iteration 1714, current loss: 34244.092655\n",
      "iteration 1715, current loss: 34088.480113\n",
      "iteration 1716, current loss: 34191.702621\n",
      "iteration 1717, current loss: 34716.157836\n",
      "iteration 1718, current loss: 34525.990832\n",
      "iteration 1719, current loss: 34382.175177\n",
      "iteration 1720, current loss: 34213.081336\n",
      "iteration 1721, current loss: 33621.447381\n",
      "iteration 1722, current loss: 33540.811537\n",
      "iteration 1723, current loss: 33409.625169\n",
      "iteration 1724, current loss: 33616.230529\n",
      "iteration 1725, current loss: 33583.015745\n",
      "iteration 1726, current loss: 33507.819872\n",
      "iteration 1727, current loss: 33995.828160\n",
      "iteration 1728, current loss: 34289.477832\n",
      "iteration 1729, current loss: 34532.578427\n",
      "iteration 1730, current loss: 34397.521437\n",
      "iteration 1731, current loss: 34837.473823\n",
      "iteration 1732, current loss: 34802.019840\n",
      "iteration 1733, current loss: 34523.263658\n",
      "iteration 1734, current loss: 33742.832337\n",
      "iteration 1735, current loss: 34206.091890\n",
      "iteration 1736, current loss: 33695.520657\n",
      "iteration 1737, current loss: 33226.252677\n",
      "iteration 1738, current loss: 33142.152311\n",
      "iteration 1739, current loss: 33319.787870\n",
      "iteration 1740, current loss: 33697.969212\n",
      "iteration 1741, current loss: 33534.995240\n",
      "iteration 1742, current loss: 33826.599319\n",
      "iteration 1743, current loss: 34474.101273\n",
      "iteration 1744, current loss: 34775.779671\n",
      "iteration 1745, current loss: 36040.388498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1746, current loss: 35136.392248\n",
      "iteration 1747, current loss: 34317.999822\n",
      "iteration 1748, current loss: 33749.743367\n",
      "iteration 1749, current loss: 33624.075602\n",
      "iteration 1750, current loss: 34273.413940\n",
      "iteration 1751, current loss: 34293.848007\n",
      "iteration 1752, current loss: 34054.308405\n",
      "iteration 1753, current loss: 34078.102700\n",
      "iteration 1754, current loss: 34827.531276\n",
      "iteration 1755, current loss: 34245.531355\n",
      "iteration 1756, current loss: 34253.994663\n",
      "iteration 1757, current loss: 33535.105363\n",
      "iteration 1758, current loss: 33430.973483\n",
      "iteration 1759, current loss: 33092.183555\n",
      "iteration 1760, current loss: 32831.491011\n",
      "iteration 1761, current loss: 32643.112245\n",
      "iteration 1762, current loss: 32803.659082\n",
      "iteration 1763, current loss: 32700.123606\n",
      "iteration 1764, current loss: 33166.738336\n",
      "iteration 1765, current loss: 33695.845158\n",
      "iteration 1766, current loss: 33651.988330\n",
      "iteration 1767, current loss: 34040.799152\n",
      "iteration 1768, current loss: 34141.728597\n",
      "iteration 1769, current loss: 34889.750756\n",
      "iteration 1770, current loss: 34937.616510\n",
      "iteration 1771, current loss: 34654.859134\n",
      "iteration 1772, current loss: 33222.543943\n",
      "iteration 1773, current loss: 32977.019277\n",
      "iteration 1774, current loss: 32967.753683\n",
      "iteration 1775, current loss: 33106.052513\n",
      "iteration 1776, current loss: 33323.247505\n",
      "iteration 1777, current loss: 34280.157809\n",
      "iteration 1778, current loss: 33683.143602\n",
      "iteration 1779, current loss: 33674.761660\n",
      "iteration 1780, current loss: 33882.397870\n",
      "iteration 1781, current loss: 34116.646129\n",
      "iteration 1782, current loss: 33209.001100\n",
      "iteration 1783, current loss: 33189.299553\n",
      "iteration 1784, current loss: 33366.567536\n",
      "iteration 1785, current loss: 34277.130158\n",
      "iteration 1786, current loss: 33741.943241\n",
      "iteration 1787, current loss: 33867.921207\n",
      "iteration 1788, current loss: 34007.216361\n",
      "iteration 1789, current loss: 33312.882131\n",
      "iteration 1790, current loss: 33698.248879\n",
      "iteration 1791, current loss: 33080.471451\n",
      "iteration 1792, current loss: 33349.898900\n",
      "iteration 1793, current loss: 33826.610524\n",
      "iteration 1794, current loss: 34689.913678\n",
      "iteration 1795, current loss: 34908.200698\n",
      "iteration 1796, current loss: 35316.331376\n",
      "iteration 1797, current loss: 34299.879845\n",
      "iteration 1798, current loss: 34203.564122\n",
      "iteration 1799, current loss: 33282.883859\n",
      "iteration 1800, current loss: 32998.771937\n",
      "iteration 1801, current loss: 32819.015742\n",
      "iteration 1802, current loss: 32699.056799\n",
      "iteration 1803, current loss: 32663.261685\n",
      "iteration 1804, current loss: 32641.491410\n",
      "iteration 1805, current loss: 33371.282031\n",
      "iteration 1806, current loss: 33381.714251\n",
      "iteration 1807, current loss: 34002.369346\n",
      "iteration 1808, current loss: 34671.320564\n",
      "iteration 1809, current loss: 35010.325277\n",
      "iteration 1810, current loss: 36883.210489\n",
      "iteration 1811, current loss: 34659.521892\n",
      "iteration 1812, current loss: 33722.654401\n",
      "iteration 1813, current loss: 33096.334177\n",
      "iteration 1814, current loss: 33070.128360\n",
      "iteration 1815, current loss: 33116.281561\n",
      "iteration 1816, current loss: 32962.937053\n",
      "iteration 1817, current loss: 33499.778366\n",
      "iteration 1818, current loss: 32798.514832\n",
      "iteration 1819, current loss: 33234.850404\n",
      "iteration 1820, current loss: 33057.245660\n",
      "iteration 1821, current loss: 32985.015923\n",
      "iteration 1822, current loss: 32855.685836\n",
      "iteration 1823, current loss: 33316.105094\n",
      "iteration 1824, current loss: 33359.939895\n",
      "iteration 1825, current loss: 33726.999307\n",
      "iteration 1826, current loss: 33885.675141\n",
      "iteration 1827, current loss: 35006.051037\n",
      "iteration 1828, current loss: 34311.674477\n",
      "iteration 1829, current loss: 34264.078224\n",
      "iteration 1830, current loss: 34425.571913\n",
      "iteration 1831, current loss: 34028.070375\n",
      "iteration 1832, current loss: 34126.147720\n",
      "iteration 1833, current loss: 33560.375740\n",
      "iteration 1834, current loss: 33126.947069\n",
      "iteration 1835, current loss: 32987.325478\n",
      "iteration 1836, current loss: 33275.196851\n",
      "iteration 1837, current loss: 33133.439972\n",
      "iteration 1838, current loss: 33105.071397\n",
      "iteration 1839, current loss: 33013.120357\n",
      "iteration 1840, current loss: 32819.552504\n",
      "iteration 1841, current loss: 32959.649931\n",
      "iteration 1842, current loss: 33620.851804\n",
      "iteration 1843, current loss: 33970.808994\n",
      "iteration 1844, current loss: 34424.962690\n",
      "iteration 1845, current loss: 34716.924307\n",
      "iteration 1846, current loss: 34465.132890\n",
      "iteration 1847, current loss: 33714.996490\n",
      "iteration 1848, current loss: 33102.428025\n",
      "iteration 1849, current loss: 32996.205073\n",
      "iteration 1850, current loss: 32231.605407\n",
      "iteration 1851, current loss: 32301.097319\n",
      "iteration 1852, current loss: 32408.831232\n",
      "iteration 1853, current loss: 32621.081619\n",
      "iteration 1854, current loss: 32689.806958\n",
      "iteration 1855, current loss: 33096.181874\n",
      "iteration 1856, current loss: 33010.640555\n",
      "iteration 1857, current loss: 33453.070959\n",
      "iteration 1858, current loss: 33679.103720\n",
      "iteration 1859, current loss: 33741.678047\n",
      "iteration 1860, current loss: 33382.175678\n",
      "iteration 1861, current loss: 33416.185435\n",
      "iteration 1862, current loss: 33499.856054\n",
      "iteration 1863, current loss: 33108.061466\n",
      "iteration 1864, current loss: 32693.201460\n",
      "iteration 1865, current loss: 32777.619217\n",
      "iteration 1866, current loss: 33174.719924\n",
      "iteration 1867, current loss: 32889.872092\n",
      "iteration 1868, current loss: 32880.460603\n",
      "iteration 1869, current loss: 32959.336665\n",
      "iteration 1870, current loss: 33190.446047\n",
      "iteration 1871, current loss: 33231.040594\n",
      "iteration 1872, current loss: 32719.669321\n",
      "iteration 1873, current loss: 33386.799768\n",
      "iteration 1874, current loss: 33828.840774\n",
      "iteration 1875, current loss: 34893.530959\n",
      "iteration 1876, current loss: 33163.356630\n",
      "iteration 1877, current loss: 33176.887341\n",
      "iteration 1878, current loss: 32992.856662\n",
      "iteration 1879, current loss: 32859.106324\n",
      "iteration 1880, current loss: 32445.534060\n",
      "iteration 1881, current loss: 32635.321123\n",
      "iteration 1882, current loss: 32707.930270\n",
      "iteration 1883, current loss: 32733.156883\n",
      "iteration 1884, current loss: 32712.756957\n",
      "iteration 1885, current loss: 33405.287026\n",
      "iteration 1886, current loss: 34098.165094\n",
      "iteration 1887, current loss: 34230.812793\n",
      "iteration 1888, current loss: 34518.110569\n",
      "iteration 1889, current loss: 33490.745373\n",
      "iteration 1890, current loss: 33074.289819\n",
      "iteration 1891, current loss: 32333.322796\n",
      "iteration 1892, current loss: 32733.300177\n",
      "iteration 1893, current loss: 32861.960737\n",
      "iteration 1894, current loss: 33348.500452\n",
      "iteration 1895, current loss: 32838.580146\n",
      "iteration 1896, current loss: 33063.026027\n",
      "iteration 1897, current loss: 32929.102792\n",
      "iteration 1898, current loss: 33368.588845\n",
      "iteration 1899, current loss: 33620.737221\n",
      "iteration 1900, current loss: 33614.569614\n",
      "iteration 1901, current loss: 33012.954256\n",
      "iteration 1902, current loss: 32902.281289\n",
      "iteration 1903, current loss: 32380.919994\n",
      "iteration 1904, current loss: 32622.541741\n",
      "iteration 1905, current loss: 33181.273164\n",
      "iteration 1906, current loss: 32478.786752\n",
      "iteration 1907, current loss: 32354.669104\n",
      "iteration 1908, current loss: 32241.708726\n",
      "iteration 1909, current loss: 32449.106326\n",
      "iteration 1910, current loss: 32485.426709\n",
      "iteration 1911, current loss: 32626.009401\n",
      "iteration 1912, current loss: 32881.581871\n",
      "iteration 1913, current loss: 33125.460870\n",
      "iteration 1914, current loss: 33445.129689\n",
      "iteration 1915, current loss: 33299.057387\n",
      "iteration 1916, current loss: 33769.524471\n",
      "iteration 1917, current loss: 33131.909430\n",
      "iteration 1918, current loss: 32796.792414\n",
      "iteration 1919, current loss: 32171.505608\n",
      "iteration 1920, current loss: 32541.357860\n",
      "iteration 1921, current loss: 32330.049844\n",
      "iteration 1922, current loss: 32612.314130\n",
      "iteration 1923, current loss: 33173.986826\n",
      "iteration 1924, current loss: 33888.243468\n",
      "iteration 1925, current loss: 33255.262138\n",
      "iteration 1926, current loss: 33034.251350\n",
      "iteration 1927, current loss: 33346.524286\n",
      "iteration 1928, current loss: 33196.758947\n",
      "iteration 1929, current loss: 33232.091919\n",
      "iteration 1930, current loss: 32859.831733\n",
      "iteration 1931, current loss: 32610.735774\n",
      "iteration 1932, current loss: 32161.795523\n",
      "iteration 1933, current loss: 32016.745040\n",
      "iteration 1934, current loss: 32148.938346\n",
      "iteration 1935, current loss: 32916.152084\n",
      "iteration 1936, current loss: 33352.164706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1937, current loss: 33169.330048\n",
      "iteration 1938, current loss: 33078.440579\n",
      "iteration 1939, current loss: 32749.097368\n",
      "iteration 1940, current loss: 32628.194972\n",
      "iteration 1941, current loss: 32880.914332\n",
      "iteration 1942, current loss: 33472.606912\n",
      "iteration 1943, current loss: 33168.183325\n",
      "iteration 1944, current loss: 34442.225327\n",
      "iteration 1945, current loss: 33157.449767\n",
      "iteration 1946, current loss: 33237.797931\n",
      "iteration 1947, current loss: 32566.494364\n",
      "iteration 1948, current loss: 32225.073024\n",
      "iteration 1949, current loss: 31739.987921\n",
      "iteration 1950, current loss: 31889.534809\n",
      "iteration 1951, current loss: 32149.753920\n",
      "iteration 1952, current loss: 32312.825012\n",
      "iteration 1953, current loss: 32355.175895\n",
      "iteration 1954, current loss: 32418.118736\n",
      "iteration 1955, current loss: 32552.256773\n",
      "iteration 1956, current loss: 32853.710273\n",
      "iteration 1957, current loss: 33151.227337\n",
      "iteration 1958, current loss: 33720.389024\n",
      "iteration 1959, current loss: 33070.847943\n",
      "iteration 1960, current loss: 32974.112246\n",
      "iteration 1961, current loss: 32164.883015\n",
      "iteration 1962, current loss: 31859.367512\n",
      "iteration 1963, current loss: 31735.539817\n",
      "iteration 1964, current loss: 31904.191183\n",
      "iteration 1965, current loss: 31440.020302\n",
      "iteration 1966, current loss: 31651.804801\n",
      "iteration 1967, current loss: 31503.812040\n",
      "iteration 1968, current loss: 32278.775255\n",
      "iteration 1969, current loss: 32264.762604\n",
      "iteration 1970, current loss: 32334.671930\n",
      "iteration 1971, current loss: 32605.543938\n",
      "iteration 1972, current loss: 33532.167134\n",
      "iteration 1973, current loss: 33628.122240\n",
      "iteration 1974, current loss: 33649.679303\n",
      "iteration 1975, current loss: 33240.872099\n",
      "iteration 1976, current loss: 33497.522334\n",
      "iteration 1977, current loss: 33493.425722\n",
      "iteration 1978, current loss: 33519.342895\n",
      "iteration 1979, current loss: 33350.057217\n",
      "iteration 1980, current loss: 32592.798773\n",
      "iteration 1981, current loss: 32430.657473\n",
      "iteration 1982, current loss: 33175.125837\n",
      "iteration 1983, current loss: 33455.394072\n",
      "iteration 1984, current loss: 32878.245558\n",
      "iteration 1985, current loss: 32314.231343\n",
      "iteration 1986, current loss: 31961.339511\n",
      "iteration 1987, current loss: 32043.955917\n",
      "iteration 1988, current loss: 32253.070568\n",
      "iteration 1989, current loss: 32497.767158\n",
      "iteration 1990, current loss: 33246.602365\n",
      "iteration 1991, current loss: 32976.100209\n",
      "iteration 1992, current loss: 33389.165295\n",
      "iteration 1993, current loss: 33686.097254\n",
      "iteration 1994, current loss: 34103.580439\n",
      "iteration 1995, current loss: 32922.820111\n",
      "iteration 1996, current loss: 32652.391077\n",
      "iteration 1997, current loss: 32470.473009\n",
      "iteration 1998, current loss: 32228.631722\n",
      "iteration 1999, current loss: 32461.830617\n",
      "iteration 2000, current loss: 32615.894121\n",
      "iteration 2001, current loss: 32671.994629\n",
      "iteration 2002, current loss: 32584.866397\n",
      "iteration 2003, current loss: 32355.447725\n",
      "iteration 2004, current loss: 32181.864282\n",
      "iteration 2005, current loss: 32212.043833\n",
      "iteration 2006, current loss: 32010.513319\n",
      "iteration 2007, current loss: 32165.473370\n",
      "iteration 2008, current loss: 32401.281161\n",
      "iteration 2009, current loss: 32429.235893\n",
      "iteration 2010, current loss: 32239.515973\n",
      "iteration 2011, current loss: 32029.020415\n",
      "iteration 2012, current loss: 31996.267421\n",
      "iteration 2013, current loss: 32590.797416\n",
      "iteration 2014, current loss: 32378.992356\n",
      "iteration 2015, current loss: 33249.610043\n",
      "iteration 2016, current loss: 32593.957393\n",
      "iteration 2017, current loss: 33398.053026\n",
      "iteration 2018, current loss: 33410.545449\n",
      "iteration 2019, current loss: 33397.287791\n",
      "iteration 2020, current loss: 32900.317801\n",
      "iteration 2021, current loss: 32594.191010\n",
      "iteration 2022, current loss: 32261.142136\n",
      "iteration 2023, current loss: 32133.899160\n",
      "iteration 2024, current loss: 32225.187620\n",
      "iteration 2025, current loss: 32241.728328\n",
      "iteration 2026, current loss: 32273.388743\n",
      "iteration 2027, current loss: 31809.178340\n",
      "iteration 2028, current loss: 31898.660363\n",
      "iteration 2029, current loss: 31893.856140\n",
      "iteration 2030, current loss: 32562.665621\n",
      "iteration 2031, current loss: 32412.839088\n",
      "iteration 2032, current loss: 32484.762039\n",
      "iteration 2033, current loss: 32471.587727\n",
      "iteration 2034, current loss: 33147.260190\n",
      "iteration 2035, current loss: 32561.948290\n",
      "iteration 2036, current loss: 33540.970383\n",
      "iteration 2037, current loss: 32563.672547\n",
      "iteration 2038, current loss: 32345.006290\n",
      "iteration 2039, current loss: 32286.590245\n",
      "iteration 2040, current loss: 32219.949666\n",
      "iteration 2041, current loss: 32237.418057\n",
      "iteration 2042, current loss: 32260.926517\n",
      "iteration 2043, current loss: 32916.118904\n",
      "iteration 2044, current loss: 32575.091163\n",
      "iteration 2045, current loss: 33139.677733\n",
      "iteration 2046, current loss: 32671.709789\n",
      "iteration 2047, current loss: 32920.242277\n",
      "iteration 2048, current loss: 32395.834660\n",
      "iteration 2049, current loss: 32158.209699\n",
      "iteration 2050, current loss: 32056.536409\n",
      "iteration 2051, current loss: 31801.709965\n",
      "iteration 2052, current loss: 31528.518182\n",
      "iteration 2053, current loss: 31201.261643\n",
      "iteration 2054, current loss: 31159.066317\n",
      "iteration 2055, current loss: 31557.180166\n",
      "iteration 2056, current loss: 32336.581474\n",
      "iteration 2057, current loss: 31592.963952\n",
      "iteration 2058, current loss: 31485.271865\n",
      "iteration 2059, current loss: 31779.288446\n",
      "iteration 2060, current loss: 31933.946645\n",
      "iteration 2061, current loss: 32058.123281\n",
      "iteration 2062, current loss: 32628.957802\n",
      "iteration 2063, current loss: 33013.945809\n",
      "iteration 2064, current loss: 33025.424593\n",
      "iteration 2065, current loss: 33420.428165\n",
      "iteration 2066, current loss: 32630.118408\n",
      "iteration 2067, current loss: 32068.851011\n",
      "iteration 2068, current loss: 31486.028090\n",
      "iteration 2069, current loss: 31927.026683\n",
      "iteration 2070, current loss: 31917.344731\n",
      "iteration 2071, current loss: 32352.563162\n",
      "iteration 2072, current loss: 32081.857852\n",
      "iteration 2073, current loss: 32852.014110\n",
      "iteration 2074, current loss: 32279.509930\n",
      "iteration 2075, current loss: 32110.784297\n",
      "iteration 2076, current loss: 31758.884305\n",
      "iteration 2077, current loss: 31903.166420\n",
      "iteration 2078, current loss: 31809.899713\n",
      "iteration 2079, current loss: 32180.444998\n",
      "iteration 2080, current loss: 32391.786180\n",
      "iteration 2081, current loss: 32532.035474\n",
      "iteration 2082, current loss: 33185.338731\n",
      "iteration 2083, current loss: 32611.741811\n",
      "iteration 2084, current loss: 32807.488507\n",
      "iteration 2085, current loss: 32503.686994\n",
      "iteration 2086, current loss: 32354.308603\n",
      "iteration 2087, current loss: 31953.149221\n",
      "iteration 2088, current loss: 32330.251018\n",
      "iteration 2089, current loss: 31937.503176\n",
      "iteration 2090, current loss: 31364.654917\n",
      "iteration 2091, current loss: 31191.594390\n",
      "iteration 2092, current loss: 31022.139405\n",
      "iteration 2093, current loss: 31054.960475\n",
      "iteration 2094, current loss: 31233.509799\n",
      "iteration 2095, current loss: 31119.892511\n",
      "iteration 2096, current loss: 30802.833662\n",
      "iteration 2097, current loss: 30855.996315\n",
      "iteration 2098, current loss: 30822.979661\n",
      "iteration 2099, current loss: 31025.688276\n",
      "iteration 2100, current loss: 31462.511218\n",
      "iteration 2101, current loss: 31548.298996\n",
      "iteration 2102, current loss: 32293.359815\n",
      "iteration 2103, current loss: 33061.832227\n",
      "iteration 2104, current loss: 32822.464096\n",
      "iteration 2105, current loss: 32930.810935\n",
      "iteration 2106, current loss: 32075.691047\n",
      "iteration 2107, current loss: 31789.178039\n",
      "iteration 2108, current loss: 31458.753444\n",
      "iteration 2109, current loss: 31331.945459\n",
      "iteration 2110, current loss: 31329.678937\n",
      "iteration 2111, current loss: 31769.304459\n",
      "iteration 2112, current loss: 32614.477926\n",
      "iteration 2113, current loss: 33559.125036\n",
      "iteration 2114, current loss: 33753.105493\n",
      "iteration 2115, current loss: 34004.301308\n",
      "iteration 2116, current loss: 32575.945550\n",
      "iteration 2117, current loss: 32055.459204\n",
      "iteration 2118, current loss: 31251.113350\n",
      "iteration 2119, current loss: 31239.681737\n",
      "iteration 2120, current loss: 31186.225104\n",
      "iteration 2121, current loss: 31411.839009\n",
      "iteration 2122, current loss: 32089.491498\n",
      "iteration 2123, current loss: 32501.597641\n",
      "iteration 2124, current loss: 32845.084534\n",
      "iteration 2125, current loss: 32959.768136\n",
      "iteration 2126, current loss: 33332.076557\n",
      "iteration 2127, current loss: 33041.013558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2128, current loss: 32453.894798\n",
      "iteration 2129, current loss: 31961.842362\n",
      "iteration 2130, current loss: 31734.685668\n",
      "iteration 2131, current loss: 31277.716036\n",
      "iteration 2132, current loss: 31396.519868\n",
      "iteration 2133, current loss: 30908.274029\n",
      "iteration 2134, current loss: 30969.945974\n",
      "iteration 2135, current loss: 30618.133633\n",
      "iteration 2136, current loss: 30928.741639\n",
      "iteration 2137, current loss: 30767.966910\n",
      "iteration 2138, current loss: 31569.002576\n",
      "iteration 2139, current loss: 32200.328226\n",
      "iteration 2140, current loss: 32866.233003\n",
      "iteration 2141, current loss: 32410.289095\n",
      "iteration 2142, current loss: 32787.669063\n",
      "iteration 2143, current loss: 32076.193132\n",
      "iteration 2144, current loss: 32781.342894\n",
      "iteration 2145, current loss: 31561.683080\n",
      "iteration 2146, current loss: 31538.351127\n",
      "iteration 2147, current loss: 31071.688466\n",
      "iteration 2148, current loss: 31141.965021\n",
      "iteration 2149, current loss: 30861.559147\n",
      "iteration 2150, current loss: 30910.506693\n",
      "iteration 2151, current loss: 31298.051610\n",
      "iteration 2152, current loss: 31717.000242\n",
      "iteration 2153, current loss: 31953.398861\n",
      "iteration 2154, current loss: 31878.358642\n",
      "iteration 2155, current loss: 31646.690377\n",
      "iteration 2156, current loss: 31463.205042\n",
      "iteration 2157, current loss: 32016.942663\n",
      "iteration 2158, current loss: 32988.240193\n",
      "iteration 2159, current loss: 34028.910532\n",
      "iteration 2160, current loss: 34611.615573\n",
      "iteration 2161, current loss: 33756.254156\n",
      "iteration 2162, current loss: 32318.757134\n",
      "iteration 2163, current loss: 32550.357612\n",
      "iteration 2164, current loss: 32794.594426\n",
      "iteration 2165, current loss: 32307.192481\n",
      "iteration 2166, current loss: 31834.911209\n",
      "iteration 2167, current loss: 32240.157535\n",
      "iteration 2168, current loss: 31654.505808\n",
      "iteration 2169, current loss: 31319.450594\n",
      "iteration 2170, current loss: 31230.534969\n",
      "iteration 2171, current loss: 31416.393692\n",
      "iteration 2172, current loss: 31388.362720\n",
      "iteration 2173, current loss: 30983.858006\n",
      "iteration 2174, current loss: 30662.096657\n",
      "iteration 2175, current loss: 31267.210240\n",
      "iteration 2176, current loss: 31800.872338\n",
      "iteration 2177, current loss: 32518.590641\n",
      "iteration 2178, current loss: 31521.834875\n",
      "iteration 2179, current loss: 32398.710117\n",
      "iteration 2180, current loss: 32180.164631\n",
      "iteration 2181, current loss: 32894.896701\n",
      "iteration 2182, current loss: 31709.121799\n",
      "iteration 2183, current loss: 31691.212382\n",
      "iteration 2184, current loss: 31705.197691\n",
      "iteration 2185, current loss: 31105.947786\n",
      "iteration 2186, current loss: 30839.243829\n",
      "iteration 2187, current loss: 30989.610462\n",
      "iteration 2188, current loss: 30920.802866\n",
      "iteration 2189, current loss: 30972.412115\n",
      "iteration 2190, current loss: 30922.599975\n",
      "iteration 2191, current loss: 31083.179163\n",
      "iteration 2192, current loss: 31182.717759\n",
      "iteration 2193, current loss: 31837.351438\n",
      "iteration 2194, current loss: 31701.771396\n",
      "iteration 2195, current loss: 32035.069372\n",
      "iteration 2196, current loss: 32724.677321\n",
      "iteration 2197, current loss: 33413.036313\n",
      "iteration 2198, current loss: 32863.403726\n",
      "iteration 2199, current loss: 32359.897321\n",
      "iteration 2200, current loss: 32602.002966\n",
      "iteration 2201, current loss: 32696.365340\n",
      "iteration 2202, current loss: 31318.998618\n",
      "iteration 2203, current loss: 31279.124955\n",
      "iteration 2204, current loss: 31262.501761\n",
      "iteration 2205, current loss: 30854.239281\n",
      "iteration 2206, current loss: 30925.724039\n",
      "iteration 2207, current loss: 31005.424622\n",
      "iteration 2208, current loss: 31480.699680\n",
      "iteration 2209, current loss: 31405.034509\n",
      "iteration 2210, current loss: 31042.695395\n",
      "iteration 2211, current loss: 31412.838425\n",
      "iteration 2212, current loss: 31420.857851\n",
      "iteration 2213, current loss: 31631.508558\n",
      "iteration 2214, current loss: 31901.375692\n",
      "iteration 2215, current loss: 32162.988043\n",
      "iteration 2216, current loss: 32296.761911\n",
      "iteration 2217, current loss: 31837.912314\n",
      "iteration 2218, current loss: 31360.157512\n",
      "iteration 2219, current loss: 30901.549268\n",
      "iteration 2220, current loss: 30623.230397\n",
      "iteration 2221, current loss: 30564.770258\n",
      "iteration 2222, current loss: 30946.842276\n",
      "iteration 2223, current loss: 31376.494514\n",
      "iteration 2224, current loss: 31152.441504\n",
      "iteration 2225, current loss: 31674.857382\n",
      "iteration 2226, current loss: 31607.134837\n",
      "iteration 2227, current loss: 31233.908108\n",
      "iteration 2228, current loss: 31450.273944\n",
      "iteration 2229, current loss: 31891.078076\n",
      "iteration 2230, current loss: 31089.483472\n",
      "iteration 2231, current loss: 31450.023507\n",
      "iteration 2232, current loss: 31113.369446\n",
      "iteration 2233, current loss: 31426.664099\n",
      "iteration 2234, current loss: 31668.272457\n",
      "iteration 2235, current loss: 32179.913891\n",
      "iteration 2236, current loss: 31194.931570\n",
      "iteration 2237, current loss: 30973.592783\n",
      "iteration 2238, current loss: 30691.571496\n",
      "iteration 2239, current loss: 30608.046521\n",
      "iteration 2240, current loss: 30358.898639\n",
      "iteration 2241, current loss: 30570.136390\n",
      "iteration 2242, current loss: 30438.793239\n",
      "iteration 2243, current loss: 30547.228630\n",
      "iteration 2244, current loss: 31216.705029\n",
      "iteration 2245, current loss: 31800.009045\n",
      "iteration 2246, current loss: 32309.309966\n",
      "iteration 2247, current loss: 32535.794350\n",
      "iteration 2248, current loss: 31568.020630\n",
      "iteration 2249, current loss: 30993.461205\n",
      "iteration 2250, current loss: 31392.590519\n",
      "iteration 2251, current loss: 31325.487144\n",
      "iteration 2252, current loss: 31143.668622\n",
      "iteration 2253, current loss: 31016.500148\n",
      "iteration 2254, current loss: 31571.922475\n",
      "iteration 2255, current loss: 31646.918312\n",
      "iteration 2256, current loss: 32102.959430\n",
      "iteration 2257, current loss: 32642.035215\n",
      "iteration 2258, current loss: 33274.581130\n",
      "iteration 2259, current loss: 31957.413373\n",
      "iteration 2260, current loss: 31794.579543\n",
      "iteration 2261, current loss: 31911.796522\n",
      "iteration 2262, current loss: 31420.546540\n",
      "iteration 2263, current loss: 31598.768702\n",
      "iteration 2264, current loss: 31109.613703\n",
      "iteration 2265, current loss: 30836.738339\n",
      "iteration 2266, current loss: 30574.362979\n",
      "iteration 2267, current loss: 30934.532233\n",
      "iteration 2268, current loss: 30681.480008\n",
      "iteration 2269, current loss: 30522.878509\n",
      "iteration 2270, current loss: 30491.167140\n",
      "iteration 2271, current loss: 30285.938085\n",
      "iteration 2272, current loss: 30296.514817\n",
      "iteration 2273, current loss: 30246.872171\n",
      "iteration 2274, current loss: 30306.256606\n",
      "iteration 2275, current loss: 30403.562745\n",
      "iteration 2276, current loss: 30524.659268\n",
      "iteration 2277, current loss: 30634.570933\n",
      "iteration 2278, current loss: 30934.200195\n",
      "iteration 2279, current loss: 31267.025122\n",
      "iteration 2280, current loss: 31098.752910\n",
      "iteration 2281, current loss: 30584.067496\n",
      "iteration 2282, current loss: 30867.646784\n",
      "iteration 2283, current loss: 31056.966040\n",
      "iteration 2284, current loss: 31747.062786\n",
      "iteration 2285, current loss: 31170.626546\n",
      "iteration 2286, current loss: 31555.694434\n",
      "iteration 2287, current loss: 30915.864435\n",
      "iteration 2288, current loss: 31875.011022\n",
      "iteration 2289, current loss: 31837.449165\n",
      "iteration 2290, current loss: 31860.343957\n",
      "iteration 2291, current loss: 31890.603111\n",
      "iteration 2292, current loss: 31454.758616\n",
      "iteration 2293, current loss: 31417.757341\n",
      "iteration 2294, current loss: 31611.703060\n",
      "iteration 2295, current loss: 31551.862573\n",
      "iteration 2296, current loss: 31345.124343\n",
      "iteration 2297, current loss: 31213.820333\n",
      "iteration 2298, current loss: 31575.866673\n",
      "iteration 2299, current loss: 30806.323509\n",
      "iteration 2300, current loss: 31078.251742\n",
      "iteration 2301, current loss: 30597.526672\n",
      "iteration 2302, current loss: 30405.895166\n",
      "iteration 2303, current loss: 30172.180169\n",
      "iteration 2304, current loss: 30041.873765\n",
      "iteration 2305, current loss: 30064.420625\n",
      "iteration 2306, current loss: 30508.280742\n",
      "iteration 2307, current loss: 30378.818573\n",
      "iteration 2308, current loss: 30742.388719\n",
      "iteration 2309, current loss: 31109.109328\n",
      "iteration 2310, current loss: 32489.701451\n",
      "iteration 2311, current loss: 31460.228193\n",
      "iteration 2312, current loss: 31732.561276\n",
      "iteration 2313, current loss: 31943.024635\n",
      "iteration 2314, current loss: 31804.591120\n",
      "iteration 2315, current loss: 31794.411619\n",
      "iteration 2316, current loss: 31032.886649\n",
      "iteration 2317, current loss: 30923.551107\n",
      "iteration 2318, current loss: 30359.935379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2319, current loss: 30770.723908\n",
      "iteration 2320, current loss: 30557.555883\n",
      "iteration 2321, current loss: 30573.869557\n",
      "iteration 2322, current loss: 30870.319999\n",
      "iteration 2323, current loss: 30564.173341\n",
      "iteration 2324, current loss: 30912.876401\n",
      "iteration 2325, current loss: 31578.217002\n",
      "iteration 2326, current loss: 32108.849927\n",
      "iteration 2327, current loss: 32776.929933\n",
      "iteration 2328, current loss: 32048.817430\n",
      "iteration 2329, current loss: 30881.014031\n",
      "iteration 2330, current loss: 30762.935638\n",
      "iteration 2331, current loss: 31019.779551\n",
      "iteration 2332, current loss: 30902.905354\n",
      "iteration 2333, current loss: 31636.507853\n",
      "iteration 2334, current loss: 30894.490178\n",
      "iteration 2335, current loss: 30762.801961\n",
      "iteration 2336, current loss: 30699.160099\n",
      "iteration 2337, current loss: 30737.545800\n",
      "iteration 2338, current loss: 31227.944901\n",
      "iteration 2339, current loss: 31041.685111\n",
      "iteration 2340, current loss: 30945.662860\n",
      "iteration 2341, current loss: 31154.253012\n",
      "iteration 2342, current loss: 30674.370267\n",
      "iteration 2343, current loss: 30928.260463\n",
      "iteration 2344, current loss: 30520.469044\n",
      "iteration 2345, current loss: 31186.375857\n",
      "iteration 2346, current loss: 30674.325768\n",
      "iteration 2347, current loss: 30417.820406\n",
      "iteration 2348, current loss: 30200.693022\n",
      "iteration 2349, current loss: 30549.540083\n",
      "iteration 2350, current loss: 31097.862944\n",
      "iteration 2351, current loss: 30380.107447\n",
      "iteration 2352, current loss: 30454.220058\n",
      "iteration 2353, current loss: 30377.291497\n",
      "iteration 2354, current loss: 30854.357714\n",
      "iteration 2355, current loss: 30744.522888\n",
      "iteration 2356, current loss: 31297.652920\n",
      "iteration 2357, current loss: 30565.168740\n",
      "iteration 2358, current loss: 31159.200767\n",
      "iteration 2359, current loss: 31393.905580\n",
      "iteration 2360, current loss: 31647.822502\n",
      "iteration 2361, current loss: 31403.219643\n",
      "iteration 2362, current loss: 31275.087424\n",
      "iteration 2363, current loss: 30858.616681\n",
      "iteration 2364, current loss: 30871.350344\n",
      "iteration 2365, current loss: 30451.731248\n",
      "iteration 2366, current loss: 30284.768527\n",
      "iteration 2367, current loss: 30701.177442\n",
      "iteration 2368, current loss: 30622.788641\n",
      "iteration 2369, current loss: 31693.207140\n",
      "iteration 2370, current loss: 31704.016928\n",
      "iteration 2371, current loss: 32120.211185\n",
      "iteration 2372, current loss: 31486.560911\n",
      "iteration 2373, current loss: 31199.196567\n",
      "iteration 2374, current loss: 31643.110601\n",
      "iteration 2375, current loss: 31939.339528\n",
      "iteration 2376, current loss: 32977.680017\n",
      "iteration 2377, current loss: 31947.986434\n",
      "iteration 2378, current loss: 31222.384678\n",
      "iteration 2379, current loss: 30254.692743\n",
      "iteration 2380, current loss: 30277.618922\n",
      "iteration 2381, current loss: 29799.221833\n",
      "iteration 2382, current loss: 29849.061675\n",
      "iteration 2383, current loss: 30146.086720\n",
      "iteration 2384, current loss: 29791.764779\n",
      "iteration 2385, current loss: 30194.485309\n",
      "iteration 2386, current loss: 30824.658317\n",
      "iteration 2387, current loss: 30628.930342\n",
      "iteration 2388, current loss: 30463.837062\n",
      "iteration 2389, current loss: 31400.956398\n",
      "iteration 2390, current loss: 31527.675493\n",
      "iteration 2391, current loss: 31250.487186\n",
      "iteration 2392, current loss: 30358.029043\n",
      "iteration 2393, current loss: 30094.007345\n",
      "iteration 2394, current loss: 30004.609315\n",
      "iteration 2395, current loss: 29732.166566\n",
      "iteration 2396, current loss: 29778.767982\n",
      "iteration 2397, current loss: 30149.097267\n",
      "iteration 2398, current loss: 30638.501254\n",
      "iteration 2399, current loss: 30837.446177\n",
      "iteration 2400, current loss: 32330.522082\n",
      "iteration 2401, current loss: 32730.899944\n",
      "iteration 2402, current loss: 34176.229913\n",
      "iteration 2403, current loss: 32982.794644\n",
      "iteration 2404, current loss: 32313.895266\n",
      "iteration 2405, current loss: 31344.990999\n",
      "iteration 2406, current loss: 30437.061893\n",
      "iteration 2407, current loss: 29960.912007\n",
      "iteration 2408, current loss: 29799.490204\n",
      "iteration 2409, current loss: 29518.404361\n",
      "iteration 2410, current loss: 29791.123863\n",
      "iteration 2411, current loss: 30063.169008\n",
      "iteration 2412, current loss: 29895.995148\n",
      "iteration 2413, current loss: 29933.774138\n",
      "iteration 2414, current loss: 30003.167733\n",
      "iteration 2415, current loss: 29922.428281\n",
      "iteration 2416, current loss: 30221.943163\n",
      "iteration 2417, current loss: 30131.987203\n",
      "iteration 2418, current loss: 30636.354174\n",
      "iteration 2419, current loss: 31108.854423\n",
      "iteration 2420, current loss: 30932.368743\n",
      "iteration 2421, current loss: 30880.108016\n",
      "iteration 2422, current loss: 31003.495034\n",
      "iteration 2423, current loss: 30521.146990\n",
      "iteration 2424, current loss: 30397.416635\n",
      "iteration 2425, current loss: 30288.042901\n",
      "iteration 2426, current loss: 30826.730709\n",
      "iteration 2427, current loss: 30640.685395\n",
      "iteration 2428, current loss: 31557.116962\n",
      "iteration 2429, current loss: 31587.192275\n",
      "iteration 2430, current loss: 31373.081055\n",
      "iteration 2431, current loss: 31777.714900\n",
      "iteration 2432, current loss: 31358.848726\n",
      "iteration 2433, current loss: 31678.222775\n",
      "iteration 2434, current loss: 31080.499922\n",
      "iteration 2435, current loss: 31287.621355\n",
      "iteration 2436, current loss: 30569.129573\n",
      "iteration 2437, current loss: 30054.870073\n",
      "iteration 2438, current loss: 29831.917698\n",
      "iteration 2439, current loss: 29720.626553\n",
      "iteration 2440, current loss: 29602.461826\n",
      "iteration 2441, current loss: 29593.685186\n",
      "iteration 2442, current loss: 29941.460758\n",
      "iteration 2443, current loss: 30075.406680\n",
      "iteration 2444, current loss: 30545.655507\n",
      "iteration 2445, current loss: 30653.473175\n",
      "iteration 2446, current loss: 30854.656070\n",
      "iteration 2447, current loss: 30876.502634\n",
      "iteration 2448, current loss: 30463.091422\n",
      "iteration 2449, current loss: 30489.785773\n",
      "iteration 2450, current loss: 30383.369377\n",
      "iteration 2451, current loss: 30972.732427\n",
      "iteration 2452, current loss: 30426.819850\n",
      "iteration 2453, current loss: 30677.101662\n",
      "iteration 2454, current loss: 30356.637625\n",
      "iteration 2455, current loss: 30799.130834\n",
      "iteration 2456, current loss: 30249.369683\n",
      "iteration 2457, current loss: 29509.180292\n",
      "iteration 2458, current loss: 29363.994885\n",
      "iteration 2459, current loss: 29314.748444\n",
      "iteration 2460, current loss: 29571.746939\n",
      "iteration 2461, current loss: 30441.434360\n",
      "iteration 2462, current loss: 30425.114320\n",
      "iteration 2463, current loss: 31400.730106\n",
      "iteration 2464, current loss: 30681.653948\n",
      "iteration 2465, current loss: 31187.579829\n",
      "iteration 2466, current loss: 30890.475955\n",
      "iteration 2467, current loss: 30528.606768\n",
      "iteration 2468, current loss: 30095.689103\n",
      "iteration 2469, current loss: 30385.601074\n",
      "iteration 2470, current loss: 30212.881477\n",
      "iteration 2471, current loss: 30424.352291\n",
      "iteration 2472, current loss: 30967.397052\n",
      "iteration 2473, current loss: 31068.874540\n",
      "iteration 2474, current loss: 31725.532702\n",
      "iteration 2475, current loss: 31462.556014\n",
      "iteration 2476, current loss: 32100.939516\n",
      "iteration 2477, current loss: 31239.241552\n",
      "iteration 2478, current loss: 31583.514550\n",
      "iteration 2479, current loss: 31088.013192\n",
      "iteration 2480, current loss: 30615.304897\n",
      "iteration 2481, current loss: 30138.517716\n",
      "iteration 2482, current loss: 29534.703815\n",
      "iteration 2483, current loss: 29446.749968\n",
      "iteration 2484, current loss: 29309.878086\n",
      "iteration 2485, current loss: 29357.026432\n",
      "iteration 2486, current loss: 29508.387783\n",
      "iteration 2487, current loss: 29664.603612\n",
      "iteration 2488, current loss: 29528.004012\n",
      "iteration 2489, current loss: 30023.896630\n",
      "iteration 2490, current loss: 30803.538272\n",
      "iteration 2491, current loss: 30324.849383\n",
      "iteration 2492, current loss: 30962.488741\n",
      "iteration 2493, current loss: 31060.404879\n",
      "iteration 2494, current loss: 32602.309179\n",
      "iteration 2495, current loss: 31276.457677\n",
      "iteration 2496, current loss: 31560.430169\n",
      "iteration 2497, current loss: 30536.396704\n",
      "iteration 2498, current loss: 29641.607064\n",
      "iteration 2499, current loss: 29037.007551\n",
      "iteration 2500, current loss: 28948.917913\n",
      "iteration 2501, current loss: 28973.157749\n",
      "iteration 2502, current loss: 29350.352231\n",
      "iteration 2503, current loss: 29325.370414\n",
      "iteration 2504, current loss: 30051.733024\n",
      "iteration 2505, current loss: 30744.650971\n",
      "iteration 2506, current loss: 31614.838702\n",
      "iteration 2507, current loss: 31780.567438\n",
      "iteration 2508, current loss: 31648.112296\n",
      "iteration 2509, current loss: 31517.545477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2510, current loss: 31295.791221\n",
      "iteration 2511, current loss: 30610.458208\n",
      "iteration 2512, current loss: 29883.374835\n",
      "iteration 2513, current loss: 29812.333074\n",
      "iteration 2514, current loss: 30126.031773\n",
      "iteration 2515, current loss: 29647.410986\n",
      "iteration 2516, current loss: 29661.360221\n",
      "iteration 2517, current loss: 29997.822276\n",
      "iteration 2518, current loss: 30618.893794\n",
      "iteration 2519, current loss: 30407.398661\n",
      "iteration 2520, current loss: 30202.691233\n",
      "iteration 2521, current loss: 30586.637794\n",
      "iteration 2522, current loss: 31140.955721\n",
      "iteration 2523, current loss: 31074.585540\n",
      "iteration 2524, current loss: 31143.283854\n",
      "iteration 2525, current loss: 29726.005697\n",
      "iteration 2526, current loss: 29143.451407\n",
      "iteration 2527, current loss: 29192.393934\n",
      "iteration 2528, current loss: 29361.377740\n",
      "iteration 2529, current loss: 29496.392641\n",
      "iteration 2530, current loss: 29794.955212\n",
      "iteration 2531, current loss: 30621.114312\n",
      "iteration 2532, current loss: 30389.676412\n",
      "iteration 2533, current loss: 29974.412887\n",
      "iteration 2534, current loss: 29491.980246\n",
      "iteration 2535, current loss: 29520.386172\n",
      "iteration 2536, current loss: 29748.868025\n",
      "iteration 2537, current loss: 30092.885686\n",
      "iteration 2538, current loss: 29881.215035\n",
      "iteration 2539, current loss: 30030.585266\n",
      "iteration 2540, current loss: 29994.724380\n",
      "iteration 2541, current loss: 30713.633539\n",
      "iteration 2542, current loss: 30570.502487\n",
      "iteration 2543, current loss: 31610.134422\n",
      "iteration 2544, current loss: 30687.642125\n",
      "iteration 2545, current loss: 31105.593454\n",
      "iteration 2546, current loss: 30694.079720\n",
      "iteration 2547, current loss: 31418.392558\n",
      "iteration 2548, current loss: 30937.540495\n",
      "iteration 2549, current loss: 31498.235918\n",
      "iteration 2550, current loss: 31106.203126\n",
      "iteration 2551, current loss: 30774.599193\n",
      "iteration 2552, current loss: 30583.943825\n",
      "iteration 2553, current loss: 30812.398324\n",
      "iteration 2554, current loss: 30293.128458\n",
      "iteration 2555, current loss: 30023.364350\n",
      "iteration 2556, current loss: 29832.635594\n",
      "iteration 2557, current loss: 29622.366987\n",
      "iteration 2558, current loss: 29503.959056\n",
      "iteration 2559, current loss: 30279.045168\n",
      "iteration 2560, current loss: 30022.224165\n",
      "iteration 2561, current loss: 30266.014998\n",
      "iteration 2562, current loss: 29775.088644\n",
      "iteration 2563, current loss: 29984.790286\n",
      "iteration 2564, current loss: 30251.440263\n",
      "iteration 2565, current loss: 29774.410133\n",
      "iteration 2566, current loss: 29971.712174\n",
      "iteration 2567, current loss: 29966.955186\n",
      "iteration 2568, current loss: 29823.287146\n",
      "iteration 2569, current loss: 29700.305222\n",
      "iteration 2570, current loss: 29384.404515\n",
      "iteration 2571, current loss: 29200.855584\n",
      "iteration 2572, current loss: 29204.063002\n",
      "iteration 2573, current loss: 29499.605642\n",
      "iteration 2574, current loss: 30124.097924\n",
      "iteration 2575, current loss: 30250.190523\n",
      "iteration 2576, current loss: 30881.630368\n",
      "iteration 2577, current loss: 31138.808035\n",
      "iteration 2578, current loss: 30648.070814\n",
      "iteration 2579, current loss: 30608.560193\n",
      "iteration 2580, current loss: 31450.235249\n",
      "iteration 2581, current loss: 31451.278638\n",
      "iteration 2582, current loss: 30311.119017\n",
      "iteration 2583, current loss: 29720.161962\n",
      "iteration 2584, current loss: 29562.955256\n",
      "iteration 2585, current loss: 29607.925477\n",
      "iteration 2586, current loss: 29893.955785\n",
      "iteration 2587, current loss: 30403.031869\n",
      "iteration 2588, current loss: 29880.464350\n",
      "iteration 2589, current loss: 29957.992208\n",
      "iteration 2590, current loss: 29861.192941\n",
      "iteration 2591, current loss: 30370.786604\n",
      "iteration 2592, current loss: 30491.743941\n",
      "iteration 2593, current loss: 30366.626537\n",
      "iteration 2594, current loss: 30339.829224\n",
      "iteration 2595, current loss: 29827.825241\n",
      "iteration 2596, current loss: 29867.704891\n",
      "iteration 2597, current loss: 29481.887877\n",
      "iteration 2598, current loss: 29990.887914\n",
      "iteration 2599, current loss: 29219.329924\n",
      "iteration 2600, current loss: 29498.533505\n",
      "iteration 2601, current loss: 29083.653184\n",
      "iteration 2602, current loss: 29550.533206\n",
      "iteration 2603, current loss: 29163.669313\n",
      "iteration 2604, current loss: 30037.495538\n",
      "iteration 2605, current loss: 29716.014644\n",
      "iteration 2606, current loss: 30000.255923\n",
      "iteration 2607, current loss: 29957.862779\n",
      "iteration 2608, current loss: 31017.439463\n",
      "iteration 2609, current loss: 29852.381232\n",
      "iteration 2610, current loss: 30768.494636\n",
      "iteration 2611, current loss: 30477.660103\n",
      "iteration 2612, current loss: 30584.799373\n",
      "iteration 2613, current loss: 30460.257080\n",
      "iteration 2614, current loss: 30471.043708\n",
      "iteration 2615, current loss: 30852.265429\n",
      "iteration 2616, current loss: 30406.687917\n",
      "iteration 2617, current loss: 30784.888539\n",
      "iteration 2618, current loss: 29550.559642\n",
      "iteration 2619, current loss: 29672.947579\n",
      "iteration 2620, current loss: 29501.873820\n",
      "iteration 2621, current loss: 30133.910413\n",
      "iteration 2622, current loss: 30514.501743\n",
      "iteration 2623, current loss: 31599.531511\n",
      "iteration 2624, current loss: 30686.233907\n",
      "iteration 2625, current loss: 30239.045859\n",
      "iteration 2626, current loss: 29707.206156\n",
      "iteration 2627, current loss: 29647.180099\n",
      "iteration 2628, current loss: 29559.015443\n",
      "iteration 2629, current loss: 29001.037797\n",
      "iteration 2630, current loss: 29220.231371\n",
      "iteration 2631, current loss: 28571.410144\n",
      "iteration 2632, current loss: 28677.325839\n",
      "iteration 2633, current loss: 28633.901533\n",
      "iteration 2634, current loss: 29067.864588\n",
      "iteration 2635, current loss: 28988.537214\n",
      "iteration 2636, current loss: 29527.835438\n",
      "iteration 2637, current loss: 29653.882124\n",
      "iteration 2638, current loss: 30520.387625\n",
      "iteration 2639, current loss: 29757.109483\n",
      "iteration 2640, current loss: 30335.708648\n",
      "iteration 2641, current loss: 29580.213589\n",
      "iteration 2642, current loss: 30738.937858\n",
      "iteration 2643, current loss: 29974.469166\n",
      "iteration 2644, current loss: 30397.256150\n",
      "iteration 2645, current loss: 30535.346849\n",
      "iteration 2646, current loss: 30643.275995\n",
      "iteration 2647, current loss: 30035.067976\n",
      "iteration 2648, current loss: 29980.931372\n",
      "iteration 2649, current loss: 30537.847054\n",
      "iteration 2650, current loss: 30693.291007\n",
      "iteration 2651, current loss: 30895.729210\n",
      "iteration 2652, current loss: 30496.671528\n",
      "iteration 2653, current loss: 30764.280993\n",
      "iteration 2654, current loss: 29906.250603\n",
      "iteration 2655, current loss: 29122.768907\n",
      "iteration 2656, current loss: 29169.865034\n",
      "iteration 2657, current loss: 29095.833564\n",
      "iteration 2658, current loss: 28966.687938\n",
      "iteration 2659, current loss: 29189.878313\n",
      "iteration 2660, current loss: 29128.106691\n",
      "iteration 2661, current loss: 29321.977122\n",
      "iteration 2662, current loss: 29853.880750\n",
      "iteration 2663, current loss: 30173.815232\n",
      "iteration 2664, current loss: 30877.804860\n",
      "iteration 2665, current loss: 30548.894689\n",
      "iteration 2666, current loss: 30400.148660\n",
      "iteration 2667, current loss: 30748.053402\n",
      "iteration 2668, current loss: 29813.101768\n",
      "iteration 2669, current loss: 30077.288108\n",
      "iteration 2670, current loss: 29278.495178\n",
      "iteration 2671, current loss: 29206.137335\n",
      "iteration 2672, current loss: 29004.950984\n",
      "iteration 2673, current loss: 29462.301217\n",
      "iteration 2674, current loss: 29229.976641\n",
      "iteration 2675, current loss: 29797.786768\n",
      "iteration 2676, current loss: 29490.894328\n",
      "iteration 2677, current loss: 29881.080425\n",
      "iteration 2678, current loss: 29606.464980\n",
      "iteration 2679, current loss: 29375.282559\n",
      "iteration 2680, current loss: 29601.971219\n",
      "iteration 2681, current loss: 29747.224289\n",
      "iteration 2682, current loss: 29619.016708\n",
      "iteration 2683, current loss: 29850.895816\n",
      "iteration 2684, current loss: 29861.475195\n",
      "iteration 2685, current loss: 30492.935460\n",
      "iteration 2686, current loss: 31055.476011\n",
      "iteration 2687, current loss: 31118.320161\n",
      "iteration 2688, current loss: 29997.676858\n",
      "iteration 2689, current loss: 29736.888498\n",
      "iteration 2690, current loss: 29795.637077\n",
      "iteration 2691, current loss: 29038.743753\n",
      "iteration 2692, current loss: 29044.820568\n",
      "iteration 2693, current loss: 28543.905525\n",
      "iteration 2694, current loss: 28489.718717\n",
      "iteration 2695, current loss: 28499.454987\n",
      "iteration 2696, current loss: 28602.165136\n",
      "iteration 2697, current loss: 28459.815383\n",
      "iteration 2698, current loss: 28882.100507\n",
      "iteration 2699, current loss: 28912.927566\n",
      "iteration 2700, current loss: 30317.644582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2701, current loss: 29633.256483\n",
      "iteration 2702, current loss: 30633.676297\n",
      "iteration 2703, current loss: 30215.892195\n",
      "iteration 2704, current loss: 30666.107894\n",
      "iteration 2705, current loss: 30840.083389\n",
      "iteration 2706, current loss: 30445.212736\n",
      "iteration 2707, current loss: 29580.825164\n",
      "iteration 2708, current loss: 29818.126033\n",
      "iteration 2709, current loss: 29915.585512\n",
      "iteration 2710, current loss: 29039.610811\n",
      "iteration 2711, current loss: 28841.090856\n",
      "iteration 2712, current loss: 28625.485799\n",
      "iteration 2713, current loss: 28285.347769\n",
      "iteration 2714, current loss: 28519.680105\n",
      "iteration 2715, current loss: 28325.612588\n",
      "iteration 2716, current loss: 28831.532388\n",
      "iteration 2717, current loss: 29009.026607\n",
      "iteration 2718, current loss: 29766.343455\n",
      "iteration 2719, current loss: 29085.483353\n",
      "iteration 2720, current loss: 29513.150058\n",
      "iteration 2721, current loss: 29735.607247\n",
      "iteration 2722, current loss: 30310.639903\n",
      "iteration 2723, current loss: 30785.018839\n",
      "iteration 2724, current loss: 29943.184335\n",
      "iteration 2725, current loss: 30226.942146\n",
      "iteration 2726, current loss: 29450.518910\n",
      "iteration 2727, current loss: 29355.119236\n",
      "iteration 2728, current loss: 28648.919558\n",
      "iteration 2729, current loss: 28773.881537\n",
      "iteration 2730, current loss: 28470.503256\n",
      "iteration 2731, current loss: 28583.389598\n",
      "iteration 2732, current loss: 28637.400913\n",
      "iteration 2733, current loss: 28702.597592\n",
      "iteration 2734, current loss: 28706.678080\n",
      "iteration 2735, current loss: 29246.937151\n",
      "iteration 2736, current loss: 29836.644919\n",
      "iteration 2737, current loss: 30595.161354\n",
      "iteration 2738, current loss: 30209.117770\n",
      "iteration 2739, current loss: 30750.099158\n",
      "iteration 2740, current loss: 29861.820568\n",
      "iteration 2741, current loss: 29749.834056\n",
      "iteration 2742, current loss: 28939.181790\n",
      "iteration 2743, current loss: 29285.510309\n",
      "iteration 2744, current loss: 29287.462403\n",
      "iteration 2745, current loss: 29584.852243\n",
      "iteration 2746, current loss: 30975.168936\n",
      "iteration 2747, current loss: 30439.214920\n",
      "iteration 2748, current loss: 30579.144554\n",
      "iteration 2749, current loss: 29841.112029\n",
      "iteration 2750, current loss: 30957.068517\n",
      "iteration 2751, current loss: 30655.545977\n",
      "iteration 2752, current loss: 30576.587367\n",
      "iteration 2753, current loss: 29485.870960\n",
      "iteration 2754, current loss: 29292.166080\n",
      "iteration 2755, current loss: 28600.728702\n",
      "iteration 2756, current loss: 28594.566853\n",
      "iteration 2757, current loss: 28689.091341\n",
      "iteration 2758, current loss: 29087.176273\n",
      "iteration 2759, current loss: 29266.251154\n",
      "iteration 2760, current loss: 30490.159773\n",
      "iteration 2761, current loss: 30583.236738\n",
      "iteration 2762, current loss: 31595.383531\n",
      "iteration 2763, current loss: 30063.953722\n",
      "iteration 2764, current loss: 30111.111778\n",
      "iteration 2765, current loss: 29225.606609\n",
      "iteration 2766, current loss: 29201.191792\n",
      "iteration 2767, current loss: 29354.945601\n",
      "iteration 2768, current loss: 28995.100606\n",
      "iteration 2769, current loss: 28902.846810\n",
      "iteration 2770, current loss: 28847.637941\n",
      "iteration 2771, current loss: 28742.002526\n",
      "iteration 2772, current loss: 29432.788584\n",
      "iteration 2773, current loss: 30069.079622\n",
      "iteration 2774, current loss: 30283.586399\n",
      "iteration 2775, current loss: 30439.620683\n",
      "iteration 2776, current loss: 30126.504665\n",
      "iteration 2777, current loss: 30061.633894\n",
      "iteration 2778, current loss: 29978.566498\n",
      "iteration 2779, current loss: 30322.332342\n",
      "iteration 2780, current loss: 29738.576602\n",
      "iteration 2781, current loss: 29611.226713\n",
      "iteration 2782, current loss: 29720.199260\n",
      "iteration 2783, current loss: 30113.851364\n",
      "iteration 2784, current loss: 30391.650445\n",
      "iteration 2785, current loss: 29186.421189\n",
      "iteration 2786, current loss: 29113.897979\n",
      "iteration 2787, current loss: 28519.019624\n",
      "iteration 2788, current loss: 28943.215945\n",
      "iteration 2789, current loss: 28656.361300\n",
      "iteration 2790, current loss: 29275.084544\n",
      "iteration 2791, current loss: 29248.953287\n",
      "iteration 2792, current loss: 30575.743695\n",
      "iteration 2793, current loss: 29668.056021\n",
      "iteration 2794, current loss: 29770.712404\n",
      "iteration 2795, current loss: 28810.088499\n",
      "iteration 2796, current loss: 28662.148278\n",
      "iteration 2797, current loss: 28651.022257\n",
      "iteration 2798, current loss: 29061.912854\n",
      "iteration 2799, current loss: 28937.036679\n",
      "iteration 2800, current loss: 29513.097033\n",
      "iteration 2801, current loss: 28862.077423\n",
      "iteration 2802, current loss: 28904.452383\n",
      "iteration 2803, current loss: 28969.884391\n",
      "iteration 2804, current loss: 29420.941142\n",
      "iteration 2805, current loss: 29184.263683\n",
      "iteration 2806, current loss: 29588.102773\n",
      "iteration 2807, current loss: 29096.447140\n",
      "iteration 2808, current loss: 29219.554451\n",
      "iteration 2809, current loss: 29375.478421\n",
      "iteration 2810, current loss: 29761.739749\n",
      "iteration 2811, current loss: 29691.606092\n",
      "iteration 2812, current loss: 29247.708640\n",
      "iteration 2813, current loss: 29319.753256\n",
      "iteration 2814, current loss: 28909.050317\n",
      "iteration 2815, current loss: 28871.000327\n",
      "iteration 2816, current loss: 28281.240080\n",
      "iteration 2817, current loss: 28100.080957\n",
      "iteration 2818, current loss: 28138.837839\n",
      "iteration 2819, current loss: 28250.888529\n",
      "iteration 2820, current loss: 28569.604227\n",
      "iteration 2821, current loss: 29012.562638\n",
      "iteration 2822, current loss: 29001.837916\n",
      "iteration 2823, current loss: 30025.734070\n",
      "iteration 2824, current loss: 29113.282606\n",
      "iteration 2825, current loss: 30414.153918\n",
      "iteration 2826, current loss: 31736.152895\n",
      "iteration 2827, current loss: 31764.141132\n",
      "iteration 2828, current loss: 30630.801894\n",
      "iteration 2829, current loss: 30479.257072\n",
      "iteration 2830, current loss: 28786.665128\n",
      "iteration 2831, current loss: 28482.982635\n",
      "iteration 2832, current loss: 28206.223756\n",
      "iteration 2833, current loss: 28402.983662\n",
      "iteration 2834, current loss: 28302.368293\n",
      "iteration 2835, current loss: 28871.622098\n",
      "iteration 2836, current loss: 29044.942512\n",
      "iteration 2837, current loss: 29756.994546\n",
      "iteration 2838, current loss: 30955.987220\n",
      "iteration 2839, current loss: 30700.105747\n",
      "iteration 2840, current loss: 30899.987414\n",
      "iteration 2841, current loss: 30041.094272\n",
      "iteration 2842, current loss: 29584.026111\n",
      "iteration 2843, current loss: 29556.439544\n",
      "iteration 2844, current loss: 28837.450069\n",
      "iteration 2845, current loss: 28593.888574\n",
      "iteration 2846, current loss: 28767.703344\n",
      "iteration 2847, current loss: 28417.961657\n",
      "iteration 2848, current loss: 28759.048542\n",
      "iteration 2849, current loss: 29151.215473\n",
      "iteration 2850, current loss: 29199.380544\n",
      "iteration 2851, current loss: 29308.872305\n",
      "iteration 2852, current loss: 29930.688146\n",
      "iteration 2853, current loss: 28923.559998\n",
      "iteration 2854, current loss: 28492.791322\n",
      "iteration 2855, current loss: 28340.393924\n",
      "iteration 2856, current loss: 28790.593789\n",
      "iteration 2857, current loss: 29850.253025\n",
      "iteration 2858, current loss: 29111.916685\n",
      "iteration 2859, current loss: 29165.997654\n",
      "iteration 2860, current loss: 28345.445368\n",
      "iteration 2861, current loss: 28673.006786\n",
      "iteration 2862, current loss: 29519.431132\n",
      "iteration 2863, current loss: 29388.811572\n",
      "iteration 2864, current loss: 28646.873735\n",
      "iteration 2865, current loss: 28679.680549\n",
      "iteration 2866, current loss: 28863.271278\n",
      "iteration 2867, current loss: 28614.572558\n",
      "iteration 2868, current loss: 28502.434135\n",
      "iteration 2869, current loss: 28490.910956\n",
      "iteration 2870, current loss: 28501.578830\n",
      "iteration 2871, current loss: 28217.561308\n",
      "iteration 2872, current loss: 28651.676277\n",
      "iteration 2873, current loss: 28348.112850\n",
      "iteration 2874, current loss: 28561.693394\n",
      "iteration 2875, current loss: 28084.344575\n",
      "iteration 2876, current loss: 28143.740448\n",
      "iteration 2877, current loss: 27962.664181\n",
      "iteration 2878, current loss: 28062.016938\n",
      "iteration 2879, current loss: 28688.916982\n",
      "iteration 2880, current loss: 28461.874642\n",
      "iteration 2881, current loss: 29772.927064\n",
      "iteration 2882, current loss: 29462.019003\n",
      "iteration 2883, current loss: 31026.375445\n",
      "iteration 2884, current loss: 29046.358118\n",
      "iteration 2885, current loss: 29129.075695\n",
      "iteration 2886, current loss: 28819.592519\n",
      "iteration 2887, current loss: 29269.642358\n",
      "iteration 2888, current loss: 29134.242253\n",
      "iteration 2889, current loss: 29545.904320\n",
      "iteration 2890, current loss: 29587.425708\n",
      "iteration 2891, current loss: 29764.589575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2892, current loss: 30420.318376\n",
      "iteration 2893, current loss: 30788.433221\n",
      "iteration 2894, current loss: 29709.663230\n",
      "iteration 2895, current loss: 29862.251984\n",
      "iteration 2896, current loss: 29363.114262\n",
      "iteration 2897, current loss: 28956.513912\n",
      "iteration 2898, current loss: 28673.897430\n",
      "iteration 2899, current loss: 28433.610736\n",
      "iteration 2900, current loss: 28934.919564\n",
      "iteration 2901, current loss: 28773.924469\n",
      "iteration 2902, current loss: 29228.132994\n",
      "iteration 2903, current loss: 29009.868773\n",
      "iteration 2904, current loss: 28728.139517\n",
      "iteration 2905, current loss: 29154.157127\n",
      "iteration 2906, current loss: 29731.593055\n",
      "iteration 2907, current loss: 29444.910555\n",
      "iteration 2908, current loss: 28910.109570\n",
      "iteration 2909, current loss: 28642.019534\n",
      "iteration 2910, current loss: 28507.123606\n",
      "iteration 2911, current loss: 29023.555871\n",
      "iteration 2912, current loss: 28673.095800\n",
      "iteration 2913, current loss: 29124.366547\n",
      "iteration 2914, current loss: 28611.092239\n",
      "iteration 2915, current loss: 28926.245764\n",
      "iteration 2916, current loss: 28295.782826\n",
      "iteration 2917, current loss: 29288.750708\n",
      "iteration 2918, current loss: 28137.697338\n",
      "iteration 2919, current loss: 28394.817049\n",
      "iteration 2920, current loss: 28041.032167\n",
      "iteration 2921, current loss: 28892.568784\n",
      "iteration 2922, current loss: 28970.583724\n",
      "iteration 2923, current loss: 29627.948719\n",
      "iteration 2924, current loss: 29871.860095\n",
      "iteration 2925, current loss: 30011.708522\n",
      "iteration 2926, current loss: 28670.341664\n",
      "iteration 2927, current loss: 28400.127170\n",
      "iteration 2928, current loss: 28399.351007\n",
      "iteration 2929, current loss: 28218.403532\n",
      "iteration 2930, current loss: 28463.613907\n",
      "iteration 2931, current loss: 28361.565856\n",
      "iteration 2932, current loss: 29021.115941\n",
      "iteration 2933, current loss: 28526.988950\n",
      "iteration 2934, current loss: 28740.266785\n",
      "iteration 2935, current loss: 28821.073651\n",
      "iteration 2936, current loss: 28942.345712\n",
      "iteration 2937, current loss: 29080.646069\n",
      "iteration 2938, current loss: 29089.570460\n",
      "iteration 2939, current loss: 28906.978578\n",
      "iteration 2940, current loss: 29328.755262\n",
      "iteration 2941, current loss: 28464.837552\n",
      "iteration 2942, current loss: 29049.687726\n",
      "iteration 2943, current loss: 28295.998665\n",
      "iteration 2944, current loss: 28818.407247\n",
      "iteration 2945, current loss: 28575.407233\n",
      "iteration 2946, current loss: 29208.780470\n",
      "iteration 2947, current loss: 29522.921599\n",
      "iteration 2948, current loss: 29860.862165\n",
      "iteration 2949, current loss: 30424.755483\n",
      "iteration 2950, current loss: 30967.214764\n",
      "iteration 2951, current loss: 29851.590039\n",
      "iteration 2952, current loss: 29546.621274\n",
      "iteration 2953, current loss: 29761.717180\n",
      "iteration 2954, current loss: 29301.524301\n",
      "iteration 2955, current loss: 28859.601382\n",
      "iteration 2956, current loss: 28707.873258\n",
      "iteration 2957, current loss: 28111.472466\n",
      "iteration 2958, current loss: 28172.958778\n",
      "iteration 2959, current loss: 28205.333628\n",
      "iteration 2960, current loss: 28456.828086\n",
      "iteration 2961, current loss: 28390.303426\n",
      "iteration 2962, current loss: 29074.417599\n",
      "iteration 2963, current loss: 28756.236282\n",
      "iteration 2964, current loss: 28703.671291\n",
      "iteration 2965, current loss: 28534.945028\n",
      "iteration 2966, current loss: 28529.530651\n",
      "iteration 2967, current loss: 28163.293418\n",
      "iteration 2968, current loss: 27918.381872\n",
      "iteration 2969, current loss: 27902.615128\n",
      "iteration 2970, current loss: 27682.703571\n",
      "iteration 2971, current loss: 27642.006087\n",
      "iteration 2972, current loss: 27604.502618\n",
      "iteration 2973, current loss: 28071.772034\n",
      "iteration 2974, current loss: 28624.163022\n",
      "iteration 2975, current loss: 28531.520181\n",
      "iteration 2976, current loss: 29244.555721\n",
      "iteration 2977, current loss: 28816.714513\n",
      "iteration 2978, current loss: 28968.192385\n",
      "iteration 2979, current loss: 29042.715128\n",
      "iteration 2980, current loss: 29261.639770\n",
      "iteration 2981, current loss: 30776.765748\n",
      "iteration 2982, current loss: 30507.892828\n",
      "iteration 2983, current loss: 31272.205485\n",
      "iteration 2984, current loss: 28831.490354\n",
      "iteration 2985, current loss: 28384.099437\n",
      "iteration 2986, current loss: 27680.646959\n",
      "iteration 2987, current loss: 27690.734960\n",
      "iteration 2988, current loss: 27419.336568\n",
      "iteration 2989, current loss: 27624.524624\n",
      "iteration 2990, current loss: 27600.024377\n",
      "iteration 2991, current loss: 27981.238124\n",
      "iteration 2992, current loss: 28110.871515\n",
      "iteration 2993, current loss: 29068.240994\n",
      "iteration 2994, current loss: 29507.382425\n",
      "iteration 2995, current loss: 30758.748216\n",
      "iteration 2996, current loss: 31446.962290\n",
      "iteration 2997, current loss: 30342.702387\n",
      "iteration 2998, current loss: 30224.507726\n",
      "iteration 2999, current loss: 29960.096215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2576c3386d8>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(3000, model)\n",
    "costs = model['costs']\n",
    "plt.plot(costs[-3000:])\n",
    "#print(W[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1da3fa6ef98>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs[-3000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3, Inspect Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pred, real):\n",
    "    right = np.sum(pred==real)\n",
    "    acc = right/real.shape[1]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy: 0.884841\n",
      "test set accuracy: 0.842381\n"
     ]
    }
   ],
   "source": [
    "predict = model['predict']\n",
    "pred = predict(model, train_X)\n",
    "pred = Y_to_labels(pred)\n",
    "acc = get_accuracy(pred, train_labels)\n",
    "print(\"training set accuracy: %f\" % acc)\n",
    "pred = predict(model, test_X)\n",
    "pred = Y_to_labels(pred)\n",
    "acc = get_accuracy(pred, test_labels)\n",
    "print(\"test set accuracy: %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"W-four-layer\", *W)\n",
    "np.savez(\"b-four-layer\", *b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 28000)\n"
     ]
    }
   ],
   "source": [
    "test_X = np.genfromtxt(\"test.csv\", delimiter=',', skip_header=1)\n",
    "test_X = test_X.T\n",
    "test_X = test_X/255\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28000)\n"
     ]
    }
   ],
   "source": [
    "test_Y = predict(test_X)\n",
    "test_labels = Y_to_labels(test_Y)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.zeros((test_labels.shape[1], 2), dtype=int)\n",
    "for i in range(output.shape[0]):\n",
    "    output[i, 0] = i+1\n",
    "    output[i, 1] = test_labels[0, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"4layer-submission.csv\", output, fmt=\"%d\", delimiter=',', header='ImageId,Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17346840320>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_images = X_to_images(test_X)\n",
    "plt.imshow(test_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
