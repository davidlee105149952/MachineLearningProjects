{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digits Recognition Using Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data comes from the \"MNIST\" data set, you can download it from [Kaggle](https://www.kaggle.com/c/digit-recognizer/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1, Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"train.csv\", delimiter=',', skip_header=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "[2. 7. 7. 9. 4. 5. 7. 2. 7. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "print(data.shape)\n",
    "print(data[:10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 42000)\n",
      "(1, 42000)\n"
     ]
    }
   ],
   "source": [
    "features = data[:, 1:].T\n",
    "labels = data[:, 0]\n",
    "labels = np.reshape(labels, (1, -1))\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = features.shape[1]\n",
    "nx = features.shape[0]\n",
    "ny = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_Y(labels):\n",
    "    Y = np.zeros((ny, m))\n",
    "    for i in range(m):\n",
    "        Y[int(labels[0, i]), i] = 1\n",
    "    return Y\n",
    "\n",
    "def Y_to_labels(Y):\n",
    "    labels = np.argmax(Y, axis=0).astype(float)\n",
    "    labels = np.reshape(labels, (1, -1))\n",
    "    return labels\n",
    "\n",
    "def X_to_images(X):\n",
    "    images = [np.reshape(X[:, i], (28, 28)) for i in range(X.shape[1])]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 42000)\n",
      "2.0\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = features / 255\n",
    "Y = labels_to_Y(labels)\n",
    "images = X_to_images(X)\n",
    "print(Y.shape)\n",
    "plt.imshow(images[0])\n",
    "print(labels[0, 0])\n",
    "print(Y[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 37800)\n",
      "(10, 37800)\n",
      "(784, 4200)\n",
      "(10, 4200)\n",
      "(1, 37800)\n"
     ]
    }
   ],
   "source": [
    "test_ratio = 1 - train_ratio\n",
    "train_m = int(m * train_ratio)\n",
    "test_m = m - train_m\n",
    "train_X = X[:, :train_m]\n",
    "test_X = X[:, train_m:]\n",
    "train_Y = Y[:, :train_m]\n",
    "test_Y = Y[:, train_m:]\n",
    "train_labels = Y_to_labels(train_Y)\n",
    "test_labels = Y_to_labels(test_Y)\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2, Design Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    a = 1/ (1 + np.exp(-z))\n",
    "    return a\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    p1 = np.exp(-z)\n",
    "    a = p1/((1+p1)**2)\n",
    "    return a\n",
    "\n",
    "def relu(z):\n",
    "    a = np.maximum(z, 0.01*z)\n",
    "    return a\n",
    "\n",
    "def relu_prime(z):\n",
    "    a = np.where(z > 0, 1, 0.01)\n",
    "    return a\n",
    "\n",
    "def tanh(z):\n",
    "    p1 = np.exp(z)\n",
    "    p2 = np.exp(-z)\n",
    "    a = (p1-p2)/(p1+p2)\n",
    "    return a\n",
    "    \n",
    "def tanh_prime(z):\n",
    "    p1 = tanh(z)\n",
    "    a = 1-p1**2\n",
    "    return a\n",
    "\n",
    "def softmax(z):\n",
    "    t = np.exp(z)\n",
    "    n = np.sum(t, axis = 0)\n",
    "    a = t/n\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_for__config_multi_layer_classifier_without_regularization(Y_hat, Y):\n",
    "    delta = 1e-10\n",
    "    l = np.sum(-Y*np.log(Y_hat+delta), axis=0)\n",
    "    return l\n",
    "\n",
    "def forward_propagation_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    X = model['X']\n",
    "    Y = model['Y']\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f = model['f']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    costs = model['costs']\n",
    "    loss_function = model['loss_function']\n",
    "    A[0] = X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    costs.append(np.sum(loss_function(A[L], Y)))\n",
    "\n",
    "def back_propagation_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f_prime = model['f_prime']\n",
    "    m = model['m']\n",
    "    W = model['W']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    Y = model['Y']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    dA = model['dA']\n",
    "    dZ = model['dZ']\n",
    "    dZ[L] = A[L] - Y\n",
    "    dW[L] = np.matmul(dZ[L], A[L-1].T)/m\n",
    "    db[L] = np.sum(dZ[L], axis=1, keepdims=True)/m\n",
    "    for i in reversed(range(1, L)):\n",
    "        dZ[i] = np.matmul(W[i+1].T, dZ[i+1]) * f_prime[i](Z[i])\n",
    "        dW[i] = np.matmul(dZ[i], A[i-1].T)/m\n",
    "        db[i] = np.sum(dZ[i], axis=1, keepdims=True)/m\n",
    "        \n",
    "def update_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    L = model['L']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    learning_rate = model['learning_rate']\n",
    "    for i in range(1, L+1):\n",
    "        W[i] -= learning_rate*dW[i]\n",
    "        b[i] -= learning_rate*db[i]\n",
    "    \n",
    "    \n",
    "def predict_for_config_multi_layer_classifier_without_regularization(model, test_X):\n",
    "    L = model['L']\n",
    "    A = model['A']\n",
    "    Z = model['Z']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    f = model['f']\n",
    "    A[0] = test_X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    return A[L]\n",
    "    \n",
    "    \n",
    "def config_multi_layer_classifier_without_regularization(X, Y, neuron_of_hidden_layer, learning_rate):\n",
    "    n_input_layer = X.shape[0]\n",
    "    n_output_layer = Y.shape[0]\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = [n_input_layer] + neuron_of_hidden_layer + [n_output_layer]\n",
    "    L = len(n)-1\n",
    "    f = [tanh if i != 0 else None for i in range(L)]\n",
    "    f.append(softmax)\n",
    "    f_prime = [tanh_prime if i != 0 else None for i in range(L)]\n",
    "    f_prime.append(None)\n",
    "    W = [np.random.normal(0, 1, (n[i], n[i-1])) if i != 0 else None for i in range(L+1)]\n",
    "    b = [np.random.normal(0, 1, (n[i], 1)) if i != 0 else None for i in range(L+1)]\n",
    "    Z = [None for i in range(L+1)]\n",
    "    A = [None for i in range(L+1)]\n",
    "    dW = [None for i in range(L+1)]\n",
    "    db = [None for i in range(L+1)]\n",
    "    dA = [None for i in range(L+1)]\n",
    "    dZ = [None for i in range(L+1)]\n",
    "    costs = []\n",
    "    \n",
    "    model = dict()\n",
    "    model['X'] = X\n",
    "    model['Y'] = Y\n",
    "    model['m'] = m\n",
    "    model['n'] = n\n",
    "    model['L'] = L\n",
    "    model['f'] = f\n",
    "    model['f_prime'] = f_prime\n",
    "    model['W'] = W\n",
    "    model['b'] = b\n",
    "    model['Z'] = Z\n",
    "    model['A'] = A\n",
    "    model['dW'] = dW\n",
    "    model['db'] = db\n",
    "    model['dA'] = dA\n",
    "    model['dZ'] = dZ\n",
    "    model['loss_function'] = loss_function_for__config_multi_layer_classifier_without_regularization\n",
    "    model['costs'] = costs\n",
    "    model['learning_rate'] = learning_rate\n",
    "    model['forwardprop'] = forward_propagation_for_config_multi_layer_classifier_without_regularization\n",
    "    model['backprop'] = back_propagation_for_config_multi_layer_classifier_without_regularization\n",
    "    model['update'] = update_for_config_multi_layer_classifier_without_regularization\n",
    "    model['predict'] = predict_for_config_multi_layer_classifier_without_regularization\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_for__config_multi_layer_classifier_with_regularization(model, Y_hat):\n",
    "    n = model['n']\n",
    "    Y = model['Y']\n",
    "    W = model['W']\n",
    "    L = model['L']\n",
    "    m = model['m']\n",
    "    penalty = model['penalty']\n",
    "    \n",
    "    delta = 1e-10\n",
    "    l = np.sum(-Y*np.log(Y_hat+delta), axis=0)\n",
    "    p = 0\n",
    "    for i in range(1, L+1):\n",
    "        p = p + np.sum(W[i]**2)*penalty/(2*m)\n",
    "    l = l + p\n",
    "    return l\n",
    "\n",
    "def forward_propagation_for_config_multi_layer_classifier_with_regularization(model):\n",
    "    X = model['X']\n",
    "    Y = model['Y']\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f = model['f']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    costs = model['costs']\n",
    "    loss_function = model['loss_function']\n",
    "    A[0] = X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    costs.append(np.sum(loss_function(model, A[L])))\n",
    "\n",
    "def back_propagation_for_config_multi_layer_classifier_with_regularization(model):\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f_prime = model['f_prime']\n",
    "    m = model['m']\n",
    "    W = model['W']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    Y = model['Y']\n",
    "    penalty = model['penalty']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    dA = model['dA']\n",
    "    dZ = model['dZ']\n",
    "    dZ[L] = A[L] - Y\n",
    "    dW[L] = np.matmul(dZ[L], A[L-1].T)/m\n",
    "    db[L] = np.sum(dZ[L], axis=1, keepdims=True)/m\n",
    "    for i in reversed(range(1, L)):\n",
    "        dZ[i] = np.matmul(W[i+1].T, dZ[i+1]) * f_prime[i](Z[i])\n",
    "        dW[i] = np.matmul(dZ[i], A[i-1].T)/m + W[i]*penalty/m\n",
    "        db[i] = np.sum(dZ[i], axis=1, keepdims=True)/m\n",
    "        \n",
    "def update_for_config_multi_layer_classifier_with_regularization(model):\n",
    "    L = model['L']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    learning_rate = model['learning_rate']\n",
    "    for i in range(1, L+1):\n",
    "        W[i] -= learning_rate*dW[i]\n",
    "        b[i] -= learning_rate*db[i]\n",
    "    \n",
    "    \n",
    "def predict_for_config_multi_layer_classifier_with_regularization(model, test_X):\n",
    "    L = model['L']\n",
    "    A = model['A']\n",
    "    Z = model['Z']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    f = model['f']\n",
    "    A[0] = test_X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    return A[L]\n",
    "    \n",
    "    \n",
    "def config_multi_layer_classifier_with_regularization(X, Y, neuron_of_hidden_layer, learning_rate, penalty):\n",
    "    n_input_layer = X.shape[0]\n",
    "    n_output_layer = Y.shape[0]\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = [n_input_layer] + neuron_of_hidden_layer + [n_output_layer]\n",
    "    L = len(n)-1\n",
    "    f = [tanh if i != 0 else None for i in range(L)]\n",
    "    f.append(softmax)\n",
    "    f_prime = [tanh_prime if i != 0 else None for i in range(L)]\n",
    "    f_prime.append(None)\n",
    "    W = [np.random.normal(0, 1, (n[i], n[i-1])) if i != 0 else None for i in range(L+1)]\n",
    "    b = [np.random.normal(0, 1, (n[i], 1)) if i != 0 else None for i in range(L+1)]\n",
    "    Z = [None for i in range(L+1)]\n",
    "    A = [None for i in range(L+1)]\n",
    "    dW = [None for i in range(L+1)]\n",
    "    db = [None for i in range(L+1)]\n",
    "    dA = [None for i in range(L+1)]\n",
    "    dZ = [None for i in range(L+1)]\n",
    "    costs = []\n",
    "    \n",
    "    model = dict()\n",
    "    model['X'] = X\n",
    "    model['Y'] = Y\n",
    "    model['m'] = m\n",
    "    model['n'] = n\n",
    "    model['L'] = L\n",
    "    model['f'] = f\n",
    "    model['f_prime'] = f_prime\n",
    "    model['W'] = W\n",
    "    model['b'] = b\n",
    "    model['Z'] = Z\n",
    "    model['A'] = A\n",
    "    model['dW'] = dW\n",
    "    model['db'] = db\n",
    "    model['dA'] = dA\n",
    "    model['dZ'] = dZ\n",
    "    model['loss_function'] = loss_function_for__config_multi_layer_classifier_with_regularization\n",
    "    model['costs'] = costs\n",
    "    model['learning_rate'] = learning_rate\n",
    "    model['penalty'] = penalty\n",
    "    model['forwardprop'] = forward_propagation_for_config_multi_layer_classifier_with_regularization\n",
    "    model['backprop'] = back_propagation_for_config_multi_layer_classifier_with_regularization\n",
    "    model['update'] = update_for_config_multi_layer_classifier_with_regularization\n",
    "    model['predict'] = predict_for_config_multi_layer_classifier_with_regularization\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(iteration_times, model):\n",
    "    forwardprop = model['forwardprop']\n",
    "    backprop = model['backprop']\n",
    "    update = model['update']\n",
    "    costs = model['costs']\n",
    "    for i in range(iteration_times):\n",
    "        forwardprop(model)\n",
    "        backprop(model)\n",
    "        update(model)\n",
    "        print(\"iteration %d, current loss: %f\" % (i, costs[len(costs)-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = config_multi_layer_classifier_with_regularization(train_X, train_Y, [28, 28, 28, 28], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, current loss: 78715.264597\n",
      "iteration 1, current loss: 78236.464794\n",
      "iteration 2, current loss: 77600.507393\n",
      "iteration 3, current loss: 77057.673517\n",
      "iteration 4, current loss: 76360.385192\n",
      "iteration 5, current loss: 75914.287180\n",
      "iteration 6, current loss: 75421.685645\n",
      "iteration 7, current loss: 75084.391366\n",
      "iteration 8, current loss: 74559.457360\n",
      "iteration 9, current loss: 74039.086338\n",
      "iteration 10, current loss: 73493.296201\n",
      "iteration 11, current loss: 72970.476781\n",
      "iteration 12, current loss: 72609.322378\n",
      "iteration 13, current loss: 72172.417691\n",
      "iteration 14, current loss: 71950.954370\n",
      "iteration 15, current loss: 71562.467286\n",
      "iteration 16, current loss: 71446.261086\n",
      "iteration 17, current loss: 71150.341077\n",
      "iteration 18, current loss: 71167.083730\n",
      "iteration 19, current loss: 70698.907756\n",
      "iteration 20, current loss: 70551.464949\n",
      "iteration 21, current loss: 70103.605797\n",
      "iteration 22, current loss: 70023.459772\n",
      "iteration 23, current loss: 69532.539083\n",
      "iteration 24, current loss: 69359.910975\n",
      "iteration 25, current loss: 68986.531362\n",
      "iteration 26, current loss: 68936.608797\n",
      "iteration 27, current loss: 68510.300796\n",
      "iteration 28, current loss: 68271.991494\n",
      "iteration 29, current loss: 67911.787045\n",
      "iteration 30, current loss: 67621.750826\n",
      "iteration 31, current loss: 67297.059054\n",
      "iteration 32, current loss: 67016.664348\n",
      "iteration 33, current loss: 66770.414640\n",
      "iteration 34, current loss: 66488.317219\n",
      "iteration 35, current loss: 66287.237933\n",
      "iteration 36, current loss: 66006.496551\n",
      "iteration 37, current loss: 65876.923808\n",
      "iteration 38, current loss: 65521.273688\n",
      "iteration 39, current loss: 65347.485719\n",
      "iteration 40, current loss: 64891.779813\n",
      "iteration 41, current loss: 64697.942885\n",
      "iteration 42, current loss: 64265.287827\n",
      "iteration 43, current loss: 64107.518842\n",
      "iteration 44, current loss: 63707.279302\n",
      "iteration 45, current loss: 63589.866049\n",
      "iteration 46, current loss: 63225.753021\n",
      "iteration 47, current loss: 63081.609391\n",
      "iteration 48, current loss: 62850.808228\n",
      "iteration 49, current loss: 62763.145021\n",
      "iteration 50, current loss: 62647.552523\n",
      "iteration 51, current loss: 62487.419909\n",
      "iteration 52, current loss: 62338.374782\n",
      "iteration 53, current loss: 62191.891558\n",
      "iteration 54, current loss: 61994.365180\n",
      "iteration 55, current loss: 61798.841633\n",
      "iteration 56, current loss: 61641.951306\n",
      "iteration 57, current loss: 61485.871192\n",
      "iteration 58, current loss: 61477.798187\n",
      "iteration 59, current loss: 61300.256927\n",
      "iteration 60, current loss: 61244.505237\n",
      "iteration 61, current loss: 61091.999556\n",
      "iteration 62, current loss: 61025.492672\n",
      "iteration 63, current loss: 60780.576510\n",
      "iteration 64, current loss: 60662.293754\n",
      "iteration 65, current loss: 60330.752711\n",
      "iteration 66, current loss: 60214.488812\n",
      "iteration 67, current loss: 59882.366266\n",
      "iteration 68, current loss: 59779.800475\n",
      "iteration 69, current loss: 59517.405839\n",
      "iteration 70, current loss: 59419.152647\n",
      "iteration 71, current loss: 59221.271403\n",
      "iteration 72, current loss: 59124.850381\n",
      "iteration 73, current loss: 58974.248663\n",
      "iteration 74, current loss: 58911.200087\n",
      "iteration 75, current loss: 58824.614866\n",
      "iteration 76, current loss: 58832.307816\n",
      "iteration 77, current loss: 58732.165592\n",
      "iteration 78, current loss: 58677.277985\n",
      "iteration 79, current loss: 58549.543177\n",
      "iteration 80, current loss: 58448.148169\n",
      "iteration 81, current loss: 58282.871214\n",
      "iteration 82, current loss: 58146.285763\n",
      "iteration 83, current loss: 57906.221171\n",
      "iteration 84, current loss: 57778.488308\n",
      "iteration 85, current loss: 57543.767345\n",
      "iteration 86, current loss: 57463.504548\n",
      "iteration 87, current loss: 57277.445992\n",
      "iteration 88, current loss: 57275.589789\n",
      "iteration 89, current loss: 57144.878301\n",
      "iteration 90, current loss: 57277.272136\n",
      "iteration 91, current loss: 57075.129895\n",
      "iteration 92, current loss: 57196.802308\n",
      "iteration 93, current loss: 56902.485518\n",
      "iteration 94, current loss: 57040.742489\n",
      "iteration 95, current loss: 56686.877707\n",
      "iteration 96, current loss: 56776.818085\n",
      "iteration 97, current loss: 56513.982532\n",
      "iteration 98, current loss: 56561.496184\n",
      "iteration 99, current loss: 56407.130516\n",
      "iteration 100, current loss: 56372.159885\n",
      "iteration 101, current loss: 56349.870707\n",
      "iteration 102, current loss: 56131.632188\n",
      "iteration 103, current loss: 56151.597399\n",
      "iteration 104, current loss: 55846.998451\n",
      "iteration 105, current loss: 55864.661695\n",
      "iteration 106, current loss: 55586.865010\n",
      "iteration 107, current loss: 55651.245413\n",
      "iteration 108, current loss: 55346.867164\n",
      "iteration 109, current loss: 55413.234319\n",
      "iteration 110, current loss: 55125.533729\n",
      "iteration 111, current loss: 55189.549038\n",
      "iteration 112, current loss: 54935.140947\n",
      "iteration 113, current loss: 55019.827178\n",
      "iteration 114, current loss: 54786.271339\n",
      "iteration 115, current loss: 54897.169821\n",
      "iteration 116, current loss: 54674.525003\n",
      "iteration 117, current loss: 54815.634888\n",
      "iteration 118, current loss: 54635.961215\n",
      "iteration 119, current loss: 54690.683882\n",
      "iteration 120, current loss: 54459.399119\n",
      "iteration 121, current loss: 54446.330992\n",
      "iteration 122, current loss: 54219.361797\n",
      "iteration 123, current loss: 54173.822559\n",
      "iteration 124, current loss: 53987.733671\n",
      "iteration 125, current loss: 53965.410082\n",
      "iteration 126, current loss: 53810.176348\n",
      "iteration 127, current loss: 53822.107114\n",
      "iteration 128, current loss: 53664.605456\n",
      "iteration 129, current loss: 53702.183789\n",
      "iteration 130, current loss: 53547.752620\n",
      "iteration 131, current loss: 53610.309913\n",
      "iteration 132, current loss: 53543.033105\n",
      "iteration 133, current loss: 53616.519323\n",
      "iteration 134, current loss: 53570.816287\n",
      "iteration 135, current loss: 53690.411606\n",
      "iteration 136, current loss: 53542.734801\n",
      "iteration 137, current loss: 53596.399517\n",
      "iteration 138, current loss: 53333.692547\n",
      "iteration 139, current loss: 53397.172406\n",
      "iteration 140, current loss: 53111.152155\n",
      "iteration 141, current loss: 53169.517949\n",
      "iteration 142, current loss: 52892.539567\n",
      "iteration 143, current loss: 52947.524393\n",
      "iteration 144, current loss: 52739.912593\n",
      "iteration 145, current loss: 52815.333171\n",
      "iteration 146, current loss: 52637.060289\n",
      "iteration 147, current loss: 52723.932824\n",
      "iteration 148, current loss: 52580.668728\n",
      "iteration 149, current loss: 52644.157490\n",
      "iteration 150, current loss: 52489.503664\n",
      "iteration 151, current loss: 52525.432001\n",
      "iteration 152, current loss: 52360.053891\n",
      "iteration 153, current loss: 52387.058200\n",
      "iteration 154, current loss: 52214.222768\n",
      "iteration 155, current loss: 52220.447082\n",
      "iteration 156, current loss: 52034.801183\n",
      "iteration 157, current loss: 52034.570517\n",
      "iteration 158, current loss: 51872.094351\n",
      "iteration 159, current loss: 51869.604148\n",
      "iteration 160, current loss: 51726.694659\n",
      "iteration 161, current loss: 51729.383566\n",
      "iteration 162, current loss: 51573.456001\n",
      "iteration 163, current loss: 51591.410287\n",
      "iteration 164, current loss: 51415.006284\n",
      "iteration 165, current loss: 51439.825716\n",
      "iteration 166, current loss: 51298.991638\n",
      "iteration 167, current loss: 51319.713432\n",
      "iteration 168, current loss: 51194.093176\n",
      "iteration 169, current loss: 51120.278904\n",
      "iteration 170, current loss: 50994.250130\n",
      "iteration 171, current loss: 50943.055622\n",
      "iteration 172, current loss: 50830.673211\n",
      "iteration 173, current loss: 50767.298066\n",
      "iteration 174, current loss: 50662.619193\n",
      "iteration 175, current loss: 50607.583836\n",
      "iteration 176, current loss: 50510.123006\n",
      "iteration 177, current loss: 50473.019009\n",
      "iteration 178, current loss: 50386.793458\n",
      "iteration 179, current loss: 50382.646591\n",
      "iteration 180, current loss: 50325.732223\n",
      "iteration 181, current loss: 50342.943361\n",
      "iteration 182, current loss: 50316.645399\n",
      "iteration 183, current loss: 50312.176705\n",
      "iteration 184, current loss: 50271.620703\n",
      "iteration 185, current loss: 50291.503093\n",
      "iteration 186, current loss: 50272.128523\n",
      "iteration 187, current loss: 50305.472727\n",
      "iteration 188, current loss: 50161.823961\n",
      "iteration 189, current loss: 50269.387339\n",
      "iteration 190, current loss: 50033.481886\n",
      "iteration 191, current loss: 50151.745848\n",
      "iteration 192, current loss: 49901.477290\n",
      "iteration 193, current loss: 50004.799385\n",
      "iteration 194, current loss: 49759.802333\n",
      "iteration 195, current loss: 49825.361347\n",
      "iteration 196, current loss: 49579.404491\n",
      "iteration 197, current loss: 49643.439350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 198, current loss: 49399.237469\n",
      "iteration 199, current loss: 49487.436954\n",
      "iteration 200, current loss: 49284.147189\n",
      "iteration 201, current loss: 49399.958856\n",
      "iteration 202, current loss: 49206.000478\n",
      "iteration 203, current loss: 49328.718602\n",
      "iteration 204, current loss: 49151.348685\n",
      "iteration 205, current loss: 49292.173416\n",
      "iteration 206, current loss: 49161.671335\n",
      "iteration 207, current loss: 49271.662100\n",
      "iteration 208, current loss: 49012.204830\n",
      "iteration 209, current loss: 49049.919809\n",
      "iteration 210, current loss: 48887.654391\n",
      "iteration 211, current loss: 48964.406838\n",
      "iteration 212, current loss: 48735.377581\n",
      "iteration 213, current loss: 48787.756244\n",
      "iteration 214, current loss: 48615.230958\n",
      "iteration 215, current loss: 48682.398640\n",
      "iteration 216, current loss: 48490.410645\n",
      "iteration 217, current loss: 48584.261525\n",
      "iteration 218, current loss: 48413.685060\n",
      "iteration 219, current loss: 48572.574420\n",
      "iteration 220, current loss: 48392.268686\n",
      "iteration 221, current loss: 48631.514266\n",
      "iteration 222, current loss: 48430.763549\n",
      "iteration 223, current loss: 48653.745168\n",
      "iteration 224, current loss: 48397.866941\n",
      "iteration 225, current loss: 48542.657883\n",
      "iteration 226, current loss: 48355.411579\n",
      "iteration 227, current loss: 48450.711605\n",
      "iteration 228, current loss: 48245.136349\n",
      "iteration 229, current loss: 48311.903883\n",
      "iteration 230, current loss: 48142.255513\n",
      "iteration 231, current loss: 48181.717766\n",
      "iteration 232, current loss: 48013.838090\n",
      "iteration 233, current loss: 48032.696017\n",
      "iteration 234, current loss: 47865.591950\n",
      "iteration 235, current loss: 47889.607169\n",
      "iteration 236, current loss: 47718.304023\n",
      "iteration 237, current loss: 47767.604173\n",
      "iteration 238, current loss: 47604.630388\n",
      "iteration 239, current loss: 47734.904269\n",
      "iteration 240, current loss: 47501.829206\n",
      "iteration 241, current loss: 47643.357935\n",
      "iteration 242, current loss: 47372.549552\n",
      "iteration 243, current loss: 47510.075101\n",
      "iteration 244, current loss: 47257.053119\n",
      "iteration 245, current loss: 47381.091512\n",
      "iteration 246, current loss: 47167.962397\n",
      "iteration 247, current loss: 47308.303641\n",
      "iteration 248, current loss: 47114.557210\n",
      "iteration 249, current loss: 47288.850123\n",
      "iteration 250, current loss: 47120.782224\n",
      "iteration 251, current loss: 47323.778329\n",
      "iteration 252, current loss: 47182.329306\n",
      "iteration 253, current loss: 47390.564537\n",
      "iteration 254, current loss: 47177.051710\n",
      "iteration 255, current loss: 47285.065664\n",
      "iteration 256, current loss: 47035.491836\n",
      "iteration 257, current loss: 47071.229684\n",
      "iteration 258, current loss: 46854.795053\n",
      "iteration 259, current loss: 46881.879763\n",
      "iteration 260, current loss: 46679.587693\n",
      "iteration 261, current loss: 46722.262681\n",
      "iteration 262, current loss: 46528.771584\n",
      "iteration 263, current loss: 46595.453215\n",
      "iteration 264, current loss: 46400.631547\n",
      "iteration 265, current loss: 46502.938174\n",
      "iteration 266, current loss: 46316.079235\n",
      "iteration 267, current loss: 46446.499977\n",
      "iteration 268, current loss: 46224.549842\n",
      "iteration 269, current loss: 46395.392290\n",
      "iteration 270, current loss: 46143.426510\n",
      "iteration 271, current loss: 46350.586130\n",
      "iteration 272, current loss: 46102.761469\n",
      "iteration 273, current loss: 46353.217959\n",
      "iteration 274, current loss: 46109.950155\n",
      "iteration 275, current loss: 46358.546934\n",
      "iteration 276, current loss: 46141.302358\n",
      "iteration 277, current loss: 46352.327337\n",
      "iteration 278, current loss: 46151.221115\n",
      "iteration 279, current loss: 46297.577604\n",
      "iteration 280, current loss: 46090.125817\n",
      "iteration 281, current loss: 46179.237353\n",
      "iteration 282, current loss: 45975.170400\n",
      "iteration 283, current loss: 45988.165848\n",
      "iteration 284, current loss: 45831.973945\n",
      "iteration 285, current loss: 45807.260211\n",
      "iteration 286, current loss: 45642.217233\n",
      "iteration 287, current loss: 45625.889919\n",
      "iteration 288, current loss: 45463.380834\n",
      "iteration 289, current loss: 45493.380244\n",
      "iteration 290, current loss: 45335.088573\n",
      "iteration 291, current loss: 45417.388317\n",
      "iteration 292, current loss: 45251.461402\n",
      "iteration 293, current loss: 45379.775335\n",
      "iteration 294, current loss: 45210.652709\n",
      "iteration 295, current loss: 45395.810391\n",
      "iteration 296, current loss: 45180.899624\n",
      "iteration 297, current loss: 45395.086867\n",
      "iteration 298, current loss: 45146.643135\n",
      "iteration 299, current loss: 45339.850708\n",
      "iteration 300, current loss: 45075.728004\n",
      "iteration 301, current loss: 45235.803773\n",
      "iteration 302, current loss: 44982.021134\n",
      "iteration 303, current loss: 45110.992417\n",
      "iteration 304, current loss: 44873.792960\n",
      "iteration 305, current loss: 44973.168825\n",
      "iteration 306, current loss: 44777.110408\n",
      "iteration 307, current loss: 44877.892317\n",
      "iteration 308, current loss: 44746.999456\n",
      "iteration 309, current loss: 44877.045146\n",
      "iteration 310, current loss: 44814.370332\n",
      "iteration 311, current loss: 44913.917412\n",
      "iteration 312, current loss: 44881.275635\n",
      "iteration 313, current loss: 45012.384246\n",
      "iteration 314, current loss: 45001.677227\n",
      "iteration 315, current loss: 45036.064341\n",
      "iteration 316, current loss: 44898.235268\n",
      "iteration 317, current loss: 44935.613818\n",
      "iteration 318, current loss: 44696.626444\n",
      "iteration 319, current loss: 44734.566534\n",
      "iteration 320, current loss: 44490.317620\n",
      "iteration 321, current loss: 44517.780702\n",
      "iteration 322, current loss: 44280.544570\n",
      "iteration 323, current loss: 44279.221845\n",
      "iteration 324, current loss: 44068.438958\n",
      "iteration 325, current loss: 44066.528438\n",
      "iteration 326, current loss: 43900.247629\n",
      "iteration 327, current loss: 43905.376288\n",
      "iteration 328, current loss: 43787.725919\n",
      "iteration 329, current loss: 43810.188223\n",
      "iteration 330, current loss: 43735.222114\n",
      "iteration 331, current loss: 43767.529441\n",
      "iteration 332, current loss: 43716.709403\n",
      "iteration 333, current loss: 43748.558704\n",
      "iteration 334, current loss: 43710.881307\n",
      "iteration 335, current loss: 43735.705421\n",
      "iteration 336, current loss: 43713.261937\n",
      "iteration 337, current loss: 43695.031545\n",
      "iteration 338, current loss: 43677.622091\n",
      "iteration 339, current loss: 43608.072349\n",
      "iteration 340, current loss: 43569.977370\n",
      "iteration 341, current loss: 43477.897033\n",
      "iteration 342, current loss: 43430.003784\n",
      "iteration 343, current loss: 43329.712190\n",
      "iteration 344, current loss: 43289.278506\n",
      "iteration 345, current loss: 43185.170658\n",
      "iteration 346, current loss: 43164.253011\n",
      "iteration 347, current loss: 43061.243350\n",
      "iteration 348, current loss: 43057.336457\n",
      "iteration 349, current loss: 42955.451237\n",
      "iteration 350, current loss: 42949.602880\n",
      "iteration 351, current loss: 42846.087402\n",
      "iteration 352, current loss: 42851.537140\n",
      "iteration 353, current loss: 42764.020938\n",
      "iteration 354, current loss: 42812.635251\n",
      "iteration 355, current loss: 42718.623198\n",
      "iteration 356, current loss: 42776.047995\n",
      "iteration 357, current loss: 42656.423050\n",
      "iteration 358, current loss: 42697.162280\n",
      "iteration 359, current loss: 42581.631742\n",
      "iteration 360, current loss: 42602.839397\n",
      "iteration 361, current loss: 42497.052084\n",
      "iteration 362, current loss: 42488.196751\n",
      "iteration 363, current loss: 42393.222971\n",
      "iteration 364, current loss: 42351.424221\n",
      "iteration 365, current loss: 42273.215030\n",
      "iteration 366, current loss: 42212.510832\n",
      "iteration 367, current loss: 42175.239012\n",
      "iteration 368, current loss: 42138.943221\n",
      "iteration 369, current loss: 42145.610363\n",
      "iteration 370, current loss: 42150.779649\n",
      "iteration 371, current loss: 42190.110382\n",
      "iteration 372, current loss: 42240.346183\n",
      "iteration 373, current loss: 42305.531686\n",
      "iteration 374, current loss: 42371.605196\n",
      "iteration 375, current loss: 42555.460124\n",
      "iteration 376, current loss: 42622.776835\n",
      "iteration 377, current loss: 42963.379241\n",
      "iteration 378, current loss: 42932.241858\n",
      "iteration 379, current loss: 43307.442115\n",
      "iteration 380, current loss: 43140.824586\n",
      "iteration 381, current loss: 43363.753169\n",
      "iteration 382, current loss: 43073.443971\n",
      "iteration 383, current loss: 43146.048126\n",
      "iteration 384, current loss: 42828.088962\n",
      "iteration 385, current loss: 42781.363338\n",
      "iteration 386, current loss: 42505.038720\n",
      "iteration 387, current loss: 42430.354678\n",
      "iteration 388, current loss: 42231.877234\n",
      "iteration 389, current loss: 42154.257171\n",
      "iteration 390, current loss: 42013.786489\n",
      "iteration 391, current loss: 41941.555249\n",
      "iteration 392, current loss: 41840.299429\n",
      "iteration 393, current loss: 41775.025805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 394, current loss: 41725.399631\n",
      "iteration 395, current loss: 41668.443072\n",
      "iteration 396, current loss: 41681.756945\n",
      "iteration 397, current loss: 41657.806538\n",
      "iteration 398, current loss: 41684.522470\n",
      "iteration 399, current loss: 41708.890131\n",
      "iteration 400, current loss: 41654.967927\n",
      "iteration 401, current loss: 41656.788237\n",
      "iteration 402, current loss: 41551.380648\n",
      "iteration 403, current loss: 41507.798751\n",
      "iteration 404, current loss: 41421.370596\n",
      "iteration 405, current loss: 41329.062498\n",
      "iteration 406, current loss: 41270.357943\n",
      "iteration 407, current loss: 41172.009107\n",
      "iteration 408, current loss: 41124.949192\n",
      "iteration 409, current loss: 41060.168547\n",
      "iteration 410, current loss: 41028.984727\n",
      "iteration 411, current loss: 40987.769215\n",
      "iteration 412, current loss: 40962.241632\n",
      "iteration 413, current loss: 40941.790337\n",
      "iteration 414, current loss: 40935.094964\n",
      "iteration 415, current loss: 40932.390612\n",
      "iteration 416, current loss: 40973.273802\n",
      "iteration 417, current loss: 40988.192018\n",
      "iteration 418, current loss: 41084.179137\n",
      "iteration 419, current loss: 41117.809052\n",
      "iteration 420, current loss: 41227.663501\n",
      "iteration 421, current loss: 41259.406624\n",
      "iteration 422, current loss: 41295.606800\n",
      "iteration 423, current loss: 41247.518965\n",
      "iteration 424, current loss: 41196.417325\n",
      "iteration 425, current loss: 41094.248502\n",
      "iteration 426, current loss: 41015.825092\n",
      "iteration 427, current loss: 40890.310296\n",
      "iteration 428, current loss: 40797.957671\n",
      "iteration 429, current loss: 40668.405461\n",
      "iteration 430, current loss: 40575.804765\n",
      "iteration 431, current loss: 40496.878818\n",
      "iteration 432, current loss: 40418.736966\n",
      "iteration 433, current loss: 40372.330013\n",
      "iteration 434, current loss: 40317.472887\n",
      "iteration 435, current loss: 40269.519655\n",
      "iteration 436, current loss: 40223.982766\n",
      "iteration 437, current loss: 40169.381564\n",
      "iteration 438, current loss: 40139.708959\n",
      "iteration 439, current loss: 40073.743254\n",
      "iteration 440, current loss: 40067.700968\n",
      "iteration 441, current loss: 40009.605495\n",
      "iteration 442, current loss: 40049.335894\n",
      "iteration 443, current loss: 39997.573798\n",
      "iteration 444, current loss: 40076.643704\n",
      "iteration 445, current loss: 40015.608604\n",
      "iteration 446, current loss: 40115.672288\n",
      "iteration 447, current loss: 40018.638001\n",
      "iteration 448, current loss: 40135.686884\n",
      "iteration 449, current loss: 40036.392594\n",
      "iteration 450, current loss: 40170.054606\n",
      "iteration 451, current loss: 40039.389738\n",
      "iteration 452, current loss: 40139.912483\n",
      "iteration 453, current loss: 40047.483379\n",
      "iteration 454, current loss: 40111.846625\n",
      "iteration 455, current loss: 40049.966182\n",
      "iteration 456, current loss: 40102.975089\n",
      "iteration 457, current loss: 40077.130896\n",
      "iteration 458, current loss: 40122.326998\n",
      "iteration 459, current loss: 40090.324552\n",
      "iteration 460, current loss: 40111.404631\n",
      "iteration 461, current loss: 40055.154882\n",
      "iteration 462, current loss: 40054.274642\n",
      "iteration 463, current loss: 39979.455222\n",
      "iteration 464, current loss: 39953.981141\n",
      "iteration 465, current loss: 39867.234997\n",
      "iteration 466, current loss: 39814.430637\n",
      "iteration 467, current loss: 39728.575617\n",
      "iteration 468, current loss: 39664.916938\n",
      "iteration 469, current loss: 39590.525277\n",
      "iteration 470, current loss: 39536.572395\n",
      "iteration 471, current loss: 39470.754115\n",
      "iteration 472, current loss: 39440.136299\n",
      "iteration 473, current loss: 39415.704917\n",
      "iteration 474, current loss: 39407.392221\n",
      "iteration 475, current loss: 39383.201263\n",
      "iteration 476, current loss: 39388.827461\n",
      "iteration 477, current loss: 39361.648175\n",
      "iteration 478, current loss: 39390.906444\n",
      "iteration 479, current loss: 39292.672103\n",
      "iteration 480, current loss: 39325.793203\n",
      "iteration 481, current loss: 39189.801733\n",
      "iteration 482, current loss: 39223.649198\n",
      "iteration 483, current loss: 39108.635245\n",
      "iteration 484, current loss: 39149.515480\n",
      "iteration 485, current loss: 39051.561203\n",
      "iteration 486, current loss: 39101.732429\n",
      "iteration 487, current loss: 39025.689905\n",
      "iteration 488, current loss: 39091.652086\n",
      "iteration 489, current loss: 39021.011329\n",
      "iteration 490, current loss: 39108.874171\n",
      "iteration 491, current loss: 39024.078348\n",
      "iteration 492, current loss: 39123.989062\n",
      "iteration 493, current loss: 39024.385647\n",
      "iteration 494, current loss: 39090.552635\n",
      "iteration 495, current loss: 38997.932499\n",
      "iteration 496, current loss: 39036.436647\n",
      "iteration 497, current loss: 38957.384315\n",
      "iteration 498, current loss: 38987.853021\n",
      "iteration 499, current loss: 38903.468665\n",
      "iteration 500, current loss: 38953.648421\n",
      "iteration 501, current loss: 38906.952990\n",
      "iteration 502, current loss: 38961.790917\n",
      "iteration 503, current loss: 38866.687179\n",
      "iteration 504, current loss: 38887.467494\n",
      "iteration 505, current loss: 38784.153521\n",
      "iteration 506, current loss: 38763.806631\n",
      "iteration 507, current loss: 38661.709768\n",
      "iteration 508, current loss: 38618.974750\n",
      "iteration 509, current loss: 38537.087403\n",
      "iteration 510, current loss: 38503.114374\n",
      "iteration 511, current loss: 38434.681700\n",
      "iteration 512, current loss: 38410.538610\n",
      "iteration 513, current loss: 38371.870348\n",
      "iteration 514, current loss: 38367.845825\n",
      "iteration 515, current loss: 38343.744775\n",
      "iteration 516, current loss: 38372.763350\n",
      "iteration 517, current loss: 38370.632190\n",
      "iteration 518, current loss: 38468.317736\n",
      "iteration 519, current loss: 38446.312929\n",
      "iteration 520, current loss: 38592.162149\n",
      "iteration 521, current loss: 38613.541411\n",
      "iteration 522, current loss: 38816.172332\n",
      "iteration 523, current loss: 38811.714048\n",
      "iteration 524, current loss: 38985.148497\n",
      "iteration 525, current loss: 38891.767942\n",
      "iteration 526, current loss: 38961.925856\n",
      "iteration 527, current loss: 38772.299899\n",
      "iteration 528, current loss: 38748.922624\n",
      "iteration 529, current loss: 38545.862469\n",
      "iteration 530, current loss: 38472.924001\n",
      "iteration 531, current loss: 38281.375963\n",
      "iteration 532, current loss: 38194.641052\n",
      "iteration 533, current loss: 38067.092021\n",
      "iteration 534, current loss: 38005.135319\n",
      "iteration 535, current loss: 37945.131159\n",
      "iteration 536, current loss: 37911.301280\n",
      "iteration 537, current loss: 37875.406165\n",
      "iteration 538, current loss: 37847.960470\n",
      "iteration 539, current loss: 37822.459263\n",
      "iteration 540, current loss: 37799.372302\n",
      "iteration 541, current loss: 37781.721549\n",
      "iteration 542, current loss: 37765.849012\n",
      "iteration 543, current loss: 37752.318539\n",
      "iteration 544, current loss: 37749.586881\n",
      "iteration 545, current loss: 37740.599820\n",
      "iteration 546, current loss: 37761.791036\n",
      "iteration 547, current loss: 37764.780695\n",
      "iteration 548, current loss: 37823.327402\n",
      "iteration 549, current loss: 37825.997502\n",
      "iteration 550, current loss: 37908.936484\n",
      "iteration 551, current loss: 37837.846224\n",
      "iteration 552, current loss: 37930.130972\n",
      "iteration 553, current loss: 37763.868751\n",
      "iteration 554, current loss: 37831.119871\n",
      "iteration 555, current loss: 37678.158152\n",
      "iteration 556, current loss: 37751.616438\n",
      "iteration 557, current loss: 37636.118015\n",
      "iteration 558, current loss: 37689.962454\n",
      "iteration 559, current loss: 37604.610903\n",
      "iteration 560, current loss: 37643.066367\n",
      "iteration 561, current loss: 37599.600430\n",
      "iteration 562, current loss: 37635.807181\n",
      "iteration 563, current loss: 37584.755386\n",
      "iteration 564, current loss: 37611.453381\n",
      "iteration 565, current loss: 37560.107213\n",
      "iteration 566, current loss: 37568.247326\n",
      "iteration 567, current loss: 37493.056218\n",
      "iteration 568, current loss: 37471.305703\n",
      "iteration 569, current loss: 37386.019523\n",
      "iteration 570, current loss: 37358.896865\n",
      "iteration 571, current loss: 37284.873213\n",
      "iteration 572, current loss: 37270.466602\n",
      "iteration 573, current loss: 37211.147906\n",
      "iteration 574, current loss: 37204.811280\n",
      "iteration 575, current loss: 37155.077951\n",
      "iteration 576, current loss: 37150.170438\n",
      "iteration 577, current loss: 37111.927265\n",
      "iteration 578, current loss: 37109.156161\n",
      "iteration 579, current loss: 37087.445816\n",
      "iteration 580, current loss: 37096.953601\n",
      "iteration 581, current loss: 37097.234155\n",
      "iteration 582, current loss: 37120.055694\n",
      "iteration 583, current loss: 37176.956645\n",
      "iteration 584, current loss: 37209.750744\n",
      "iteration 585, current loss: 37337.515703\n",
      "iteration 586, current loss: 37332.159145\n",
      "iteration 587, current loss: 37468.392425\n",
      "iteration 588, current loss: 37331.331471\n",
      "iteration 589, current loss: 37459.970387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 590, current loss: 37316.495480\n",
      "iteration 591, current loss: 37395.373425\n",
      "iteration 592, current loss: 37279.644304\n",
      "iteration 593, current loss: 37297.757294\n",
      "iteration 594, current loss: 37321.181391\n",
      "iteration 595, current loss: 37333.637891\n",
      "iteration 596, current loss: 37381.960510\n",
      "iteration 597, current loss: 37305.535782\n",
      "iteration 598, current loss: 37373.252965\n",
      "iteration 599, current loss: 37220.495023\n",
      "iteration 600, current loss: 37269.698839\n",
      "iteration 601, current loss: 37131.698725\n",
      "iteration 602, current loss: 37153.308075\n",
      "iteration 603, current loss: 37028.312160\n",
      "iteration 604, current loss: 37013.385130\n",
      "iteration 605, current loss: 36910.465077\n",
      "iteration 606, current loss: 36884.332916\n",
      "iteration 607, current loss: 36787.646248\n",
      "iteration 608, current loss: 36770.337655\n",
      "iteration 609, current loss: 36666.444747\n",
      "iteration 610, current loss: 36647.776258\n",
      "iteration 611, current loss: 36570.599604\n",
      "iteration 612, current loss: 36558.081448\n",
      "iteration 613, current loss: 36502.142691\n",
      "iteration 614, current loss: 36494.038186\n",
      "iteration 615, current loss: 36453.510875\n",
      "iteration 616, current loss: 36454.152943\n",
      "iteration 617, current loss: 36435.045376\n",
      "iteration 618, current loss: 36444.254451\n",
      "iteration 619, current loss: 36465.321183\n",
      "iteration 620, current loss: 36460.223914\n",
      "iteration 621, current loss: 36484.977323\n",
      "iteration 622, current loss: 36474.593846\n",
      "iteration 623, current loss: 36495.963037\n",
      "iteration 624, current loss: 36535.199780\n",
      "iteration 625, current loss: 36560.481463\n",
      "iteration 626, current loss: 36650.439454\n",
      "iteration 627, current loss: 36612.583767\n",
      "iteration 628, current loss: 36689.998336\n",
      "iteration 629, current loss: 36625.488013\n",
      "iteration 630, current loss: 36631.734210\n",
      "iteration 631, current loss: 36574.652312\n",
      "iteration 632, current loss: 36533.102816\n",
      "iteration 633, current loss: 36493.730122\n",
      "iteration 634, current loss: 36447.265591\n",
      "iteration 635, current loss: 36436.951742\n",
      "iteration 636, current loss: 36395.277114\n",
      "iteration 637, current loss: 36395.684932\n",
      "iteration 638, current loss: 36332.509496\n",
      "iteration 639, current loss: 36343.135730\n",
      "iteration 640, current loss: 36274.687939\n",
      "iteration 641, current loss: 36295.647465\n",
      "iteration 642, current loss: 36210.468774\n",
      "iteration 643, current loss: 36219.999258\n",
      "iteration 644, current loss: 36140.153427\n",
      "iteration 645, current loss: 36139.156576\n",
      "iteration 646, current loss: 36072.750850\n",
      "iteration 647, current loss: 36072.684545\n",
      "iteration 648, current loss: 36022.117701\n",
      "iteration 649, current loss: 35999.616189\n",
      "iteration 650, current loss: 35957.553383\n",
      "iteration 651, current loss: 35955.113963\n",
      "iteration 652, current loss: 35938.175862\n",
      "iteration 653, current loss: 35963.226616\n",
      "iteration 654, current loss: 35994.484310\n",
      "iteration 655, current loss: 36064.449482\n",
      "iteration 656, current loss: 36151.237440\n",
      "iteration 657, current loss: 36155.038170\n",
      "iteration 658, current loss: 36277.689903\n",
      "iteration 659, current loss: 36159.427366\n",
      "iteration 660, current loss: 36281.963227\n",
      "iteration 661, current loss: 36099.343919\n",
      "iteration 662, current loss: 36189.345717\n",
      "iteration 663, current loss: 36013.232692\n",
      "iteration 664, current loss: 36075.777200\n",
      "iteration 665, current loss: 35935.589503\n",
      "iteration 666, current loss: 35991.885128\n",
      "iteration 667, current loss: 35881.190899\n",
      "iteration 668, current loss: 35917.987639\n",
      "iteration 669, current loss: 35814.382461\n",
      "iteration 670, current loss: 35844.509002\n",
      "iteration 671, current loss: 35745.767119\n",
      "iteration 672, current loss: 35771.391682\n",
      "iteration 673, current loss: 35680.055251\n",
      "iteration 674, current loss: 35700.026331\n",
      "iteration 675, current loss: 35614.885544\n",
      "iteration 676, current loss: 35635.149424\n",
      "iteration 677, current loss: 35555.996362\n",
      "iteration 678, current loss: 35575.459883\n",
      "iteration 679, current loss: 35509.941088\n",
      "iteration 680, current loss: 35530.172565\n",
      "iteration 681, current loss: 35492.114325\n",
      "iteration 682, current loss: 35521.669180\n",
      "iteration 683, current loss: 35489.498630\n",
      "iteration 684, current loss: 35498.709733\n",
      "iteration 685, current loss: 35466.106899\n",
      "iteration 686, current loss: 35460.837608\n",
      "iteration 687, current loss: 35422.328148\n",
      "iteration 688, current loss: 35415.585482\n",
      "iteration 689, current loss: 35384.028901\n",
      "iteration 690, current loss: 35389.803168\n",
      "iteration 691, current loss: 35404.086164\n",
      "iteration 692, current loss: 35467.128973\n",
      "iteration 693, current loss: 35557.849705\n",
      "iteration 694, current loss: 35676.625670\n",
      "iteration 695, current loss: 35755.301536\n",
      "iteration 696, current loss: 35817.849393\n",
      "iteration 697, current loss: 35903.855607\n",
      "iteration 698, current loss: 35865.484272\n",
      "iteration 699, current loss: 35932.881529\n",
      "iteration 700, current loss: 35842.992535\n",
      "iteration 701, current loss: 35877.142471\n",
      "iteration 702, current loss: 35765.409410\n",
      "iteration 703, current loss: 35776.018839\n",
      "iteration 704, current loss: 35627.527296\n",
      "iteration 705, current loss: 35625.561208\n",
      "iteration 706, current loss: 35478.778802\n",
      "iteration 707, current loss: 35457.161160\n",
      "iteration 708, current loss: 35339.902222\n",
      "iteration 709, current loss: 35332.358804\n",
      "iteration 710, current loss: 35240.681109\n",
      "iteration 711, current loss: 35249.608486\n",
      "iteration 712, current loss: 35173.115979\n",
      "iteration 713, current loss: 35179.005678\n",
      "iteration 714, current loss: 35111.546461\n",
      "iteration 715, current loss: 35113.253896\n",
      "iteration 716, current loss: 35053.606756\n",
      "iteration 717, current loss: 35061.836246\n",
      "iteration 718, current loss: 35006.791331\n",
      "iteration 719, current loss: 35029.545434\n",
      "iteration 720, current loss: 34982.368250\n",
      "iteration 721, current loss: 35046.637071\n",
      "iteration 722, current loss: 34983.374885\n",
      "iteration 723, current loss: 35089.507038\n",
      "iteration 724, current loss: 35029.721928\n",
      "iteration 725, current loss: 35156.781744\n",
      "iteration 726, current loss: 35114.686199\n",
      "iteration 727, current loss: 35228.763483\n",
      "iteration 728, current loss: 35130.487074\n",
      "iteration 729, current loss: 35190.935611\n",
      "iteration 730, current loss: 35046.372284\n",
      "iteration 731, current loss: 35063.636537\n",
      "iteration 732, current loss: 34902.469744\n",
      "iteration 733, current loss: 34883.717269\n",
      "iteration 734, current loss: 34787.215686\n",
      "iteration 735, current loss: 34763.027940\n",
      "iteration 736, current loss: 34721.263232\n",
      "iteration 737, current loss: 34697.552531\n",
      "iteration 738, current loss: 34674.692981\n",
      "iteration 739, current loss: 34660.142146\n",
      "iteration 740, current loss: 34646.830553\n",
      "iteration 741, current loss: 34651.787013\n",
      "iteration 742, current loss: 34652.199189\n",
      "iteration 743, current loss: 34680.277573\n",
      "iteration 744, current loss: 34687.662472\n",
      "iteration 745, current loss: 34722.344836\n",
      "iteration 746, current loss: 34738.620076\n",
      "iteration 747, current loss: 34839.182514\n",
      "iteration 748, current loss: 34873.647167\n",
      "iteration 749, current loss: 34999.672040\n",
      "iteration 750, current loss: 34998.689562\n",
      "iteration 751, current loss: 35121.736934\n",
      "iteration 752, current loss: 35115.152932\n",
      "iteration 753, current loss: 35180.350802\n",
      "iteration 754, current loss: 35135.628639\n",
      "iteration 755, current loss: 35140.763240\n",
      "iteration 756, current loss: 35060.816161\n",
      "iteration 757, current loss: 35017.483716\n",
      "iteration 758, current loss: 34913.026810\n",
      "iteration 759, current loss: 34868.556081\n",
      "iteration 760, current loss: 34774.187043\n",
      "iteration 761, current loss: 34715.783561\n",
      "iteration 762, current loss: 34639.597500\n",
      "iteration 763, current loss: 34581.499217\n",
      "iteration 764, current loss: 34516.590107\n",
      "iteration 765, current loss: 34471.260854\n",
      "iteration 766, current loss: 34425.833836\n",
      "iteration 767, current loss: 34401.530289\n",
      "iteration 768, current loss: 34380.141368\n",
      "iteration 769, current loss: 34380.955509\n",
      "iteration 770, current loss: 34387.045819\n",
      "iteration 771, current loss: 34413.344236\n",
      "iteration 772, current loss: 34430.787159\n",
      "iteration 773, current loss: 34472.609934\n",
      "iteration 774, current loss: 34486.465551\n",
      "iteration 775, current loss: 34532.243236\n",
      "iteration 776, current loss: 34528.690914\n",
      "iteration 777, current loss: 34577.221008\n",
      "iteration 778, current loss: 34548.768117\n",
      "iteration 779, current loss: 34584.713870\n",
      "iteration 780, current loss: 34536.498668\n",
      "iteration 781, current loss: 34548.107504\n",
      "iteration 782, current loss: 34484.841866\n",
      "iteration 783, current loss: 34475.277683\n",
      "iteration 784, current loss: 34412.136282\n",
      "iteration 785, current loss: 34391.083110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 786, current loss: 34353.400700\n",
      "iteration 787, current loss: 34328.478420\n",
      "iteration 788, current loss: 34334.851530\n",
      "iteration 789, current loss: 34308.535862\n",
      "iteration 790, current loss: 34329.305627\n",
      "iteration 791, current loss: 34269.369133\n",
      "iteration 792, current loss: 34274.169383\n",
      "iteration 793, current loss: 34232.972529\n",
      "iteration 794, current loss: 34230.820212\n",
      "iteration 795, current loss: 34188.069257\n",
      "iteration 796, current loss: 34172.308438\n",
      "iteration 797, current loss: 34128.394620\n",
      "iteration 798, current loss: 34096.975641\n",
      "iteration 799, current loss: 34062.491029\n",
      "iteration 800, current loss: 34032.914336\n",
      "iteration 801, current loss: 34010.067945\n",
      "iteration 802, current loss: 33999.846608\n",
      "iteration 803, current loss: 34010.722871\n",
      "iteration 804, current loss: 34044.776097\n",
      "iteration 805, current loss: 34108.670146\n",
      "iteration 806, current loss: 34150.584263\n",
      "iteration 807, current loss: 34241.039905\n",
      "iteration 808, current loss: 34239.747604\n",
      "iteration 809, current loss: 34354.926665\n",
      "iteration 810, current loss: 34306.030807\n",
      "iteration 811, current loss: 34415.155517\n",
      "iteration 812, current loss: 34326.408917\n",
      "iteration 813, current loss: 34413.595879\n",
      "iteration 814, current loss: 34287.645341\n",
      "iteration 815, current loss: 34382.359919\n",
      "iteration 816, current loss: 34221.677391\n",
      "iteration 817, current loss: 34318.371504\n",
      "iteration 818, current loss: 34138.417752\n",
      "iteration 819, current loss: 34199.908522\n",
      "iteration 820, current loss: 34012.593303\n",
      "iteration 821, current loss: 34046.589150\n",
      "iteration 822, current loss: 33867.653429\n",
      "iteration 823, current loss: 33872.779488\n",
      "iteration 824, current loss: 33738.310466\n",
      "iteration 825, current loss: 33725.550661\n",
      "iteration 826, current loss: 33652.039583\n",
      "iteration 827, current loss: 33638.292020\n",
      "iteration 828, current loss: 33592.737196\n",
      "iteration 829, current loss: 33580.194840\n",
      "iteration 830, current loss: 33551.234756\n",
      "iteration 831, current loss: 33544.880523\n",
      "iteration 832, current loss: 33540.037188\n",
      "iteration 833, current loss: 33551.227483\n",
      "iteration 834, current loss: 33584.402522\n",
      "iteration 835, current loss: 33648.461063\n",
      "iteration 836, current loss: 33736.009380\n",
      "iteration 837, current loss: 33844.186129\n",
      "iteration 838, current loss: 33847.716010\n",
      "iteration 839, current loss: 33871.973947\n",
      "iteration 840, current loss: 33804.333418\n",
      "iteration 841, current loss: 33819.760849\n",
      "iteration 842, current loss: 33727.412119\n",
      "iteration 843, current loss: 33750.076807\n",
      "iteration 844, current loss: 33664.857154\n",
      "iteration 845, current loss: 33692.617929\n",
      "iteration 846, current loss: 33639.528708\n",
      "iteration 847, current loss: 33676.799854\n",
      "iteration 848, current loss: 33669.199033\n",
      "iteration 849, current loss: 33703.278662\n",
      "iteration 850, current loss: 33718.306437\n",
      "iteration 851, current loss: 33756.476914\n",
      "iteration 852, current loss: 33812.766982\n",
      "iteration 853, current loss: 33829.456252\n",
      "iteration 854, current loss: 33912.203722\n",
      "iteration 855, current loss: 33896.007707\n",
      "iteration 856, current loss: 33975.962506\n",
      "iteration 857, current loss: 33929.361501\n",
      "iteration 858, current loss: 33989.509506\n",
      "iteration 859, current loss: 33903.881227\n",
      "iteration 860, current loss: 33932.610587\n",
      "iteration 861, current loss: 33833.034515\n",
      "iteration 862, current loss: 33807.236148\n",
      "iteration 863, current loss: 33706.058275\n",
      "iteration 864, current loss: 33642.135902\n",
      "iteration 865, current loss: 33570.453239\n",
      "iteration 866, current loss: 33519.674781\n",
      "iteration 867, current loss: 33482.205033\n",
      "iteration 868, current loss: 33462.347851\n",
      "iteration 869, current loss: 33433.919892\n",
      "iteration 870, current loss: 33432.224359\n",
      "iteration 871, current loss: 33416.439781\n",
      "iteration 872, current loss: 33433.390645\n",
      "iteration 873, current loss: 33419.704520\n",
      "iteration 874, current loss: 33453.087426\n",
      "iteration 875, current loss: 33432.172333\n",
      "iteration 876, current loss: 33432.026643\n",
      "iteration 877, current loss: 33402.244966\n",
      "iteration 878, current loss: 33380.277700\n",
      "iteration 879, current loss: 33385.857602\n",
      "iteration 880, current loss: 33381.268284\n",
      "iteration 881, current loss: 33402.051187\n",
      "iteration 882, current loss: 33402.434840\n",
      "iteration 883, current loss: 33415.497665\n",
      "iteration 884, current loss: 33404.678220\n",
      "iteration 885, current loss: 33405.909187\n",
      "iteration 886, current loss: 33397.378165\n",
      "iteration 887, current loss: 33388.646577\n",
      "iteration 888, current loss: 33385.156022\n",
      "iteration 889, current loss: 33393.178117\n",
      "iteration 890, current loss: 33354.546367\n",
      "iteration 891, current loss: 33360.357091\n",
      "iteration 892, current loss: 33295.522620\n",
      "iteration 893, current loss: 33318.249554\n",
      "iteration 894, current loss: 33238.644341\n",
      "iteration 895, current loss: 33273.348577\n",
      "iteration 896, current loss: 33196.569519\n",
      "iteration 897, current loss: 33217.308499\n",
      "iteration 898, current loss: 33148.436323\n",
      "iteration 899, current loss: 33154.692070\n",
      "iteration 900, current loss: 33094.979940\n",
      "iteration 901, current loss: 33098.313008\n",
      "iteration 902, current loss: 33072.947495\n",
      "iteration 903, current loss: 33090.314296\n",
      "iteration 904, current loss: 33160.092817\n",
      "iteration 905, current loss: 33186.376143\n",
      "iteration 906, current loss: 33304.761238\n",
      "iteration 907, current loss: 33287.871402\n",
      "iteration 908, current loss: 33397.903100\n",
      "iteration 909, current loss: 33315.013623\n",
      "iteration 910, current loss: 33393.059212\n",
      "iteration 911, current loss: 33265.430739\n",
      "iteration 912, current loss: 33316.770598\n",
      "iteration 913, current loss: 33175.203756\n",
      "iteration 914, current loss: 33199.481089\n",
      "iteration 915, current loss: 33058.540875\n",
      "iteration 916, current loss: 33056.494192\n",
      "iteration 917, current loss: 32974.949638\n",
      "iteration 918, current loss: 32979.365107\n",
      "iteration 919, current loss: 32917.616923\n",
      "iteration 920, current loss: 32931.031626\n",
      "iteration 921, current loss: 32875.574675\n",
      "iteration 922, current loss: 32902.461162\n",
      "iteration 923, current loss: 32854.148804\n",
      "iteration 924, current loss: 32900.470013\n",
      "iteration 925, current loss: 32884.833371\n",
      "iteration 926, current loss: 32937.098350\n",
      "iteration 927, current loss: 32935.755913\n",
      "iteration 928, current loss: 32961.185859\n",
      "iteration 929, current loss: 32977.131688\n",
      "iteration 930, current loss: 32956.106498\n",
      "iteration 931, current loss: 32967.761781\n",
      "iteration 932, current loss: 32915.120087\n",
      "iteration 933, current loss: 32890.753974\n",
      "iteration 934, current loss: 32803.272705\n",
      "iteration 935, current loss: 32736.502436\n",
      "iteration 936, current loss: 32631.080338\n",
      "iteration 937, current loss: 32577.022013\n",
      "iteration 938, current loss: 32494.994237\n",
      "iteration 939, current loss: 32473.347158\n",
      "iteration 940, current loss: 32433.238132\n",
      "iteration 941, current loss: 32425.907025\n",
      "iteration 942, current loss: 32411.585045\n",
      "iteration 943, current loss: 32422.343734\n",
      "iteration 944, current loss: 32426.861317\n",
      "iteration 945, current loss: 32459.440561\n",
      "iteration 946, current loss: 32489.765272\n",
      "iteration 947, current loss: 32559.428511\n",
      "iteration 948, current loss: 32621.386816\n",
      "iteration 949, current loss: 32740.537876\n",
      "iteration 950, current loss: 32827.781639\n",
      "iteration 951, current loss: 32974.173656\n",
      "iteration 952, current loss: 33044.226812\n",
      "iteration 953, current loss: 33147.896157\n",
      "iteration 954, current loss: 33203.510820\n",
      "iteration 955, current loss: 33261.348314\n",
      "iteration 956, current loss: 33267.528935\n",
      "iteration 957, current loss: 33276.925047\n",
      "iteration 958, current loss: 33234.340886\n",
      "iteration 959, current loss: 33198.284392\n",
      "iteration 960, current loss: 33116.277893\n",
      "iteration 961, current loss: 33075.393002\n",
      "iteration 962, current loss: 32983.536234\n",
      "iteration 963, current loss: 32948.255846\n",
      "iteration 964, current loss: 32852.098820\n",
      "iteration 965, current loss: 32801.229958\n",
      "iteration 966, current loss: 32703.838634\n",
      "iteration 967, current loss: 32660.447322\n",
      "iteration 968, current loss: 32584.301069\n",
      "iteration 969, current loss: 32576.453444\n",
      "iteration 970, current loss: 32528.486142\n",
      "iteration 971, current loss: 32540.574202\n",
      "iteration 972, current loss: 32498.450148\n",
      "iteration 973, current loss: 32477.994898\n",
      "iteration 974, current loss: 32403.745444\n",
      "iteration 975, current loss: 32387.164386\n",
      "iteration 976, current loss: 32320.229474\n",
      "iteration 977, current loss: 32290.025064\n",
      "iteration 978, current loss: 32265.716570\n",
      "iteration 979, current loss: 32227.524735\n",
      "iteration 980, current loss: 32240.408319\n",
      "iteration 981, current loss: 32204.738226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 982, current loss: 32249.445734\n",
      "iteration 983, current loss: 32214.608598\n",
      "iteration 984, current loss: 32294.089066\n",
      "iteration 985, current loss: 32252.852005\n",
      "iteration 986, current loss: 32332.900286\n",
      "iteration 987, current loss: 32277.568153\n",
      "iteration 988, current loss: 32331.998828\n",
      "iteration 989, current loss: 32257.414042\n",
      "iteration 990, current loss: 32292.487709\n",
      "iteration 991, current loss: 32211.317739\n",
      "iteration 992, current loss: 32258.535065\n",
      "iteration 993, current loss: 32178.690579\n",
      "iteration 994, current loss: 32216.294668\n",
      "iteration 995, current loss: 32153.603953\n",
      "iteration 996, current loss: 32172.753964\n",
      "iteration 997, current loss: 32117.179673\n",
      "iteration 998, current loss: 32126.954202\n",
      "iteration 999, current loss: 32086.191007\n",
      "iteration 1000, current loss: 32113.701048\n",
      "iteration 1001, current loss: 32112.350974\n",
      "iteration 1002, current loss: 32186.405372\n",
      "iteration 1003, current loss: 32203.186555\n",
      "iteration 1004, current loss: 32287.987824\n",
      "iteration 1005, current loss: 32305.573978\n",
      "iteration 1006, current loss: 32395.790364\n",
      "iteration 1007, current loss: 32403.577591\n",
      "iteration 1008, current loss: 32464.353807\n",
      "iteration 1009, current loss: 32452.765137\n",
      "iteration 1010, current loss: 32505.454914\n",
      "iteration 1011, current loss: 32467.467466\n",
      "iteration 1012, current loss: 32501.232552\n",
      "iteration 1013, current loss: 32430.364355\n",
      "iteration 1014, current loss: 32429.735371\n",
      "iteration 1015, current loss: 32340.808218\n",
      "iteration 1016, current loss: 32300.588441\n",
      "iteration 1017, current loss: 32206.966541\n",
      "iteration 1018, current loss: 32163.988184\n",
      "iteration 1019, current loss: 32125.465854\n",
      "iteration 1020, current loss: 32096.170288\n",
      "iteration 1021, current loss: 32124.662619\n",
      "iteration 1022, current loss: 32085.564289\n",
      "iteration 1023, current loss: 32134.894405\n",
      "iteration 1024, current loss: 32075.753861\n",
      "iteration 1025, current loss: 32117.952387\n",
      "iteration 1026, current loss: 32051.780384\n",
      "iteration 1027, current loss: 32083.656877\n",
      "iteration 1028, current loss: 32032.464970\n",
      "iteration 1029, current loss: 32058.111983\n",
      "iteration 1030, current loss: 32040.994300\n",
      "iteration 1031, current loss: 32045.455263\n",
      "iteration 1032, current loss: 32062.850843\n",
      "iteration 1033, current loss: 32045.411776\n",
      "iteration 1034, current loss: 32078.292245\n",
      "iteration 1035, current loss: 32068.956672\n",
      "iteration 1036, current loss: 32075.091564\n",
      "iteration 1037, current loss: 32092.071130\n",
      "iteration 1038, current loss: 32079.511434\n",
      "iteration 1039, current loss: 32084.182881\n",
      "iteration 1040, current loss: 32038.456926\n",
      "iteration 1041, current loss: 32029.633829\n",
      "iteration 1042, current loss: 31958.656374\n",
      "iteration 1043, current loss: 31893.156912\n",
      "iteration 1044, current loss: 31861.191682\n",
      "iteration 1045, current loss: 31771.883019\n",
      "iteration 1046, current loss: 31747.896992\n",
      "iteration 1047, current loss: 31663.688760\n",
      "iteration 1048, current loss: 31647.029661\n",
      "iteration 1049, current loss: 31596.866194\n",
      "iteration 1050, current loss: 31617.214705\n",
      "iteration 1051, current loss: 31591.216872\n",
      "iteration 1052, current loss: 31613.814235\n",
      "iteration 1053, current loss: 31591.778982\n",
      "iteration 1054, current loss: 31609.821347\n",
      "iteration 1055, current loss: 31591.765686\n",
      "iteration 1056, current loss: 31611.514488\n",
      "iteration 1057, current loss: 31603.825316\n",
      "iteration 1058, current loss: 31630.900381\n",
      "iteration 1059, current loss: 31640.976221\n",
      "iteration 1060, current loss: 31682.579624\n",
      "iteration 1061, current loss: 31696.526981\n",
      "iteration 1062, current loss: 31748.399624\n",
      "iteration 1063, current loss: 31746.330697\n",
      "iteration 1064, current loss: 31799.533230\n",
      "iteration 1065, current loss: 31780.948850\n",
      "iteration 1066, current loss: 31834.056130\n",
      "iteration 1067, current loss: 31815.486919\n",
      "iteration 1068, current loss: 31878.041009\n",
      "iteration 1069, current loss: 31894.276197\n",
      "iteration 1070, current loss: 31951.910762\n",
      "iteration 1071, current loss: 32001.774225\n",
      "iteration 1072, current loss: 32000.171279\n",
      "iteration 1073, current loss: 32037.298516\n",
      "iteration 1074, current loss: 31952.362864\n",
      "iteration 1075, current loss: 31950.174966\n",
      "iteration 1076, current loss: 31844.247929\n",
      "iteration 1077, current loss: 31805.403985\n",
      "iteration 1078, current loss: 31720.451020\n",
      "iteration 1079, current loss: 31672.423103\n",
      "iteration 1080, current loss: 31605.098162\n",
      "iteration 1081, current loss: 31567.489291\n",
      "iteration 1082, current loss: 31514.978602\n",
      "iteration 1083, current loss: 31496.490625\n",
      "iteration 1084, current loss: 31473.760258\n",
      "iteration 1085, current loss: 31480.700141\n",
      "iteration 1086, current loss: 31493.298019\n",
      "iteration 1087, current loss: 31539.521834\n",
      "iteration 1088, current loss: 31533.886839\n",
      "iteration 1089, current loss: 31567.250326\n",
      "iteration 1090, current loss: 31561.523012\n",
      "iteration 1091, current loss: 31571.654777\n",
      "iteration 1092, current loss: 31562.947464\n",
      "iteration 1093, current loss: 31575.288753\n",
      "iteration 1094, current loss: 31524.652708\n",
      "iteration 1095, current loss: 31490.525263\n",
      "iteration 1096, current loss: 31423.474273\n",
      "iteration 1097, current loss: 31374.156031\n",
      "iteration 1098, current loss: 31321.218881\n",
      "iteration 1099, current loss: 31256.323650\n",
      "iteration 1100, current loss: 31214.179435\n",
      "iteration 1101, current loss: 31154.312274\n",
      "iteration 1102, current loss: 31123.847813\n",
      "iteration 1103, current loss: 31085.026371\n",
      "iteration 1104, current loss: 31080.703600\n",
      "iteration 1105, current loss: 31068.599760\n",
      "iteration 1106, current loss: 31108.135891\n",
      "iteration 1107, current loss: 31144.028549\n",
      "iteration 1108, current loss: 31271.515624\n",
      "iteration 1109, current loss: 31397.720141\n",
      "iteration 1110, current loss: 31572.847028\n",
      "iteration 1111, current loss: 31616.815324\n",
      "iteration 1112, current loss: 31612.802799\n",
      "iteration 1113, current loss: 31621.738148\n",
      "iteration 1114, current loss: 31552.698824\n",
      "iteration 1115, current loss: 31537.916172\n",
      "iteration 1116, current loss: 31443.813471\n",
      "iteration 1117, current loss: 31430.748488\n",
      "iteration 1118, current loss: 31333.088809\n",
      "iteration 1119, current loss: 31327.319237\n",
      "iteration 1120, current loss: 31238.096698\n",
      "iteration 1121, current loss: 31211.140709\n",
      "iteration 1122, current loss: 31149.643730\n",
      "iteration 1123, current loss: 31117.651256\n",
      "iteration 1124, current loss: 31066.859803\n",
      "iteration 1125, current loss: 31037.641968\n",
      "iteration 1126, current loss: 31009.804206\n",
      "iteration 1127, current loss: 31023.000067\n",
      "iteration 1128, current loss: 31036.942421\n",
      "iteration 1129, current loss: 31103.842112\n",
      "iteration 1130, current loss: 31134.489706\n",
      "iteration 1131, current loss: 31228.155612\n",
      "iteration 1132, current loss: 31271.249795\n",
      "iteration 1133, current loss: 31392.233521\n",
      "iteration 1134, current loss: 31424.597936\n",
      "iteration 1135, current loss: 31490.870850\n",
      "iteration 1136, current loss: 31512.593908\n",
      "iteration 1137, current loss: 31514.882573\n",
      "iteration 1138, current loss: 31517.486956\n",
      "iteration 1139, current loss: 31496.013589\n",
      "iteration 1140, current loss: 31488.633916\n",
      "iteration 1141, current loss: 31473.488583\n",
      "iteration 1142, current loss: 31471.480143\n",
      "iteration 1143, current loss: 31460.573539\n",
      "iteration 1144, current loss: 31459.262241\n",
      "iteration 1145, current loss: 31417.636739\n",
      "iteration 1146, current loss: 31392.364918\n",
      "iteration 1147, current loss: 31320.450824\n",
      "iteration 1148, current loss: 31267.588294\n",
      "iteration 1149, current loss: 31158.790962\n",
      "iteration 1150, current loss: 31111.407611\n",
      "iteration 1151, current loss: 31009.369492\n",
      "iteration 1152, current loss: 30976.427331\n",
      "iteration 1153, current loss: 30892.091422\n",
      "iteration 1154, current loss: 30867.512964\n",
      "iteration 1155, current loss: 30800.133856\n",
      "iteration 1156, current loss: 30783.224209\n",
      "iteration 1157, current loss: 30737.596541\n",
      "iteration 1158, current loss: 30732.444781\n",
      "iteration 1159, current loss: 30704.400897\n",
      "iteration 1160, current loss: 30718.070372\n",
      "iteration 1161, current loss: 30706.123277\n",
      "iteration 1162, current loss: 30750.079977\n",
      "iteration 1163, current loss: 30779.037790\n",
      "iteration 1164, current loss: 30868.265030\n",
      "iteration 1165, current loss: 30918.061476\n",
      "iteration 1166, current loss: 31018.932996\n",
      "iteration 1167, current loss: 31136.879286\n",
      "iteration 1168, current loss: 31193.579846\n",
      "iteration 1169, current loss: 31323.170101\n",
      "iteration 1170, current loss: 31266.306657\n",
      "iteration 1171, current loss: 31325.191297\n",
      "iteration 1172, current loss: 31212.476122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1173, current loss: 31199.793952\n",
      "iteration 1174, current loss: 31092.861902\n",
      "iteration 1175, current loss: 31019.936073\n",
      "iteration 1176, current loss: 30942.598509\n",
      "iteration 1177, current loss: 30855.653125\n",
      "iteration 1178, current loss: 30760.644100\n",
      "iteration 1179, current loss: 30704.232362\n",
      "iteration 1180, current loss: 30639.753752\n",
      "iteration 1181, current loss: 30607.572641\n",
      "iteration 1182, current loss: 30582.949012\n",
      "iteration 1183, current loss: 30577.559236\n",
      "iteration 1184, current loss: 30572.582424\n",
      "iteration 1185, current loss: 30594.601160\n",
      "iteration 1186, current loss: 30597.522015\n",
      "iteration 1187, current loss: 30634.611068\n",
      "iteration 1188, current loss: 30652.088766\n",
      "iteration 1189, current loss: 30700.663083\n",
      "iteration 1190, current loss: 30736.813386\n",
      "iteration 1191, current loss: 30781.309591\n",
      "iteration 1192, current loss: 30827.386709\n",
      "iteration 1193, current loss: 30862.432540\n",
      "iteration 1194, current loss: 30901.892442\n",
      "iteration 1195, current loss: 30925.958864\n",
      "iteration 1196, current loss: 30951.769216\n",
      "iteration 1197, current loss: 30975.077148\n",
      "iteration 1198, current loss: 30988.398490\n",
      "iteration 1199, current loss: 31028.311674\n",
      "iteration 1200, current loss: 31044.603308\n",
      "iteration 1201, current loss: 31109.650690\n",
      "iteration 1202, current loss: 31124.749230\n",
      "iteration 1203, current loss: 31198.331924\n",
      "iteration 1204, current loss: 31159.457925\n",
      "iteration 1205, current loss: 31188.394485\n",
      "iteration 1206, current loss: 31086.606486\n",
      "iteration 1207, current loss: 31049.335349\n",
      "iteration 1208, current loss: 30920.298772\n",
      "iteration 1209, current loss: 30852.065935\n",
      "iteration 1210, current loss: 30731.728348\n",
      "iteration 1211, current loss: 30654.309636\n",
      "iteration 1212, current loss: 30561.884761\n",
      "iteration 1213, current loss: 30505.145077\n",
      "iteration 1214, current loss: 30455.927034\n",
      "iteration 1215, current loss: 30423.034300\n",
      "iteration 1216, current loss: 30407.118617\n",
      "iteration 1217, current loss: 30398.210801\n",
      "iteration 1218, current loss: 30416.613430\n",
      "iteration 1219, current loss: 30444.904790\n",
      "iteration 1220, current loss: 30493.886285\n",
      "iteration 1221, current loss: 30573.936283\n",
      "iteration 1222, current loss: 30613.298962\n",
      "iteration 1223, current loss: 30684.744238\n",
      "iteration 1224, current loss: 30689.158438\n",
      "iteration 1225, current loss: 30702.360578\n",
      "iteration 1226, current loss: 30684.453028\n",
      "iteration 1227, current loss: 30659.555384\n",
      "iteration 1228, current loss: 30640.445592\n",
      "iteration 1229, current loss: 30621.042648\n",
      "iteration 1230, current loss: 30614.537155\n",
      "iteration 1231, current loss: 30637.939492\n",
      "iteration 1232, current loss: 30626.006439\n",
      "iteration 1233, current loss: 30637.393782\n",
      "iteration 1234, current loss: 30573.898028\n",
      "iteration 1235, current loss: 30590.942750\n",
      "iteration 1236, current loss: 30509.988221\n",
      "iteration 1237, current loss: 30554.499949\n",
      "iteration 1238, current loss: 30471.138981\n",
      "iteration 1239, current loss: 30556.361970\n",
      "iteration 1240, current loss: 30454.135797\n",
      "iteration 1241, current loss: 30512.072136\n",
      "iteration 1242, current loss: 30412.941589\n",
      "iteration 1243, current loss: 30445.620803\n",
      "iteration 1244, current loss: 30332.399330\n",
      "iteration 1245, current loss: 30324.423811\n",
      "iteration 1246, current loss: 30234.182281\n",
      "iteration 1247, current loss: 30229.699280\n",
      "iteration 1248, current loss: 30163.877007\n",
      "iteration 1249, current loss: 30183.460323\n",
      "iteration 1250, current loss: 30129.939224\n",
      "iteration 1251, current loss: 30166.822459\n",
      "iteration 1252, current loss: 30121.968750\n",
      "iteration 1253, current loss: 30161.253584\n",
      "iteration 1254, current loss: 30132.888742\n",
      "iteration 1255, current loss: 30177.560430\n",
      "iteration 1256, current loss: 30174.571743\n",
      "iteration 1257, current loss: 30235.999902\n",
      "iteration 1258, current loss: 30264.708100\n",
      "iteration 1259, current loss: 30348.771205\n",
      "iteration 1260, current loss: 30404.494920\n",
      "iteration 1261, current loss: 30512.091748\n",
      "iteration 1262, current loss: 30576.666558\n",
      "iteration 1263, current loss: 30691.611327\n",
      "iteration 1264, current loss: 30713.125609\n",
      "iteration 1265, current loss: 30764.431751\n",
      "iteration 1266, current loss: 30740.551582\n",
      "iteration 1267, current loss: 30701.460059\n",
      "iteration 1268, current loss: 30646.877503\n",
      "iteration 1269, current loss: 30548.525576\n",
      "iteration 1270, current loss: 30455.984913\n",
      "iteration 1271, current loss: 30335.861004\n",
      "iteration 1272, current loss: 30239.723015\n",
      "iteration 1273, current loss: 30134.842282\n",
      "iteration 1274, current loss: 30073.258770\n",
      "iteration 1275, current loss: 29996.675129\n",
      "iteration 1276, current loss: 29959.629917\n",
      "iteration 1277, current loss: 29907.970016\n",
      "iteration 1278, current loss: 29887.718581\n",
      "iteration 1279, current loss: 29851.894989\n",
      "iteration 1280, current loss: 29842.793357\n",
      "iteration 1281, current loss: 29821.442258\n",
      "iteration 1282, current loss: 29822.747500\n",
      "iteration 1283, current loss: 29814.263381\n",
      "iteration 1284, current loss: 29829.835275\n",
      "iteration 1285, current loss: 29841.389197\n",
      "iteration 1286, current loss: 29884.781204\n",
      "iteration 1287, current loss: 29949.017336\n",
      "iteration 1288, current loss: 30041.144324\n",
      "iteration 1289, current loss: 30168.160079\n",
      "iteration 1290, current loss: 30272.802182\n",
      "iteration 1291, current loss: 30430.198606\n",
      "iteration 1292, current loss: 30352.641068\n",
      "iteration 1293, current loss: 30497.271707\n",
      "iteration 1294, current loss: 30308.470585\n",
      "iteration 1295, current loss: 30380.261163\n",
      "iteration 1296, current loss: 30199.335159\n",
      "iteration 1297, current loss: 30192.422269\n",
      "iteration 1298, current loss: 30100.329678\n",
      "iteration 1299, current loss: 30056.082110\n",
      "iteration 1300, current loss: 30026.078890\n",
      "iteration 1301, current loss: 29993.465390\n",
      "iteration 1302, current loss: 30008.847356\n",
      "iteration 1303, current loss: 30016.733932\n",
      "iteration 1304, current loss: 30085.705395\n",
      "iteration 1305, current loss: 30115.554256\n",
      "iteration 1306, current loss: 30208.306964\n",
      "iteration 1307, current loss: 30274.700225\n",
      "iteration 1308, current loss: 30335.142286\n",
      "iteration 1309, current loss: 30399.659355\n",
      "iteration 1310, current loss: 30394.424447\n",
      "iteration 1311, current loss: 30408.391270\n",
      "iteration 1312, current loss: 30363.954986\n",
      "iteration 1313, current loss: 30332.550468\n",
      "iteration 1314, current loss: 30280.248779\n",
      "iteration 1315, current loss: 30228.641605\n",
      "iteration 1316, current loss: 30177.822413\n",
      "iteration 1317, current loss: 30156.142023\n",
      "iteration 1318, current loss: 30123.326687\n",
      "iteration 1319, current loss: 30155.057930\n",
      "iteration 1320, current loss: 30101.823134\n",
      "iteration 1321, current loss: 30159.736374\n",
      "iteration 1322, current loss: 30088.783132\n",
      "iteration 1323, current loss: 30159.739803\n",
      "iteration 1324, current loss: 30056.188365\n",
      "iteration 1325, current loss: 30129.602338\n",
      "iteration 1326, current loss: 30033.254471\n",
      "iteration 1327, current loss: 30074.525682\n",
      "iteration 1328, current loss: 29950.031544\n",
      "iteration 1329, current loss: 29979.860302\n",
      "iteration 1330, current loss: 29859.348755\n",
      "iteration 1331, current loss: 29886.928788\n",
      "iteration 1332, current loss: 29782.556477\n",
      "iteration 1333, current loss: 29805.259815\n",
      "iteration 1334, current loss: 29719.298708\n",
      "iteration 1335, current loss: 29732.320879\n",
      "iteration 1336, current loss: 29667.020674\n",
      "iteration 1337, current loss: 29671.944176\n",
      "iteration 1338, current loss: 29627.457504\n",
      "iteration 1339, current loss: 29636.735146\n",
      "iteration 1340, current loss: 29601.108315\n",
      "iteration 1341, current loss: 29631.431145\n",
      "iteration 1342, current loss: 29627.519477\n",
      "iteration 1343, current loss: 29660.436494\n",
      "iteration 1344, current loss: 29697.783724\n",
      "iteration 1345, current loss: 29721.698981\n",
      "iteration 1346, current loss: 29779.268561\n",
      "iteration 1347, current loss: 29789.923344\n",
      "iteration 1348, current loss: 29844.770109\n",
      "iteration 1349, current loss: 29837.365651\n",
      "iteration 1350, current loss: 29871.986957\n",
      "iteration 1351, current loss: 29831.441159\n",
      "iteration 1352, current loss: 29823.717756\n",
      "iteration 1353, current loss: 29763.624550\n",
      "iteration 1354, current loss: 29721.660497\n",
      "iteration 1355, current loss: 29664.854078\n",
      "iteration 1356, current loss: 29618.878820\n",
      "iteration 1357, current loss: 29577.558051\n",
      "iteration 1358, current loss: 29531.976304\n",
      "iteration 1359, current loss: 29502.157873\n",
      "iteration 1360, current loss: 29482.253179\n",
      "iteration 1361, current loss: 29483.474363\n",
      "iteration 1362, current loss: 29507.649635\n",
      "iteration 1363, current loss: 29542.016756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1364, current loss: 29594.256124\n",
      "iteration 1365, current loss: 29635.570314\n",
      "iteration 1366, current loss: 29634.351445\n",
      "iteration 1367, current loss: 29669.082674\n",
      "iteration 1368, current loss: 29654.037763\n",
      "iteration 1369, current loss: 29659.811624\n",
      "iteration 1370, current loss: 29644.071419\n",
      "iteration 1371, current loss: 29628.351204\n",
      "iteration 1372, current loss: 29614.245140\n",
      "iteration 1373, current loss: 29599.299503\n",
      "iteration 1374, current loss: 29592.453530\n",
      "iteration 1375, current loss: 29601.457128\n",
      "iteration 1376, current loss: 29609.218648\n",
      "iteration 1377, current loss: 29653.380384\n",
      "iteration 1378, current loss: 29691.802604\n",
      "iteration 1379, current loss: 29795.780244\n",
      "iteration 1380, current loss: 29872.014959\n",
      "iteration 1381, current loss: 30058.225484\n",
      "iteration 1382, current loss: 30119.725505\n",
      "iteration 1383, current loss: 30289.760002\n",
      "iteration 1384, current loss: 30209.134249\n",
      "iteration 1385, current loss: 30305.116833\n",
      "iteration 1386, current loss: 30130.247083\n",
      "iteration 1387, current loss: 30153.790789\n",
      "iteration 1388, current loss: 29947.194740\n",
      "iteration 1389, current loss: 29920.171073\n",
      "iteration 1390, current loss: 29721.797475\n",
      "iteration 1391, current loss: 29666.075429\n",
      "iteration 1392, current loss: 29496.390093\n",
      "iteration 1393, current loss: 29434.810553\n",
      "iteration 1394, current loss: 29310.260076\n",
      "iteration 1395, current loss: 29279.344373\n",
      "iteration 1396, current loss: 29211.155469\n",
      "iteration 1397, current loss: 29222.004288\n",
      "iteration 1398, current loss: 29202.800135\n",
      "iteration 1399, current loss: 29271.033320\n",
      "iteration 1400, current loss: 29298.700025\n",
      "iteration 1401, current loss: 29437.520457\n",
      "iteration 1402, current loss: 29427.058061\n",
      "iteration 1403, current loss: 29585.153228\n",
      "iteration 1404, current loss: 29518.234295\n",
      "iteration 1405, current loss: 29656.384084\n",
      "iteration 1406, current loss: 29490.460404\n",
      "iteration 1407, current loss: 29569.861017\n",
      "iteration 1408, current loss: 29426.215977\n",
      "iteration 1409, current loss: 29499.752807\n",
      "iteration 1410, current loss: 29380.785107\n",
      "iteration 1411, current loss: 29435.037795\n",
      "iteration 1412, current loss: 29347.987677\n",
      "iteration 1413, current loss: 29397.267202\n",
      "iteration 1414, current loss: 29326.470845\n",
      "iteration 1415, current loss: 29355.674591\n",
      "iteration 1416, current loss: 29280.473085\n",
      "iteration 1417, current loss: 29281.483766\n",
      "iteration 1418, current loss: 29221.966634\n",
      "iteration 1419, current loss: 29222.398603\n",
      "iteration 1420, current loss: 29176.893919\n",
      "iteration 1421, current loss: 29183.274572\n",
      "iteration 1422, current loss: 29141.639010\n",
      "iteration 1423, current loss: 29149.425007\n",
      "iteration 1424, current loss: 29110.373680\n",
      "iteration 1425, current loss: 29118.578135\n",
      "iteration 1426, current loss: 29083.643217\n",
      "iteration 1427, current loss: 29097.041010\n",
      "iteration 1428, current loss: 29072.879881\n",
      "iteration 1429, current loss: 29101.065885\n",
      "iteration 1430, current loss: 29101.378067\n",
      "iteration 1431, current loss: 29138.872398\n",
      "iteration 1432, current loss: 29187.235386\n",
      "iteration 1433, current loss: 29213.040008\n",
      "iteration 1434, current loss: 29302.661299\n",
      "iteration 1435, current loss: 29336.365432\n",
      "iteration 1436, current loss: 29481.848050\n",
      "iteration 1437, current loss: 29430.230537\n",
      "iteration 1438, current loss: 29598.776665\n",
      "iteration 1439, current loss: 29497.991291\n",
      "iteration 1440, current loss: 29607.384056\n",
      "iteration 1441, current loss: 29490.188005\n",
      "iteration 1442, current loss: 29549.474032\n",
      "iteration 1443, current loss: 29430.673689\n",
      "iteration 1444, current loss: 29460.134288\n",
      "iteration 1445, current loss: 29351.544756\n",
      "iteration 1446, current loss: 29374.886646\n",
      "iteration 1447, current loss: 29305.747707\n",
      "iteration 1448, current loss: 29330.755226\n",
      "iteration 1449, current loss: 29274.319553\n",
      "iteration 1450, current loss: 29301.002719\n",
      "iteration 1451, current loss: 29229.085794\n",
      "iteration 1452, current loss: 29255.847057\n",
      "iteration 1453, current loss: 29187.155772\n",
      "iteration 1454, current loss: 29218.130811\n",
      "iteration 1455, current loss: 29122.437414\n",
      "iteration 1456, current loss: 29163.980631\n",
      "iteration 1457, current loss: 29065.199958\n",
      "iteration 1458, current loss: 29115.673108\n",
      "iteration 1459, current loss: 29022.840809\n",
      "iteration 1460, current loss: 29083.711730\n",
      "iteration 1461, current loss: 28984.990842\n",
      "iteration 1462, current loss: 29051.860437\n",
      "iteration 1463, current loss: 28929.802810\n",
      "iteration 1464, current loss: 28976.973094\n",
      "iteration 1465, current loss: 28877.326588\n",
      "iteration 1466, current loss: 28910.991279\n",
      "iteration 1467, current loss: 28846.157714\n",
      "iteration 1468, current loss: 28885.693690\n",
      "iteration 1469, current loss: 28858.501662\n",
      "iteration 1470, current loss: 28936.092984\n",
      "iteration 1471, current loss: 28943.633930\n",
      "iteration 1472, current loss: 29056.323944\n",
      "iteration 1473, current loss: 29030.667877\n",
      "iteration 1474, current loss: 29131.941839\n",
      "iteration 1475, current loss: 29052.113823\n",
      "iteration 1476, current loss: 29136.949904\n",
      "iteration 1477, current loss: 29030.304237\n",
      "iteration 1478, current loss: 29115.151723\n",
      "iteration 1479, current loss: 29017.472024\n",
      "iteration 1480, current loss: 29088.911871\n",
      "iteration 1481, current loss: 29000.640943\n",
      "iteration 1482, current loss: 29015.373142\n",
      "iteration 1483, current loss: 28924.966238\n",
      "iteration 1484, current loss: 28924.409118\n",
      "iteration 1485, current loss: 28841.038164\n",
      "iteration 1486, current loss: 28823.610772\n",
      "iteration 1487, current loss: 28767.298045\n",
      "iteration 1488, current loss: 28738.001021\n",
      "iteration 1489, current loss: 28720.241950\n",
      "iteration 1490, current loss: 28694.349564\n",
      "iteration 1491, current loss: 28707.023309\n",
      "iteration 1492, current loss: 28691.128717\n",
      "iteration 1493, current loss: 28718.610171\n",
      "iteration 1494, current loss: 28711.383180\n",
      "iteration 1495, current loss: 28744.654821\n",
      "iteration 1496, current loss: 28754.839552\n",
      "iteration 1497, current loss: 28798.768696\n",
      "iteration 1498, current loss: 28803.950086\n",
      "iteration 1499, current loss: 28859.683520\n",
      "iteration 1500, current loss: 28855.447499\n",
      "iteration 1501, current loss: 28930.769858\n",
      "iteration 1502, current loss: 28919.473345\n",
      "iteration 1503, current loss: 29022.142866\n",
      "iteration 1504, current loss: 29003.475045\n",
      "iteration 1505, current loss: 29129.581759\n",
      "iteration 1506, current loss: 29086.245512\n",
      "iteration 1507, current loss: 29217.816053\n",
      "iteration 1508, current loss: 29117.824151\n",
      "iteration 1509, current loss: 29234.620777\n",
      "iteration 1510, current loss: 29105.513098\n",
      "iteration 1511, current loss: 29180.926961\n",
      "iteration 1512, current loss: 29037.445364\n",
      "iteration 1513, current loss: 29064.243401\n",
      "iteration 1514, current loss: 28954.801922\n",
      "iteration 1515, current loss: 28970.664627\n",
      "iteration 1516, current loss: 28959.704526\n",
      "iteration 1517, current loss: 28973.627959\n",
      "iteration 1518, current loss: 28976.370830\n",
      "iteration 1519, current loss: 28951.220709\n",
      "iteration 1520, current loss: 28938.409563\n",
      "iteration 1521, current loss: 28909.023988\n",
      "iteration 1522, current loss: 28873.673812\n",
      "iteration 1523, current loss: 28853.144390\n",
      "iteration 1524, current loss: 28822.819225\n",
      "iteration 1525, current loss: 28841.124376\n",
      "iteration 1526, current loss: 28823.397703\n",
      "iteration 1527, current loss: 28840.649920\n",
      "iteration 1528, current loss: 28798.302085\n",
      "iteration 1529, current loss: 28791.759294\n",
      "iteration 1530, current loss: 28734.874672\n",
      "iteration 1531, current loss: 28709.914866\n",
      "iteration 1532, current loss: 28646.964168\n",
      "iteration 1533, current loss: 28607.104258\n",
      "iteration 1534, current loss: 28554.216092\n",
      "iteration 1535, current loss: 28518.685326\n",
      "iteration 1536, current loss: 28486.934632\n",
      "iteration 1537, current loss: 28468.143538\n",
      "iteration 1538, current loss: 28467.202251\n",
      "iteration 1539, current loss: 28461.433257\n",
      "iteration 1540, current loss: 28480.986520\n",
      "iteration 1541, current loss: 28499.158835\n",
      "iteration 1542, current loss: 28542.500563\n",
      "iteration 1543, current loss: 28573.206041\n",
      "iteration 1544, current loss: 28640.307129\n",
      "iteration 1545, current loss: 28678.065154\n",
      "iteration 1546, current loss: 28736.016153\n",
      "iteration 1547, current loss: 28759.780695\n",
      "iteration 1548, current loss: 28783.050067\n",
      "iteration 1549, current loss: 28797.646762\n",
      "iteration 1550, current loss: 28781.309899\n",
      "iteration 1551, current loss: 28783.689600\n",
      "iteration 1552, current loss: 28734.708195\n",
      "iteration 1553, current loss: 28716.568851\n",
      "iteration 1554, current loss: 28650.386084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1555, current loss: 28622.025154\n",
      "iteration 1556, current loss: 28553.429089\n",
      "iteration 1557, current loss: 28544.744331\n",
      "iteration 1558, current loss: 28484.335279\n",
      "iteration 1559, current loss: 28499.770003\n",
      "iteration 1560, current loss: 28459.516982\n",
      "iteration 1561, current loss: 28479.535687\n",
      "iteration 1562, current loss: 28464.400085\n",
      "iteration 1563, current loss: 28489.171230\n",
      "iteration 1564, current loss: 28483.768513\n",
      "iteration 1565, current loss: 28444.377205\n",
      "iteration 1566, current loss: 28460.610315\n",
      "iteration 1567, current loss: 28397.014251\n",
      "iteration 1568, current loss: 28407.239838\n",
      "iteration 1569, current loss: 28340.312237\n",
      "iteration 1570, current loss: 28334.955845\n",
      "iteration 1571, current loss: 28288.788192\n",
      "iteration 1572, current loss: 28278.499880\n",
      "iteration 1573, current loss: 28240.153022\n",
      "iteration 1574, current loss: 28227.843238\n",
      "iteration 1575, current loss: 28218.448954\n",
      "iteration 1576, current loss: 28228.664970\n",
      "iteration 1577, current loss: 28218.518907\n",
      "iteration 1578, current loss: 28250.352089\n",
      "iteration 1579, current loss: 28241.320597\n",
      "iteration 1580, current loss: 28295.366534\n",
      "iteration 1581, current loss: 28300.246376\n",
      "iteration 1582, current loss: 28391.112681\n",
      "iteration 1583, current loss: 28394.244373\n",
      "iteration 1584, current loss: 28534.998290\n",
      "iteration 1585, current loss: 28512.901082\n",
      "iteration 1586, current loss: 28650.702176\n",
      "iteration 1587, current loss: 28571.092123\n",
      "iteration 1588, current loss: 28685.009178\n",
      "iteration 1589, current loss: 28573.032693\n",
      "iteration 1590, current loss: 28670.628897\n",
      "iteration 1591, current loss: 28545.064038\n",
      "iteration 1592, current loss: 28648.443828\n",
      "iteration 1593, current loss: 28540.113784\n",
      "iteration 1594, current loss: 28660.113387\n",
      "iteration 1595, current loss: 28514.193206\n",
      "iteration 1596, current loss: 28623.770624\n",
      "iteration 1597, current loss: 28485.075792\n",
      "iteration 1598, current loss: 28617.079965\n",
      "iteration 1599, current loss: 28466.883134\n",
      "iteration 1600, current loss: 28599.050407\n",
      "iteration 1601, current loss: 28436.148541\n",
      "iteration 1602, current loss: 28542.730798\n",
      "iteration 1603, current loss: 28390.023416\n",
      "iteration 1604, current loss: 28474.107027\n",
      "iteration 1605, current loss: 28328.014846\n",
      "iteration 1606, current loss: 28388.534838\n",
      "iteration 1607, current loss: 28263.632649\n",
      "iteration 1608, current loss: 28305.207491\n",
      "iteration 1609, current loss: 28204.890762\n",
      "iteration 1610, current loss: 28228.613925\n",
      "iteration 1611, current loss: 28151.857060\n",
      "iteration 1612, current loss: 28160.368913\n",
      "iteration 1613, current loss: 28112.847100\n",
      "iteration 1614, current loss: 28119.410412\n",
      "iteration 1615, current loss: 28099.141549\n",
      "iteration 1616, current loss: 28113.064994\n",
      "iteration 1617, current loss: 28106.925436\n",
      "iteration 1618, current loss: 28127.525407\n",
      "iteration 1619, current loss: 28141.952691\n",
      "iteration 1620, current loss: 28189.621337\n",
      "iteration 1621, current loss: 28265.272065\n",
      "iteration 1622, current loss: 28343.981653\n",
      "iteration 1623, current loss: 28412.837734\n",
      "iteration 1624, current loss: 28495.090152\n",
      "iteration 1625, current loss: 28537.115440\n",
      "iteration 1626, current loss: 28608.156689\n",
      "iteration 1627, current loss: 28603.630724\n",
      "iteration 1628, current loss: 28636.173726\n",
      "iteration 1629, current loss: 28601.314194\n",
      "iteration 1630, current loss: 28589.440506\n",
      "iteration 1631, current loss: 28536.504581\n",
      "iteration 1632, current loss: 28499.638804\n",
      "iteration 1633, current loss: 28448.373334\n",
      "iteration 1634, current loss: 28418.598421\n",
      "iteration 1635, current loss: 28355.028842\n",
      "iteration 1636, current loss: 28345.796924\n",
      "iteration 1637, current loss: 28279.511016\n",
      "iteration 1638, current loss: 28278.216794\n",
      "iteration 1639, current loss: 28220.250505\n",
      "iteration 1640, current loss: 28224.235944\n",
      "iteration 1641, current loss: 28174.138648\n",
      "iteration 1642, current loss: 28187.790744\n",
      "iteration 1643, current loss: 28147.449282\n",
      "iteration 1644, current loss: 28173.780211\n",
      "iteration 1645, current loss: 28133.512106\n",
      "iteration 1646, current loss: 28170.074451\n",
      "iteration 1647, current loss: 28121.491720\n",
      "iteration 1648, current loss: 28163.265033\n",
      "iteration 1649, current loss: 28114.180037\n",
      "iteration 1650, current loss: 28159.019404\n",
      "iteration 1651, current loss: 28116.949996\n",
      "iteration 1652, current loss: 28175.045206\n",
      "iteration 1653, current loss: 28138.833775\n",
      "iteration 1654, current loss: 28243.789372\n",
      "iteration 1655, current loss: 28213.514406\n",
      "iteration 1656, current loss: 28344.156965\n",
      "iteration 1657, current loss: 28277.663911\n",
      "iteration 1658, current loss: 28384.630527\n",
      "iteration 1659, current loss: 28270.644104\n",
      "iteration 1660, current loss: 28334.920424\n",
      "iteration 1661, current loss: 28175.163299\n",
      "iteration 1662, current loss: 28226.290206\n",
      "iteration 1663, current loss: 28056.596358\n",
      "iteration 1664, current loss: 28111.602281\n",
      "iteration 1665, current loss: 27963.358445\n",
      "iteration 1666, current loss: 28014.226335\n",
      "iteration 1667, current loss: 27896.271268\n",
      "iteration 1668, current loss: 27933.938256\n",
      "iteration 1669, current loss: 27839.740334\n",
      "iteration 1670, current loss: 27867.225034\n",
      "iteration 1671, current loss: 27792.842422\n",
      "iteration 1672, current loss: 27810.698802\n",
      "iteration 1673, current loss: 27764.321869\n",
      "iteration 1674, current loss: 27774.307013\n",
      "iteration 1675, current loss: 27761.368010\n",
      "iteration 1676, current loss: 27774.287550\n",
      "iteration 1677, current loss: 27793.586979\n",
      "iteration 1678, current loss: 27804.727627\n",
      "iteration 1679, current loss: 27856.328931\n",
      "iteration 1680, current loss: 27850.848633\n",
      "iteration 1681, current loss: 27915.443449\n",
      "iteration 1682, current loss: 27895.789903\n",
      "iteration 1683, current loss: 27964.896658\n",
      "iteration 1684, current loss: 27935.160036\n",
      "iteration 1685, current loss: 28009.763755\n",
      "iteration 1686, current loss: 27955.060335\n",
      "iteration 1687, current loss: 28032.861315\n",
      "iteration 1688, current loss: 27947.294479\n",
      "iteration 1689, current loss: 28024.718921\n",
      "iteration 1690, current loss: 27927.263379\n",
      "iteration 1691, current loss: 28008.297852\n",
      "iteration 1692, current loss: 27873.350703\n",
      "iteration 1693, current loss: 27936.835386\n",
      "iteration 1694, current loss: 27789.098476\n",
      "iteration 1695, current loss: 27820.265943\n",
      "iteration 1696, current loss: 27732.082260\n",
      "iteration 1697, current loss: 27768.484982\n",
      "iteration 1698, current loss: 27703.414156\n",
      "iteration 1699, current loss: 27737.899058\n",
      "iteration 1700, current loss: 27679.581534\n",
      "iteration 1701, current loss: 27703.681435\n",
      "iteration 1702, current loss: 27647.220223\n",
      "iteration 1703, current loss: 27661.040771\n",
      "iteration 1704, current loss: 27610.156834\n",
      "iteration 1705, current loss: 27617.538702\n",
      "iteration 1706, current loss: 27574.686619\n",
      "iteration 1707, current loss: 27582.777431\n",
      "iteration 1708, current loss: 27560.148188\n",
      "iteration 1709, current loss: 27582.402405\n",
      "iteration 1710, current loss: 27586.956065\n",
      "iteration 1711, current loss: 27635.008065\n",
      "iteration 1712, current loss: 27675.045784\n",
      "iteration 1713, current loss: 27762.315316\n",
      "iteration 1714, current loss: 27854.905074\n",
      "iteration 1715, current loss: 28003.957880\n",
      "iteration 1716, current loss: 28146.244541\n",
      "iteration 1717, current loss: 28353.229370\n",
      "iteration 1718, current loss: 28506.506667\n",
      "iteration 1719, current loss: 28729.825004\n",
      "iteration 1720, current loss: 28791.505357\n",
      "iteration 1721, current loss: 28949.045107\n",
      "iteration 1722, current loss: 28824.194448\n",
      "iteration 1723, current loss: 28863.047316\n",
      "iteration 1724, current loss: 28589.332939\n",
      "iteration 1725, current loss: 28528.106742\n",
      "iteration 1726, current loss: 28223.832511\n",
      "iteration 1727, current loss: 28130.216422\n",
      "iteration 1728, current loss: 27937.686318\n",
      "iteration 1729, current loss: 27879.744654\n",
      "iteration 1730, current loss: 27789.164694\n",
      "iteration 1731, current loss: 27764.383890\n",
      "iteration 1732, current loss: 27733.150552\n",
      "iteration 1733, current loss: 27739.570950\n",
      "iteration 1734, current loss: 27738.061045\n",
      "iteration 1735, current loss: 27778.017606\n",
      "iteration 1736, current loss: 27744.491243\n",
      "iteration 1737, current loss: 27800.841537\n",
      "iteration 1738, current loss: 27732.900274\n",
      "iteration 1739, current loss: 27798.486340\n",
      "iteration 1740, current loss: 27724.133443\n",
      "iteration 1741, current loss: 27799.306184\n",
      "iteration 1742, current loss: 27706.927026\n",
      "iteration 1743, current loss: 27740.647407\n",
      "iteration 1744, current loss: 27634.831425\n",
      "iteration 1745, current loss: 27625.938976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1746, current loss: 27510.890402\n",
      "iteration 1747, current loss: 27464.807553\n",
      "iteration 1748, current loss: 27391.024601\n",
      "iteration 1749, current loss: 27340.664548\n",
      "iteration 1750, current loss: 27325.193347\n",
      "iteration 1751, current loss: 27297.257137\n",
      "iteration 1752, current loss: 27320.329589\n",
      "iteration 1753, current loss: 27307.201461\n",
      "iteration 1754, current loss: 27342.001241\n",
      "iteration 1755, current loss: 27331.926386\n",
      "iteration 1756, current loss: 27358.381455\n",
      "iteration 1757, current loss: 27346.099627\n",
      "iteration 1758, current loss: 27357.983286\n",
      "iteration 1759, current loss: 27349.090951\n",
      "iteration 1760, current loss: 27349.006640\n",
      "iteration 1761, current loss: 27350.115740\n",
      "iteration 1762, current loss: 27345.326124\n",
      "iteration 1763, current loss: 27364.358795\n",
      "iteration 1764, current loss: 27372.776889\n",
      "iteration 1765, current loss: 27422.058713\n",
      "iteration 1766, current loss: 27489.717164\n",
      "iteration 1767, current loss: 27583.468686\n",
      "iteration 1768, current loss: 27751.360481\n",
      "iteration 1769, current loss: 27826.283516\n",
      "iteration 1770, current loss: 28023.424240\n",
      "iteration 1771, current loss: 27911.797535\n",
      "iteration 1772, current loss: 28055.407077\n",
      "iteration 1773, current loss: 27845.673670\n",
      "iteration 1774, current loss: 27933.990239\n",
      "iteration 1775, current loss: 27701.914754\n",
      "iteration 1776, current loss: 27753.538211\n",
      "iteration 1777, current loss: 27558.371276\n",
      "iteration 1778, current loss: 27605.695928\n",
      "iteration 1779, current loss: 27474.773079\n",
      "iteration 1780, current loss: 27533.856989\n",
      "iteration 1781, current loss: 27456.010551\n",
      "iteration 1782, current loss: 27528.850388\n",
      "iteration 1783, current loss: 27481.541508\n",
      "iteration 1784, current loss: 27555.061969\n",
      "iteration 1785, current loss: 27522.076423\n",
      "iteration 1786, current loss: 27592.740744\n",
      "iteration 1787, current loss: 27576.535695\n",
      "iteration 1788, current loss: 27638.691444\n",
      "iteration 1789, current loss: 27640.067269\n",
      "iteration 1790, current loss: 27667.081595\n",
      "iteration 1791, current loss: 27656.799471\n",
      "iteration 1792, current loss: 27644.545162\n",
      "iteration 1793, current loss: 27627.091183\n",
      "iteration 1794, current loss: 27589.444527\n",
      "iteration 1795, current loss: 27602.872830\n",
      "iteration 1796, current loss: 27607.708096\n",
      "iteration 1797, current loss: 27657.610238\n",
      "iteration 1798, current loss: 27700.359040\n",
      "iteration 1799, current loss: 27763.850287\n",
      "iteration 1800, current loss: 27787.167713\n",
      "iteration 1801, current loss: 27818.541501\n",
      "iteration 1802, current loss: 27784.760830\n",
      "iteration 1803, current loss: 27769.727282\n",
      "iteration 1804, current loss: 27680.008431\n",
      "iteration 1805, current loss: 27648.567620\n",
      "iteration 1806, current loss: 27540.242024\n",
      "iteration 1807, current loss: 27512.444206\n",
      "iteration 1808, current loss: 27412.015140\n",
      "iteration 1809, current loss: 27402.336614\n",
      "iteration 1810, current loss: 27322.851314\n",
      "iteration 1811, current loss: 27333.370303\n",
      "iteration 1812, current loss: 27275.989791\n",
      "iteration 1813, current loss: 27300.677422\n",
      "iteration 1814, current loss: 27259.865926\n",
      "iteration 1815, current loss: 27303.275887\n",
      "iteration 1816, current loss: 27271.315862\n",
      "iteration 1817, current loss: 27352.939529\n",
      "iteration 1818, current loss: 27313.833561\n",
      "iteration 1819, current loss: 27450.537328\n",
      "iteration 1820, current loss: 27386.438964\n",
      "iteration 1821, current loss: 27521.957292\n",
      "iteration 1822, current loss: 27379.048729\n",
      "iteration 1823, current loss: 27469.751452\n",
      "iteration 1824, current loss: 27321.414996\n",
      "iteration 1825, current loss: 27345.107043\n",
      "iteration 1826, current loss: 27225.701230\n",
      "iteration 1827, current loss: 27203.516792\n",
      "iteration 1828, current loss: 27120.898952\n",
      "iteration 1829, current loss: 27104.356107\n",
      "iteration 1830, current loss: 27039.028504\n",
      "iteration 1831, current loss: 27033.952082\n",
      "iteration 1832, current loss: 26975.990118\n",
      "iteration 1833, current loss: 26985.153108\n",
      "iteration 1834, current loss: 26940.774228\n",
      "iteration 1835, current loss: 26959.341129\n",
      "iteration 1836, current loss: 26930.583655\n",
      "iteration 1837, current loss: 26963.121101\n",
      "iteration 1838, current loss: 26953.979110\n",
      "iteration 1839, current loss: 27010.012209\n",
      "iteration 1840, current loss: 27030.588169\n",
      "iteration 1841, current loss: 27108.950227\n",
      "iteration 1842, current loss: 27190.137874\n",
      "iteration 1843, current loss: 27282.752716\n",
      "iteration 1844, current loss: 27448.768240\n",
      "iteration 1845, current loss: 27491.665348\n",
      "iteration 1846, current loss: 27699.510336\n",
      "iteration 1847, current loss: 27634.432809\n",
      "iteration 1848, current loss: 27795.059238\n",
      "iteration 1849, current loss: 27589.047146\n",
      "iteration 1850, current loss: 27662.767360\n",
      "iteration 1851, current loss: 27457.844622\n",
      "iteration 1852, current loss: 27483.821156\n",
      "iteration 1853, current loss: 27299.081618\n",
      "iteration 1854, current loss: 27299.293776\n",
      "iteration 1855, current loss: 27148.356524\n",
      "iteration 1856, current loss: 27151.753871\n",
      "iteration 1857, current loss: 27046.832703\n",
      "iteration 1858, current loss: 27065.427007\n",
      "iteration 1859, current loss: 27001.229670\n",
      "iteration 1860, current loss: 27031.480423\n",
      "iteration 1861, current loss: 27000.488515\n",
      "iteration 1862, current loss: 27047.106346\n",
      "iteration 1863, current loss: 27045.864372\n",
      "iteration 1864, current loss: 27127.081099\n",
      "iteration 1865, current loss: 27167.719739\n",
      "iteration 1866, current loss: 27251.847602\n",
      "iteration 1867, current loss: 27297.080197\n",
      "iteration 1868, current loss: 27330.537776\n",
      "iteration 1869, current loss: 27342.239850\n",
      "iteration 1870, current loss: 27364.657684\n",
      "iteration 1871, current loss: 27351.229686\n",
      "iteration 1872, current loss: 27383.018580\n",
      "iteration 1873, current loss: 27313.413664\n",
      "iteration 1874, current loss: 27371.288361\n",
      "iteration 1875, current loss: 27234.848662\n",
      "iteration 1876, current loss: 27318.155682\n",
      "iteration 1877, current loss: 27161.177745\n",
      "iteration 1878, current loss: 27258.238381\n",
      "iteration 1879, current loss: 27154.319808\n",
      "iteration 1880, current loss: 27204.166715\n",
      "iteration 1881, current loss: 27141.198110\n",
      "iteration 1882, current loss: 27154.431620\n",
      "iteration 1883, current loss: 27110.544618\n",
      "iteration 1884, current loss: 27123.425377\n",
      "iteration 1885, current loss: 27128.507038\n",
      "iteration 1886, current loss: 27162.072378\n",
      "iteration 1887, current loss: 27226.512771\n",
      "iteration 1888, current loss: 27325.053108\n",
      "iteration 1889, current loss: 27436.753411\n",
      "iteration 1890, current loss: 27529.844467\n",
      "iteration 1891, current loss: 27608.238408\n",
      "iteration 1892, current loss: 27690.668432\n",
      "iteration 1893, current loss: 27736.358470\n",
      "iteration 1894, current loss: 27808.730495\n",
      "iteration 1895, current loss: 27760.221253\n",
      "iteration 1896, current loss: 27792.910813\n",
      "iteration 1897, current loss: 27665.233571\n",
      "iteration 1898, current loss: 27682.216719\n",
      "iteration 1899, current loss: 27474.058021\n",
      "iteration 1900, current loss: 27481.728937\n",
      "iteration 1901, current loss: 27254.480582\n",
      "iteration 1902, current loss: 27246.338158\n",
      "iteration 1903, current loss: 27041.726131\n",
      "iteration 1904, current loss: 27017.196513\n",
      "iteration 1905, current loss: 26846.987493\n",
      "iteration 1906, current loss: 26815.527923\n",
      "iteration 1907, current loss: 26695.519519\n",
      "iteration 1908, current loss: 26674.789422\n",
      "iteration 1909, current loss: 26598.887602\n",
      "iteration 1910, current loss: 26589.445256\n",
      "iteration 1911, current loss: 26545.406728\n",
      "iteration 1912, current loss: 26548.560119\n",
      "iteration 1913, current loss: 26527.211819\n",
      "iteration 1914, current loss: 26547.820256\n",
      "iteration 1915, current loss: 26552.992341\n",
      "iteration 1916, current loss: 26607.466298\n",
      "iteration 1917, current loss: 26646.859666\n",
      "iteration 1918, current loss: 26758.657246\n",
      "iteration 1919, current loss: 26842.227667\n",
      "iteration 1920, current loss: 27042.203870\n",
      "iteration 1921, current loss: 27114.629147\n",
      "iteration 1922, current loss: 27355.508158\n",
      "iteration 1923, current loss: 27243.728495\n",
      "iteration 1924, current loss: 27399.435440\n",
      "iteration 1925, current loss: 27269.259956\n",
      "iteration 1926, current loss: 27341.642909\n",
      "iteration 1927, current loss: 27228.998782\n",
      "iteration 1928, current loss: 27244.101218\n",
      "iteration 1929, current loss: 27108.466307\n",
      "iteration 1930, current loss: 27106.091887\n",
      "iteration 1931, current loss: 26971.209882\n",
      "iteration 1932, current loss: 26958.211530\n",
      "iteration 1933, current loss: 26842.932998\n",
      "iteration 1934, current loss: 26817.414712\n",
      "iteration 1935, current loss: 26753.874527\n",
      "iteration 1936, current loss: 26742.014107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1937, current loss: 26714.556166\n",
      "iteration 1938, current loss: 26714.642264\n",
      "iteration 1939, current loss: 26709.850399\n",
      "iteration 1940, current loss: 26719.812732\n",
      "iteration 1941, current loss: 26734.210565\n",
      "iteration 1942, current loss: 26752.927133\n",
      "iteration 1943, current loss: 26784.841860\n",
      "iteration 1944, current loss: 26808.591880\n",
      "iteration 1945, current loss: 26854.791291\n",
      "iteration 1946, current loss: 26879.412346\n",
      "iteration 1947, current loss: 26945.523049\n",
      "iteration 1948, current loss: 26979.354526\n",
      "iteration 1949, current loss: 27055.850704\n",
      "iteration 1950, current loss: 27079.207900\n",
      "iteration 1951, current loss: 27139.553542\n",
      "iteration 1952, current loss: 27128.571119\n",
      "iteration 1953, current loss: 27190.563888\n",
      "iteration 1954, current loss: 27131.450902\n",
      "iteration 1955, current loss: 27195.945020\n",
      "iteration 1956, current loss: 27091.917934\n",
      "iteration 1957, current loss: 27150.601518\n",
      "iteration 1958, current loss: 27047.637739\n",
      "iteration 1959, current loss: 27105.544381\n",
      "iteration 1960, current loss: 27007.481420\n",
      "iteration 1961, current loss: 27047.140266\n",
      "iteration 1962, current loss: 26929.805895\n",
      "iteration 1963, current loss: 26927.101328\n",
      "iteration 1964, current loss: 26801.564310\n",
      "iteration 1965, current loss: 26752.238786\n",
      "iteration 1966, current loss: 26650.629454\n",
      "iteration 1967, current loss: 26586.743245\n",
      "iteration 1968, current loss: 26521.493061\n",
      "iteration 1969, current loss: 26474.791255\n",
      "iteration 1970, current loss: 26438.749434\n",
      "iteration 1971, current loss: 26416.521467\n",
      "iteration 1972, current loss: 26401.519486\n",
      "iteration 1973, current loss: 26401.365343\n",
      "iteration 1974, current loss: 26415.396768\n",
      "iteration 1975, current loss: 26455.854328\n",
      "iteration 1976, current loss: 26511.326742\n",
      "iteration 1977, current loss: 26637.962710\n",
      "iteration 1978, current loss: 26744.533033\n",
      "iteration 1979, current loss: 26952.547610\n",
      "iteration 1980, current loss: 26977.527509\n",
      "iteration 1981, current loss: 27021.148515\n",
      "iteration 1982, current loss: 27006.636569\n",
      "iteration 1983, current loss: 26929.153660\n",
      "iteration 1984, current loss: 26861.494623\n",
      "iteration 1985, current loss: 26786.145935\n",
      "iteration 1986, current loss: 26683.069877\n",
      "iteration 1987, current loss: 26572.734562\n",
      "iteration 1988, current loss: 26513.584467\n",
      "iteration 1989, current loss: 26439.326915\n",
      "iteration 1990, current loss: 26396.393367\n",
      "iteration 1991, current loss: 26367.769920\n",
      "iteration 1992, current loss: 26337.452309\n",
      "iteration 1993, current loss: 26343.141153\n",
      "iteration 1994, current loss: 26333.094324\n",
      "iteration 1995, current loss: 26374.036192\n",
      "iteration 1996, current loss: 26321.983170\n",
      "iteration 1997, current loss: 26374.825578\n",
      "iteration 1998, current loss: 26300.115323\n",
      "iteration 1999, current loss: 26355.461027\n",
      "iteration 2000, current loss: 26304.867128\n",
      "iteration 2001, current loss: 26365.135486\n",
      "iteration 2002, current loss: 26344.271255\n",
      "iteration 2003, current loss: 26417.455972\n",
      "iteration 2004, current loss: 26427.735673\n",
      "iteration 2005, current loss: 26525.798460\n",
      "iteration 2006, current loss: 26554.102345\n",
      "iteration 2007, current loss: 26669.464638\n",
      "iteration 2008, current loss: 26706.427874\n",
      "iteration 2009, current loss: 26798.573093\n",
      "iteration 2010, current loss: 26797.186948\n",
      "iteration 2011, current loss: 26855.389360\n",
      "iteration 2012, current loss: 26808.810228\n",
      "iteration 2013, current loss: 26865.559318\n",
      "iteration 2014, current loss: 26750.766310\n",
      "iteration 2015, current loss: 26800.280875\n",
      "iteration 2016, current loss: 26668.426185\n",
      "iteration 2017, current loss: 26730.563991\n",
      "iteration 2018, current loss: 26617.139096\n",
      "iteration 2019, current loss: 26741.920193\n",
      "iteration 2020, current loss: 26597.468911\n",
      "iteration 2021, current loss: 26752.990399\n",
      "iteration 2022, current loss: 26614.582432\n",
      "iteration 2023, current loss: 26768.869157\n",
      "iteration 2024, current loss: 26631.775955\n",
      "iteration 2025, current loss: 26710.677051\n",
      "iteration 2026, current loss: 26565.871682\n",
      "iteration 2027, current loss: 26608.143468\n",
      "iteration 2028, current loss: 26450.058024\n",
      "iteration 2029, current loss: 26461.835006\n",
      "iteration 2030, current loss: 26332.050865\n",
      "iteration 2031, current loss: 26312.022488\n",
      "iteration 2032, current loss: 26217.304282\n",
      "iteration 2033, current loss: 26189.620853\n",
      "iteration 2034, current loss: 26132.180381\n",
      "iteration 2035, current loss: 26117.445571\n",
      "iteration 2036, current loss: 26094.602887\n",
      "iteration 2037, current loss: 26095.051054\n",
      "iteration 2038, current loss: 26095.484764\n",
      "iteration 2039, current loss: 26115.272794\n",
      "iteration 2040, current loss: 26139.447515\n",
      "iteration 2041, current loss: 26182.436254\n",
      "iteration 2042, current loss: 26233.649853\n",
      "iteration 2043, current loss: 26309.033497\n",
      "iteration 2044, current loss: 26381.739736\n",
      "iteration 2045, current loss: 26481.295448\n",
      "iteration 2046, current loss: 26547.907611\n",
      "iteration 2047, current loss: 26634.289244\n",
      "iteration 2048, current loss: 26650.932592\n",
      "iteration 2049, current loss: 26728.258394\n",
      "iteration 2050, current loss: 26730.789096\n",
      "iteration 2051, current loss: 26829.465663\n",
      "iteration 2052, current loss: 26844.580701\n",
      "iteration 2053, current loss: 26951.895807\n",
      "iteration 2054, current loss: 26970.730126\n",
      "iteration 2055, current loss: 27064.786359\n",
      "iteration 2056, current loss: 27087.633524\n",
      "iteration 2057, current loss: 27103.487145\n",
      "iteration 2058, current loss: 27073.194177\n",
      "iteration 2059, current loss: 26969.129345\n",
      "iteration 2060, current loss: 26904.382365\n",
      "iteration 2061, current loss: 26757.727731\n",
      "iteration 2062, current loss: 26717.599796\n",
      "iteration 2063, current loss: 26586.009380\n",
      "iteration 2064, current loss: 26597.943415\n",
      "iteration 2065, current loss: 26527.093902\n",
      "iteration 2066, current loss: 26586.057417\n",
      "iteration 2067, current loss: 26506.572371\n",
      "iteration 2068, current loss: 26585.542771\n",
      "iteration 2069, current loss: 26481.448058\n",
      "iteration 2070, current loss: 26558.626218\n",
      "iteration 2071, current loss: 26436.788533\n",
      "iteration 2072, current loss: 26518.932834\n",
      "iteration 2073, current loss: 26401.894991\n",
      "iteration 2074, current loss: 26522.135542\n",
      "iteration 2075, current loss: 26397.095738\n",
      "iteration 2076, current loss: 26547.896963\n",
      "iteration 2077, current loss: 26406.107307\n",
      "iteration 2078, current loss: 26574.319341\n",
      "iteration 2079, current loss: 26386.756407\n",
      "iteration 2080, current loss: 26552.640850\n",
      "iteration 2081, current loss: 26359.935604\n",
      "iteration 2082, current loss: 26491.633939\n",
      "iteration 2083, current loss: 26326.293287\n",
      "iteration 2084, current loss: 26426.726506\n",
      "iteration 2085, current loss: 26298.840060\n",
      "iteration 2086, current loss: 26373.684225\n",
      "iteration 2087, current loss: 26302.094850\n",
      "iteration 2088, current loss: 26377.157503\n",
      "iteration 2089, current loss: 26330.199784\n",
      "iteration 2090, current loss: 26404.905419\n",
      "iteration 2091, current loss: 26340.696699\n",
      "iteration 2092, current loss: 26426.072443\n",
      "iteration 2093, current loss: 26362.252754\n",
      "iteration 2094, current loss: 26457.912392\n",
      "iteration 2095, current loss: 26410.119504\n",
      "iteration 2096, current loss: 26485.187312\n",
      "iteration 2097, current loss: 26475.339161\n",
      "iteration 2098, current loss: 26483.073652\n",
      "iteration 2099, current loss: 26417.311714\n",
      "iteration 2100, current loss: 26394.964576\n",
      "iteration 2101, current loss: 26340.036049\n",
      "iteration 2102, current loss: 26330.767687\n",
      "iteration 2103, current loss: 26303.195771\n",
      "iteration 2104, current loss: 26291.857839\n",
      "iteration 2105, current loss: 26270.211209\n",
      "iteration 2106, current loss: 26238.772923\n",
      "iteration 2107, current loss: 26218.049884\n",
      "iteration 2108, current loss: 26162.678550\n",
      "iteration 2109, current loss: 26135.012419\n",
      "iteration 2110, current loss: 26076.850110\n",
      "iteration 2111, current loss: 26056.249503\n",
      "iteration 2112, current loss: 26017.062618\n",
      "iteration 2113, current loss: 26029.747343\n",
      "iteration 2114, current loss: 26012.611154\n",
      "iteration 2115, current loss: 26044.267321\n",
      "iteration 2116, current loss: 26029.671953\n",
      "iteration 2117, current loss: 26070.455502\n",
      "iteration 2118, current loss: 26050.127245\n",
      "iteration 2119, current loss: 26098.640935\n",
      "iteration 2120, current loss: 26063.833574\n",
      "iteration 2121, current loss: 26115.778347\n",
      "iteration 2122, current loss: 26063.737169\n",
      "iteration 2123, current loss: 26117.727832\n",
      "iteration 2124, current loss: 26073.029455\n",
      "iteration 2125, current loss: 26140.838197\n",
      "iteration 2126, current loss: 26070.633310\n",
      "iteration 2127, current loss: 26141.644951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2128, current loss: 26054.415147\n",
      "iteration 2129, current loss: 26124.738625\n",
      "iteration 2130, current loss: 26030.610478\n",
      "iteration 2131, current loss: 26103.535656\n",
      "iteration 2132, current loss: 26013.350933\n",
      "iteration 2133, current loss: 26090.493318\n",
      "iteration 2134, current loss: 26016.515924\n",
      "iteration 2135, current loss: 26117.923439\n",
      "iteration 2136, current loss: 26096.837912\n",
      "iteration 2137, current loss: 26250.474204\n",
      "iteration 2138, current loss: 26277.103266\n",
      "iteration 2139, current loss: 26406.613255\n",
      "iteration 2140, current loss: 26414.086915\n",
      "iteration 2141, current loss: 26552.928094\n",
      "iteration 2142, current loss: 26485.005353\n",
      "iteration 2143, current loss: 26630.123201\n",
      "iteration 2144, current loss: 26533.708370\n",
      "iteration 2145, current loss: 26678.486167\n",
      "iteration 2146, current loss: 26546.874953\n",
      "iteration 2147, current loss: 26637.092736\n",
      "iteration 2148, current loss: 26469.233131\n",
      "iteration 2149, current loss: 26487.528160\n",
      "iteration 2150, current loss: 26299.413565\n",
      "iteration 2151, current loss: 26267.969882\n",
      "iteration 2152, current loss: 26081.612555\n",
      "iteration 2153, current loss: 26005.348171\n",
      "iteration 2154, current loss: 25911.408014\n",
      "iteration 2155, current loss: 25847.658289\n",
      "iteration 2156, current loss: 25806.002165\n",
      "iteration 2157, current loss: 25763.977182\n",
      "iteration 2158, current loss: 25748.227180\n",
      "iteration 2159, current loss: 25719.743751\n",
      "iteration 2160, current loss: 25719.183403\n",
      "iteration 2161, current loss: 25704.803966\n",
      "iteration 2162, current loss: 25717.912472\n",
      "iteration 2163, current loss: 25723.288006\n",
      "iteration 2164, current loss: 25753.510187\n",
      "iteration 2165, current loss: 25785.181707\n",
      "iteration 2166, current loss: 25836.850044\n",
      "iteration 2167, current loss: 25886.921974\n",
      "iteration 2168, current loss: 25975.232658\n",
      "iteration 2169, current loss: 25968.175266\n",
      "iteration 2170, current loss: 26071.289703\n",
      "iteration 2171, current loss: 26048.199527\n",
      "iteration 2172, current loss: 26185.384746\n",
      "iteration 2173, current loss: 26203.943066\n",
      "iteration 2174, current loss: 26365.993131\n",
      "iteration 2175, current loss: 26344.248942\n",
      "iteration 2176, current loss: 26392.042412\n",
      "iteration 2177, current loss: 26290.629906\n",
      "iteration 2178, current loss: 26298.209313\n",
      "iteration 2179, current loss: 26161.442037\n",
      "iteration 2180, current loss: 26151.759224\n",
      "iteration 2181, current loss: 25998.367358\n",
      "iteration 2182, current loss: 25960.895811\n",
      "iteration 2183, current loss: 25816.725819\n",
      "iteration 2184, current loss: 25748.523999\n",
      "iteration 2185, current loss: 25642.743955\n",
      "iteration 2186, current loss: 25583.222610\n",
      "iteration 2187, current loss: 25524.610605\n",
      "iteration 2188, current loss: 25500.540982\n",
      "iteration 2189, current loss: 25475.703324\n",
      "iteration 2190, current loss: 25477.858921\n",
      "iteration 2191, current loss: 25473.760735\n",
      "iteration 2192, current loss: 25504.888698\n",
      "iteration 2193, current loss: 25527.303089\n",
      "iteration 2194, current loss: 25615.414990\n",
      "iteration 2195, current loss: 25699.772975\n",
      "iteration 2196, current loss: 25869.384788\n",
      "iteration 2197, current loss: 26009.397232\n",
      "iteration 2198, current loss: 26147.775543\n",
      "iteration 2199, current loss: 26222.421727\n",
      "iteration 2200, current loss: 26288.712861\n",
      "iteration 2201, current loss: 26315.314860\n",
      "iteration 2202, current loss: 26372.396030\n",
      "iteration 2203, current loss: 26356.201273\n",
      "iteration 2204, current loss: 26420.599256\n",
      "iteration 2205, current loss: 26372.404280\n",
      "iteration 2206, current loss: 26461.565245\n",
      "iteration 2207, current loss: 26406.059214\n",
      "iteration 2208, current loss: 26500.367282\n",
      "iteration 2209, current loss: 26398.337839\n",
      "iteration 2210, current loss: 26493.286655\n",
      "iteration 2211, current loss: 26339.691760\n",
      "iteration 2212, current loss: 26392.161661\n",
      "iteration 2213, current loss: 26227.314552\n",
      "iteration 2214, current loss: 26229.439269\n",
      "iteration 2215, current loss: 26062.930649\n",
      "iteration 2216, current loss: 26025.064450\n",
      "iteration 2217, current loss: 25870.717633\n",
      "iteration 2218, current loss: 25819.765844\n",
      "iteration 2219, current loss: 25700.126314\n",
      "iteration 2220, current loss: 25675.706392\n",
      "iteration 2221, current loss: 25592.713217\n",
      "iteration 2222, current loss: 25579.919227\n",
      "iteration 2223, current loss: 25524.920788\n",
      "iteration 2224, current loss: 25514.442033\n",
      "iteration 2225, current loss: 25481.614181\n",
      "iteration 2226, current loss: 25475.952150\n",
      "iteration 2227, current loss: 25463.234354\n",
      "iteration 2228, current loss: 25459.747861\n",
      "iteration 2229, current loss: 25462.310964\n",
      "iteration 2230, current loss: 25461.049169\n",
      "iteration 2231, current loss: 25472.566366\n",
      "iteration 2232, current loss: 25474.429433\n",
      "iteration 2233, current loss: 25489.786854\n",
      "iteration 2234, current loss: 25495.639462\n",
      "iteration 2235, current loss: 25508.522019\n",
      "iteration 2236, current loss: 25519.309208\n",
      "iteration 2237, current loss: 25517.506131\n",
      "iteration 2238, current loss: 25530.505984\n",
      "iteration 2239, current loss: 25519.290788\n",
      "iteration 2240, current loss: 25532.354212\n",
      "iteration 2241, current loss: 25518.218817\n",
      "iteration 2242, current loss: 25525.328863\n",
      "iteration 2243, current loss: 25507.134172\n",
      "iteration 2244, current loss: 25511.173377\n",
      "iteration 2245, current loss: 25500.774134\n",
      "iteration 2246, current loss: 25545.654199\n",
      "iteration 2247, current loss: 25560.715091\n",
      "iteration 2248, current loss: 25680.865225\n",
      "iteration 2249, current loss: 25691.286262\n",
      "iteration 2250, current loss: 25866.779328\n",
      "iteration 2251, current loss: 25869.231788\n",
      "iteration 2252, current loss: 25996.744664\n",
      "iteration 2253, current loss: 25908.139805\n",
      "iteration 2254, current loss: 26013.112933\n",
      "iteration 2255, current loss: 25898.927975\n",
      "iteration 2256, current loss: 25967.729796\n",
      "iteration 2257, current loss: 25883.255629\n",
      "iteration 2258, current loss: 25888.250452\n",
      "iteration 2259, current loss: 25849.671368\n",
      "iteration 2260, current loss: 25812.827974\n",
      "iteration 2261, current loss: 25794.383564\n",
      "iteration 2262, current loss: 25737.610947\n",
      "iteration 2263, current loss: 25774.576015\n",
      "iteration 2264, current loss: 25685.276359\n",
      "iteration 2265, current loss: 25752.097924\n",
      "iteration 2266, current loss: 25638.976742\n",
      "iteration 2267, current loss: 25698.783353\n",
      "iteration 2268, current loss: 25601.006814\n",
      "iteration 2269, current loss: 25667.496151\n",
      "iteration 2270, current loss: 25623.308120\n",
      "iteration 2271, current loss: 25721.104045\n",
      "iteration 2272, current loss: 25737.237937\n",
      "iteration 2273, current loss: 25872.399102\n",
      "iteration 2274, current loss: 25943.348828\n",
      "iteration 2275, current loss: 26119.029045\n",
      "iteration 2276, current loss: 26131.279288\n",
      "iteration 2277, current loss: 26303.424839\n",
      "iteration 2278, current loss: 26143.262766\n",
      "iteration 2279, current loss: 26248.029315\n",
      "iteration 2280, current loss: 26033.507897\n",
      "iteration 2281, current loss: 26121.858768\n",
      "iteration 2282, current loss: 25918.825919\n",
      "iteration 2283, current loss: 26023.188678\n",
      "iteration 2284, current loss: 25848.711781\n",
      "iteration 2285, current loss: 25963.840672\n",
      "iteration 2286, current loss: 25802.485040\n",
      "iteration 2287, current loss: 25901.465985\n",
      "iteration 2288, current loss: 25754.875344\n",
      "iteration 2289, current loss: 25853.097342\n",
      "iteration 2290, current loss: 25680.663999\n",
      "iteration 2291, current loss: 25758.875715\n",
      "iteration 2292, current loss: 25575.117790\n",
      "iteration 2293, current loss: 25621.949857\n",
      "iteration 2294, current loss: 25488.082107\n",
      "iteration 2295, current loss: 25506.185897\n",
      "iteration 2296, current loss: 25499.035692\n",
      "iteration 2297, current loss: 25500.649922\n",
      "iteration 2298, current loss: 25537.995084\n",
      "iteration 2299, current loss: 25563.465296\n",
      "iteration 2300, current loss: 25626.176344\n",
      "iteration 2301, current loss: 25683.083096\n",
      "iteration 2302, current loss: 25751.446598\n",
      "iteration 2303, current loss: 25818.782066\n",
      "iteration 2304, current loss: 25858.429803\n",
      "iteration 2305, current loss: 25928.270360\n",
      "iteration 2306, current loss: 25934.828149\n",
      "iteration 2307, current loss: 25984.951043\n",
      "iteration 2308, current loss: 25951.343126\n",
      "iteration 2309, current loss: 25957.124400\n",
      "iteration 2310, current loss: 25870.715619\n",
      "iteration 2311, current loss: 25843.297465\n",
      "iteration 2312, current loss: 25712.317164\n",
      "iteration 2313, current loss: 25646.754959\n",
      "iteration 2314, current loss: 25518.495400\n",
      "iteration 2315, current loss: 25438.237315\n",
      "iteration 2316, current loss: 25331.637662\n",
      "iteration 2317, current loss: 25267.725427\n",
      "iteration 2318, current loss: 25198.424372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2319, current loss: 25171.892223\n",
      "iteration 2320, current loss: 25130.695862\n",
      "iteration 2321, current loss: 25126.075839\n",
      "iteration 2322, current loss: 25107.364964\n",
      "iteration 2323, current loss: 25128.619000\n",
      "iteration 2324, current loss: 25131.664769\n",
      "iteration 2325, current loss: 25197.684708\n",
      "iteration 2326, current loss: 25212.421541\n",
      "iteration 2327, current loss: 25313.147025\n",
      "iteration 2328, current loss: 25355.603459\n",
      "iteration 2329, current loss: 25504.825752\n",
      "iteration 2330, current loss: 25532.446186\n",
      "iteration 2331, current loss: 25706.254098\n",
      "iteration 2332, current loss: 25680.765311\n",
      "iteration 2333, current loss: 25844.919306\n",
      "iteration 2334, current loss: 25796.068623\n",
      "iteration 2335, current loss: 25925.665236\n",
      "iteration 2336, current loss: 25833.131646\n",
      "iteration 2337, current loss: 25910.700774\n",
      "iteration 2338, current loss: 25773.454818\n",
      "iteration 2339, current loss: 25819.754566\n",
      "iteration 2340, current loss: 25661.642552\n",
      "iteration 2341, current loss: 25692.244591\n",
      "iteration 2342, current loss: 25515.279041\n",
      "iteration 2343, current loss: 25520.266131\n",
      "iteration 2344, current loss: 25368.503039\n",
      "iteration 2345, current loss: 25365.784316\n",
      "iteration 2346, current loss: 25244.703855\n",
      "iteration 2347, current loss: 25237.597683\n",
      "iteration 2348, current loss: 25141.061509\n",
      "iteration 2349, current loss: 25132.797709\n",
      "iteration 2350, current loss: 25065.415118\n",
      "iteration 2351, current loss: 25059.334229\n",
      "iteration 2352, current loss: 25018.635663\n",
      "iteration 2353, current loss: 25018.016057\n",
      "iteration 2354, current loss: 24990.451007\n",
      "iteration 2355, current loss: 24998.508656\n",
      "iteration 2356, current loss: 24980.090158\n",
      "iteration 2357, current loss: 25005.980660\n",
      "iteration 2358, current loss: 25006.075952\n",
      "iteration 2359, current loss: 25073.789554\n",
      "iteration 2360, current loss: 25107.326949\n",
      "iteration 2361, current loss: 25253.191527\n",
      "iteration 2362, current loss: 25340.781254\n",
      "iteration 2363, current loss: 25541.175571\n",
      "iteration 2364, current loss: 25589.321735\n",
      "iteration 2365, current loss: 25769.370769\n",
      "iteration 2366, current loss: 25715.060161\n",
      "iteration 2367, current loss: 25831.837837\n",
      "iteration 2368, current loss: 25679.252528\n",
      "iteration 2369, current loss: 25748.744212\n",
      "iteration 2370, current loss: 25576.822349\n",
      "iteration 2371, current loss: 25611.039630\n",
      "iteration 2372, current loss: 25466.242737\n",
      "iteration 2373, current loss: 25466.492845\n",
      "iteration 2374, current loss: 25393.305512\n",
      "iteration 2375, current loss: 25406.846948\n",
      "iteration 2376, current loss: 25432.533256\n",
      "iteration 2377, current loss: 25468.727195\n",
      "iteration 2378, current loss: 25563.085865\n",
      "iteration 2379, current loss: 25564.065342\n",
      "iteration 2380, current loss: 25647.675184\n",
      "iteration 2381, current loss: 25611.900487\n",
      "iteration 2382, current loss: 25667.654630\n",
      "iteration 2383, current loss: 25568.465340\n",
      "iteration 2384, current loss: 25605.248896\n",
      "iteration 2385, current loss: 25481.117269\n",
      "iteration 2386, current loss: 25497.842040\n",
      "iteration 2387, current loss: 25393.701024\n",
      "iteration 2388, current loss: 25423.582373\n",
      "iteration 2389, current loss: 25345.546611\n",
      "iteration 2390, current loss: 25411.236366\n",
      "iteration 2391, current loss: 25377.382509\n",
      "iteration 2392, current loss: 25481.777031\n",
      "iteration 2393, current loss: 25473.028513\n",
      "iteration 2394, current loss: 25616.113197\n",
      "iteration 2395, current loss: 25627.486484\n",
      "iteration 2396, current loss: 25784.343849\n",
      "iteration 2397, current loss: 25788.094270\n",
      "iteration 2398, current loss: 25915.718658\n",
      "iteration 2399, current loss: 25851.212338\n",
      "iteration 2400, current loss: 25959.656132\n",
      "iteration 2401, current loss: 25793.198158\n",
      "iteration 2402, current loss: 25843.400446\n",
      "iteration 2403, current loss: 25626.508116\n",
      "iteration 2404, current loss: 25635.321557\n",
      "iteration 2405, current loss: 25423.902198\n",
      "iteration 2406, current loss: 25399.527043\n",
      "iteration 2407, current loss: 25240.466811\n",
      "iteration 2408, current loss: 25198.396385\n",
      "iteration 2409, current loss: 25096.570365\n",
      "iteration 2410, current loss: 25042.918231\n",
      "iteration 2411, current loss: 24980.505335\n",
      "iteration 2412, current loss: 24937.819374\n",
      "iteration 2413, current loss: 24899.084954\n",
      "iteration 2414, current loss: 24864.737301\n",
      "iteration 2415, current loss: 24841.216062\n",
      "iteration 2416, current loss: 24814.965637\n",
      "iteration 2417, current loss: 24816.660254\n",
      "iteration 2418, current loss: 24804.620387\n",
      "iteration 2419, current loss: 24826.677604\n",
      "iteration 2420, current loss: 24836.460694\n",
      "iteration 2421, current loss: 24884.026197\n",
      "iteration 2422, current loss: 24924.764493\n",
      "iteration 2423, current loss: 24998.582065\n",
      "iteration 2424, current loss: 25028.667203\n",
      "iteration 2425, current loss: 25130.681494\n",
      "iteration 2426, current loss: 25119.768117\n",
      "iteration 2427, current loss: 25219.098283\n",
      "iteration 2428, current loss: 25164.559267\n",
      "iteration 2429, current loss: 25266.452363\n",
      "iteration 2430, current loss: 25239.760143\n",
      "iteration 2431, current loss: 25383.653063\n",
      "iteration 2432, current loss: 25324.825187\n",
      "iteration 2433, current loss: 25472.436642\n",
      "iteration 2434, current loss: 25382.979707\n",
      "iteration 2435, current loss: 25530.423286\n",
      "iteration 2436, current loss: 25387.536063\n",
      "iteration 2437, current loss: 25528.429050\n",
      "iteration 2438, current loss: 25297.997749\n",
      "iteration 2439, current loss: 25400.094860\n",
      "iteration 2440, current loss: 25174.599869\n",
      "iteration 2441, current loss: 25262.444525\n",
      "iteration 2442, current loss: 25099.096206\n",
      "iteration 2443, current loss: 25174.578929\n",
      "iteration 2444, current loss: 25064.021660\n",
      "iteration 2445, current loss: 25137.439740\n",
      "iteration 2446, current loss: 25017.189765\n",
      "iteration 2447, current loss: 25067.900297\n",
      "iteration 2448, current loss: 24977.238865\n",
      "iteration 2449, current loss: 25030.662178\n",
      "iteration 2450, current loss: 24992.287594\n",
      "iteration 2451, current loss: 25067.907347\n",
      "iteration 2452, current loss: 25055.802943\n",
      "iteration 2453, current loss: 25161.557383\n",
      "iteration 2454, current loss: 25172.373292\n",
      "iteration 2455, current loss: 25267.555239\n",
      "iteration 2456, current loss: 25255.577915\n",
      "iteration 2457, current loss: 25374.745492\n",
      "iteration 2458, current loss: 25336.863927\n",
      "iteration 2459, current loss: 25479.583147\n",
      "iteration 2460, current loss: 25434.261525\n",
      "iteration 2461, current loss: 25601.835309\n",
      "iteration 2462, current loss: 25578.922217\n",
      "iteration 2463, current loss: 25751.296033\n",
      "iteration 2464, current loss: 25696.404131\n",
      "iteration 2465, current loss: 25806.648332\n",
      "iteration 2466, current loss: 25639.908154\n",
      "iteration 2467, current loss: 25669.211651\n",
      "iteration 2468, current loss: 25439.402300\n",
      "iteration 2469, current loss: 25401.040799\n",
      "iteration 2470, current loss: 25183.123986\n",
      "iteration 2471, current loss: 25134.679282\n",
      "iteration 2472, current loss: 24979.884593\n",
      "iteration 2473, current loss: 24956.467735\n",
      "iteration 2474, current loss: 24860.535614\n",
      "iteration 2475, current loss: 24863.917372\n",
      "iteration 2476, current loss: 24801.933836\n",
      "iteration 2477, current loss: 24828.719627\n",
      "iteration 2478, current loss: 24776.118990\n",
      "iteration 2479, current loss: 24828.261541\n",
      "iteration 2480, current loss: 24770.505104\n",
      "iteration 2481, current loss: 24840.285379\n",
      "iteration 2482, current loss: 24776.139099\n",
      "iteration 2483, current loss: 24858.038068\n",
      "iteration 2484, current loss: 24786.794857\n",
      "iteration 2485, current loss: 24887.410460\n",
      "iteration 2486, current loss: 24804.413372\n",
      "iteration 2487, current loss: 24943.936148\n",
      "iteration 2488, current loss: 24888.772772\n",
      "iteration 2489, current loss: 25073.943533\n",
      "iteration 2490, current loss: 24967.032134\n",
      "iteration 2491, current loss: 25173.801965\n",
      "iteration 2492, current loss: 25022.084510\n",
      "iteration 2493, current loss: 25214.274491\n",
      "iteration 2494, current loss: 25040.435495\n",
      "iteration 2495, current loss: 25197.180058\n",
      "iteration 2496, current loss: 25046.786217\n",
      "iteration 2497, current loss: 25191.735956\n",
      "iteration 2498, current loss: 25105.354828\n",
      "iteration 2499, current loss: 25281.107220\n",
      "iteration 2500, current loss: 25167.281037\n",
      "iteration 2501, current loss: 25306.031397\n",
      "iteration 2502, current loss: 25170.440783\n",
      "iteration 2503, current loss: 25227.223095\n",
      "iteration 2504, current loss: 25101.099495\n",
      "iteration 2505, current loss: 25070.934721\n",
      "iteration 2506, current loss: 24994.480918\n",
      "iteration 2507, current loss: 24919.226060\n",
      "iteration 2508, current loss: 24905.033014\n",
      "iteration 2509, current loss: 24835.915309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2510, current loss: 24850.728106\n",
      "iteration 2511, current loss: 24798.722425\n",
      "iteration 2512, current loss: 24823.222410\n",
      "iteration 2513, current loss: 24776.479280\n",
      "iteration 2514, current loss: 24799.386240\n",
      "iteration 2515, current loss: 24777.043076\n",
      "iteration 2516, current loss: 24793.198051\n",
      "iteration 2517, current loss: 24794.795830\n",
      "iteration 2518, current loss: 24812.868659\n",
      "iteration 2519, current loss: 24831.247232\n",
      "iteration 2520, current loss: 24872.480356\n",
      "iteration 2521, current loss: 24902.727990\n",
      "iteration 2522, current loss: 24981.694480\n",
      "iteration 2523, current loss: 24956.459457\n",
      "iteration 2524, current loss: 25040.435424\n",
      "iteration 2525, current loss: 24972.468040\n",
      "iteration 2526, current loss: 25084.549219\n",
      "iteration 2527, current loss: 24943.455233\n",
      "iteration 2528, current loss: 25029.647041\n",
      "iteration 2529, current loss: 24892.523251\n",
      "iteration 2530, current loss: 24948.028452\n",
      "iteration 2531, current loss: 24837.960509\n",
      "iteration 2532, current loss: 24877.194800\n",
      "iteration 2533, current loss: 24778.283378\n",
      "iteration 2534, current loss: 24803.706320\n",
      "iteration 2535, current loss: 24731.448818\n",
      "iteration 2536, current loss: 24746.375916\n",
      "iteration 2537, current loss: 24753.735565\n",
      "iteration 2538, current loss: 24774.398828\n",
      "iteration 2539, current loss: 24835.025634\n",
      "iteration 2540, current loss: 24861.107422\n",
      "iteration 2541, current loss: 24993.125318\n",
      "iteration 2542, current loss: 25050.431392\n",
      "iteration 2543, current loss: 25274.023889\n",
      "iteration 2544, current loss: 25341.942748\n",
      "iteration 2545, current loss: 25602.822528\n",
      "iteration 2546, current loss: 25658.617889\n",
      "iteration 2547, current loss: 25828.936359\n",
      "iteration 2548, current loss: 25737.145026\n",
      "iteration 2549, current loss: 25796.374021\n",
      "iteration 2550, current loss: 25577.989449\n",
      "iteration 2551, current loss: 25589.003542\n",
      "iteration 2552, current loss: 25279.788965\n",
      "iteration 2553, current loss: 25245.334464\n",
      "iteration 2554, current loss: 24977.509449\n",
      "iteration 2555, current loss: 24933.035508\n",
      "iteration 2556, current loss: 24756.993521\n",
      "iteration 2557, current loss: 24740.282938\n",
      "iteration 2558, current loss: 24622.486369\n",
      "iteration 2559, current loss: 24629.512566\n",
      "iteration 2560, current loss: 24545.429821\n",
      "iteration 2561, current loss: 24558.106061\n",
      "iteration 2562, current loss: 24488.257273\n",
      "iteration 2563, current loss: 24525.010258\n",
      "iteration 2564, current loss: 24461.933381\n",
      "iteration 2565, current loss: 24526.809457\n",
      "iteration 2566, current loss: 24474.566021\n",
      "iteration 2567, current loss: 24571.090529\n",
      "iteration 2568, current loss: 24525.898593\n",
      "iteration 2569, current loss: 24664.147807\n",
      "iteration 2570, current loss: 24618.176173\n",
      "iteration 2571, current loss: 24808.651868\n",
      "iteration 2572, current loss: 24734.455241\n",
      "iteration 2573, current loss: 24950.362645\n",
      "iteration 2574, current loss: 24835.041751\n",
      "iteration 2575, current loss: 25074.252399\n",
      "iteration 2576, current loss: 24906.494122\n",
      "iteration 2577, current loss: 25111.075143\n",
      "iteration 2578, current loss: 24889.813170\n",
      "iteration 2579, current loss: 24994.989624\n",
      "iteration 2580, current loss: 24826.334429\n",
      "iteration 2581, current loss: 24858.668688\n",
      "iteration 2582, current loss: 24739.325437\n",
      "iteration 2583, current loss: 24723.316657\n",
      "iteration 2584, current loss: 24599.252515\n",
      "iteration 2585, current loss: 24574.809031\n",
      "iteration 2586, current loss: 24522.180496\n",
      "iteration 2587, current loss: 24536.210474\n",
      "iteration 2588, current loss: 24523.867998\n",
      "iteration 2589, current loss: 24554.804257\n",
      "iteration 2590, current loss: 24568.590230\n",
      "iteration 2591, current loss: 24618.093770\n",
      "iteration 2592, current loss: 24667.933111\n",
      "iteration 2593, current loss: 24716.712721\n",
      "iteration 2594, current loss: 24778.647185\n",
      "iteration 2595, current loss: 24773.510395\n",
      "iteration 2596, current loss: 24823.544728\n",
      "iteration 2597, current loss: 24811.450033\n",
      "iteration 2598, current loss: 24840.313753\n",
      "iteration 2599, current loss: 24831.930821\n",
      "iteration 2600, current loss: 24838.646803\n",
      "iteration 2601, current loss: 24867.531295\n",
      "iteration 2602, current loss: 24856.455037\n",
      "iteration 2603, current loss: 24892.365960\n",
      "iteration 2604, current loss: 24873.771923\n",
      "iteration 2605, current loss: 24907.619588\n",
      "iteration 2606, current loss: 24928.118242\n",
      "iteration 2607, current loss: 24967.586227\n",
      "iteration 2608, current loss: 25073.829906\n",
      "iteration 2609, current loss: 25095.237560\n",
      "iteration 2610, current loss: 25271.535580\n",
      "iteration 2611, current loss: 25254.335350\n",
      "iteration 2612, current loss: 25468.527082\n",
      "iteration 2613, current loss: 25334.994644\n",
      "iteration 2614, current loss: 25484.882435\n",
      "iteration 2615, current loss: 25228.322462\n",
      "iteration 2616, current loss: 25274.348733\n",
      "iteration 2617, current loss: 24965.523096\n",
      "iteration 2618, current loss: 24949.486431\n",
      "iteration 2619, current loss: 24702.008799\n",
      "iteration 2620, current loss: 24679.836869\n",
      "iteration 2621, current loss: 24525.824361\n",
      "iteration 2622, current loss: 24508.836417\n",
      "iteration 2623, current loss: 24413.657957\n",
      "iteration 2624, current loss: 24401.158813\n",
      "iteration 2625, current loss: 24325.083997\n",
      "iteration 2626, current loss: 24318.943443\n",
      "iteration 2627, current loss: 24263.916938\n",
      "iteration 2628, current loss: 24271.475532\n",
      "iteration 2629, current loss: 24242.348758\n",
      "iteration 2630, current loss: 24266.279198\n",
      "iteration 2631, current loss: 24261.768549\n",
      "iteration 2632, current loss: 24310.100138\n",
      "iteration 2633, current loss: 24334.849785\n",
      "iteration 2634, current loss: 24417.755702\n",
      "iteration 2635, current loss: 24476.622835\n",
      "iteration 2636, current loss: 24663.464391\n",
      "iteration 2637, current loss: 24714.920161\n",
      "iteration 2638, current loss: 24947.539367\n",
      "iteration 2639, current loss: 24812.345757\n",
      "iteration 2640, current loss: 25006.276696\n",
      "iteration 2641, current loss: 24785.640905\n",
      "iteration 2642, current loss: 24938.988594\n",
      "iteration 2643, current loss: 24710.978623\n",
      "iteration 2644, current loss: 24831.458979\n",
      "iteration 2645, current loss: 24611.945458\n",
      "iteration 2646, current loss: 24705.196710\n",
      "iteration 2647, current loss: 24542.923268\n",
      "iteration 2648, current loss: 24612.553676\n",
      "iteration 2649, current loss: 24514.801954\n",
      "iteration 2650, current loss: 24540.163673\n",
      "iteration 2651, current loss: 24494.854411\n",
      "iteration 2652, current loss: 24502.649439\n",
      "iteration 2653, current loss: 24491.193937\n",
      "iteration 2654, current loss: 24488.308280\n",
      "iteration 2655, current loss: 24481.509332\n",
      "iteration 2656, current loss: 24461.570662\n",
      "iteration 2657, current loss: 24438.376408\n",
      "iteration 2658, current loss: 24405.226537\n",
      "iteration 2659, current loss: 24362.954787\n",
      "iteration 2660, current loss: 24332.191503\n",
      "iteration 2661, current loss: 24293.812427\n",
      "iteration 2662, current loss: 24265.448145\n",
      "iteration 2663, current loss: 24238.018233\n",
      "iteration 2664, current loss: 24208.187364\n",
      "iteration 2665, current loss: 24209.301307\n",
      "iteration 2666, current loss: 24192.751059\n",
      "iteration 2667, current loss: 24222.361290\n",
      "iteration 2668, current loss: 24223.134805\n",
      "iteration 2669, current loss: 24293.284185\n",
      "iteration 2670, current loss: 24334.198082\n",
      "iteration 2671, current loss: 24450.772131\n",
      "iteration 2672, current loss: 24559.840967\n",
      "iteration 2673, current loss: 24689.928470\n",
      "iteration 2674, current loss: 24814.454207\n",
      "iteration 2675, current loss: 24933.988590\n",
      "iteration 2676, current loss: 25065.532271\n",
      "iteration 2677, current loss: 25196.592593\n",
      "iteration 2678, current loss: 25285.444320\n",
      "iteration 2679, current loss: 25386.084155\n",
      "iteration 2680, current loss: 25375.698144\n",
      "iteration 2681, current loss: 25411.873200\n",
      "iteration 2682, current loss: 25239.347851\n",
      "iteration 2683, current loss: 25179.497381\n",
      "iteration 2684, current loss: 24906.207064\n",
      "iteration 2685, current loss: 24815.984800\n",
      "iteration 2686, current loss: 24585.546942\n",
      "iteration 2687, current loss: 24541.409204\n",
      "iteration 2688, current loss: 24380.107839\n",
      "iteration 2689, current loss: 24432.928329\n",
      "iteration 2690, current loss: 24312.285716\n",
      "iteration 2691, current loss: 24465.215321\n",
      "iteration 2692, current loss: 24414.883220\n",
      "iteration 2693, current loss: 24684.075848\n",
      "iteration 2694, current loss: 24622.520968\n",
      "iteration 2695, current loss: 24854.272822\n",
      "iteration 2696, current loss: 24699.580560\n",
      "iteration 2697, current loss: 24884.113346\n",
      "iteration 2698, current loss: 24698.900480\n",
      "iteration 2699, current loss: 24796.201617\n",
      "iteration 2700, current loss: 24597.022346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2701, current loss: 24605.268349\n",
      "iteration 2702, current loss: 24452.168492\n",
      "iteration 2703, current loss: 24415.242609\n",
      "iteration 2704, current loss: 24307.845956\n",
      "iteration 2705, current loss: 24269.165312\n",
      "iteration 2706, current loss: 24195.295675\n",
      "iteration 2707, current loss: 24164.957042\n",
      "iteration 2708, current loss: 24115.251951\n",
      "iteration 2709, current loss: 24093.436531\n",
      "iteration 2710, current loss: 24060.449826\n",
      "iteration 2711, current loss: 24061.806849\n",
      "iteration 2712, current loss: 24060.573185\n",
      "iteration 2713, current loss: 24075.673149\n",
      "iteration 2714, current loss: 24092.591726\n",
      "iteration 2715, current loss: 24118.527281\n",
      "iteration 2716, current loss: 24155.979154\n",
      "iteration 2717, current loss: 24208.847858\n",
      "iteration 2718, current loss: 24254.133496\n",
      "iteration 2719, current loss: 24315.912508\n",
      "iteration 2720, current loss: 24328.040101\n",
      "iteration 2721, current loss: 24387.299752\n",
      "iteration 2722, current loss: 24348.220646\n",
      "iteration 2723, current loss: 24377.769546\n",
      "iteration 2724, current loss: 24310.161868\n",
      "iteration 2725, current loss: 24284.753916\n",
      "iteration 2726, current loss: 24221.708157\n",
      "iteration 2727, current loss: 24156.540381\n",
      "iteration 2728, current loss: 24120.227520\n",
      "iteration 2729, current loss: 24051.232469\n",
      "iteration 2730, current loss: 24024.755358\n",
      "iteration 2731, current loss: 23978.632054\n",
      "iteration 2732, current loss: 23962.357505\n",
      "iteration 2733, current loss: 23936.053103\n",
      "iteration 2734, current loss: 23935.893453\n",
      "iteration 2735, current loss: 23937.391724\n",
      "iteration 2736, current loss: 23962.556956\n",
      "iteration 2737, current loss: 23992.960800\n",
      "iteration 2738, current loss: 24054.413111\n",
      "iteration 2739, current loss: 24127.599703\n",
      "iteration 2740, current loss: 24250.364336\n",
      "iteration 2741, current loss: 24413.528859\n",
      "iteration 2742, current loss: 24588.578172\n",
      "iteration 2743, current loss: 24865.426381\n",
      "iteration 2744, current loss: 24975.113580\n",
      "iteration 2745, current loss: 25264.121406\n",
      "iteration 2746, current loss: 25231.310706\n",
      "iteration 2747, current loss: 25515.783414\n",
      "iteration 2748, current loss: 25286.397635\n",
      "iteration 2749, current loss: 25524.709167\n",
      "iteration 2750, current loss: 25085.467894\n",
      "iteration 2751, current loss: 25209.737046\n",
      "iteration 2752, current loss: 24753.398005\n",
      "iteration 2753, current loss: 24785.575125\n",
      "iteration 2754, current loss: 24423.249078\n",
      "iteration 2755, current loss: 24461.503689\n",
      "iteration 2756, current loss: 24285.761174\n",
      "iteration 2757, current loss: 24383.137878\n",
      "iteration 2758, current loss: 24318.420185\n",
      "iteration 2759, current loss: 24421.628477\n",
      "iteration 2760, current loss: 24399.333646\n",
      "iteration 2761, current loss: 24425.066135\n",
      "iteration 2762, current loss: 24391.099836\n",
      "iteration 2763, current loss: 24345.845382\n",
      "iteration 2764, current loss: 24282.151440\n",
      "iteration 2765, current loss: 24226.636584\n",
      "iteration 2766, current loss: 24129.238201\n",
      "iteration 2767, current loss: 24057.564508\n",
      "iteration 2768, current loss: 23967.716641\n",
      "iteration 2769, current loss: 23887.182440\n",
      "iteration 2770, current loss: 23845.739622\n",
      "iteration 2771, current loss: 23786.656798\n",
      "iteration 2772, current loss: 23774.327993\n",
      "iteration 2773, current loss: 23732.841025\n",
      "iteration 2774, current loss: 23738.818294\n",
      "iteration 2775, current loss: 23718.610891\n",
      "iteration 2776, current loss: 23745.631286\n",
      "iteration 2777, current loss: 23756.757995\n",
      "iteration 2778, current loss: 23821.120127\n",
      "iteration 2779, current loss: 23866.403477\n",
      "iteration 2780, current loss: 23989.731460\n",
      "iteration 2781, current loss: 24059.345414\n",
      "iteration 2782, current loss: 24274.614188\n",
      "iteration 2783, current loss: 24344.794958\n",
      "iteration 2784, current loss: 24578.081693\n",
      "iteration 2785, current loss: 24551.267579\n",
      "iteration 2786, current loss: 24750.165138\n",
      "iteration 2787, current loss: 24698.970672\n",
      "iteration 2788, current loss: 24786.653629\n",
      "iteration 2789, current loss: 24652.288083\n",
      "iteration 2790, current loss: 24658.908983\n",
      "iteration 2791, current loss: 24520.984443\n",
      "iteration 2792, current loss: 24518.341399\n",
      "iteration 2793, current loss: 24360.032814\n",
      "iteration 2794, current loss: 24328.184978\n",
      "iteration 2795, current loss: 24244.161550\n",
      "iteration 2796, current loss: 24232.997698\n",
      "iteration 2797, current loss: 24219.963955\n",
      "iteration 2798, current loss: 24196.852531\n",
      "iteration 2799, current loss: 24213.515298\n",
      "iteration 2800, current loss: 24173.466730\n",
      "iteration 2801, current loss: 24235.958252\n",
      "iteration 2802, current loss: 24191.177117\n",
      "iteration 2803, current loss: 24278.814469\n",
      "iteration 2804, current loss: 24239.323281\n",
      "iteration 2805, current loss: 24343.055035\n",
      "iteration 2806, current loss: 24299.652483\n",
      "iteration 2807, current loss: 24408.389946\n",
      "iteration 2808, current loss: 24365.891009\n",
      "iteration 2809, current loss: 24485.514621\n",
      "iteration 2810, current loss: 24422.626557\n",
      "iteration 2811, current loss: 24532.756648\n",
      "iteration 2812, current loss: 24449.583046\n",
      "iteration 2813, current loss: 24543.128554\n",
      "iteration 2814, current loss: 24459.843959\n",
      "iteration 2815, current loss: 24553.253326\n",
      "iteration 2816, current loss: 24408.528213\n",
      "iteration 2817, current loss: 24447.344786\n",
      "iteration 2818, current loss: 24320.714327\n",
      "iteration 2819, current loss: 24340.449414\n",
      "iteration 2820, current loss: 24213.352755\n",
      "iteration 2821, current loss: 24239.706063\n",
      "iteration 2822, current loss: 24113.524179\n",
      "iteration 2823, current loss: 24170.770601\n",
      "iteration 2824, current loss: 24037.078908\n",
      "iteration 2825, current loss: 24106.463837\n",
      "iteration 2826, current loss: 23974.875963\n",
      "iteration 2827, current loss: 24096.929930\n",
      "iteration 2828, current loss: 23951.034271\n",
      "iteration 2829, current loss: 24095.532614\n",
      "iteration 2830, current loss: 23996.077599\n",
      "iteration 2831, current loss: 24117.266335\n",
      "iteration 2832, current loss: 24043.320599\n",
      "iteration 2833, current loss: 24106.336906\n",
      "iteration 2834, current loss: 24025.072875\n",
      "iteration 2835, current loss: 24021.146613\n",
      "iteration 2836, current loss: 23899.133982\n",
      "iteration 2837, current loss: 23880.044493\n",
      "iteration 2838, current loss: 23739.395585\n",
      "iteration 2839, current loss: 23722.704182\n",
      "iteration 2840, current loss: 23613.006902\n",
      "iteration 2841, current loss: 23603.397933\n",
      "iteration 2842, current loss: 23547.233378\n",
      "iteration 2843, current loss: 23548.618661\n",
      "iteration 2844, current loss: 23525.106939\n",
      "iteration 2845, current loss: 23538.884518\n",
      "iteration 2846, current loss: 23542.183651\n",
      "iteration 2847, current loss: 23582.846217\n",
      "iteration 2848, current loss: 23619.165362\n",
      "iteration 2849, current loss: 23696.090910\n",
      "iteration 2850, current loss: 23742.731739\n",
      "iteration 2851, current loss: 23835.056873\n",
      "iteration 2852, current loss: 23899.773409\n",
      "iteration 2853, current loss: 24014.176990\n",
      "iteration 2854, current loss: 24111.170876\n",
      "iteration 2855, current loss: 24262.399792\n",
      "iteration 2856, current loss: 24368.170220\n",
      "iteration 2857, current loss: 24527.494937\n",
      "iteration 2858, current loss: 24611.586916\n",
      "iteration 2859, current loss: 24745.063645\n",
      "iteration 2860, current loss: 24749.375010\n",
      "iteration 2861, current loss: 24834.520221\n",
      "iteration 2862, current loss: 24735.988078\n",
      "iteration 2863, current loss: 24766.086845\n",
      "iteration 2864, current loss: 24605.206784\n",
      "iteration 2865, current loss: 24628.827451\n",
      "iteration 2866, current loss: 24450.253969\n",
      "iteration 2867, current loss: 24452.019658\n",
      "iteration 2868, current loss: 24257.620363\n",
      "iteration 2869, current loss: 24242.832331\n",
      "iteration 2870, current loss: 24080.708584\n",
      "iteration 2871, current loss: 24072.424387\n",
      "iteration 2872, current loss: 23977.805219\n",
      "iteration 2873, current loss: 23989.387625\n",
      "iteration 2874, current loss: 23958.954962\n",
      "iteration 2875, current loss: 23997.123579\n",
      "iteration 2876, current loss: 24008.499565\n",
      "iteration 2877, current loss: 24095.149664\n",
      "iteration 2878, current loss: 24081.564462\n",
      "iteration 2879, current loss: 24179.281536\n",
      "iteration 2880, current loss: 24116.727084\n",
      "iteration 2881, current loss: 24194.973476\n",
      "iteration 2882, current loss: 24109.464883\n",
      "iteration 2883, current loss: 24168.161462\n",
      "iteration 2884, current loss: 24081.017866\n",
      "iteration 2885, current loss: 24133.787784\n",
      "iteration 2886, current loss: 24013.494902\n",
      "iteration 2887, current loss: 24057.812563\n",
      "iteration 2888, current loss: 23908.624585\n",
      "iteration 2889, current loss: 23972.538235\n",
      "iteration 2890, current loss: 23825.090851\n",
      "iteration 2891, current loss: 23922.983062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2892, current loss: 23807.463862\n",
      "iteration 2893, current loss: 23939.478194\n",
      "iteration 2894, current loss: 23828.026240\n",
      "iteration 2895, current loss: 23981.388993\n",
      "iteration 2896, current loss: 23860.529649\n",
      "iteration 2897, current loss: 24024.162460\n",
      "iteration 2898, current loss: 23905.603612\n",
      "iteration 2899, current loss: 24045.074724\n",
      "iteration 2900, current loss: 23878.465984\n",
      "iteration 2901, current loss: 23936.687743\n",
      "iteration 2902, current loss: 23756.340870\n",
      "iteration 2903, current loss: 23789.682140\n",
      "iteration 2904, current loss: 23619.428841\n",
      "iteration 2905, current loss: 23649.177274\n",
      "iteration 2906, current loss: 23499.646420\n",
      "iteration 2907, current loss: 23503.100726\n",
      "iteration 2908, current loss: 23441.417364\n",
      "iteration 2909, current loss: 23456.599601\n",
      "iteration 2910, current loss: 23444.125216\n",
      "iteration 2911, current loss: 23475.877058\n",
      "iteration 2912, current loss: 23498.462703\n",
      "iteration 2913, current loss: 23545.932773\n",
      "iteration 2914, current loss: 23596.203295\n",
      "iteration 2915, current loss: 23664.259344\n",
      "iteration 2916, current loss: 23736.595008\n",
      "iteration 2917, current loss: 23816.076716\n",
      "iteration 2918, current loss: 23891.993857\n",
      "iteration 2919, current loss: 23963.864660\n",
      "iteration 2920, current loss: 24045.312632\n",
      "iteration 2921, current loss: 24106.327578\n",
      "iteration 2922, current loss: 24172.441144\n",
      "iteration 2923, current loss: 24193.524039\n",
      "iteration 2924, current loss: 24207.057750\n",
      "iteration 2925, current loss: 24225.560963\n",
      "iteration 2926, current loss: 24201.914202\n",
      "iteration 2927, current loss: 24247.989772\n",
      "iteration 2928, current loss: 24182.930075\n",
      "iteration 2929, current loss: 24280.472607\n",
      "iteration 2930, current loss: 24179.874647\n",
      "iteration 2931, current loss: 24351.326056\n",
      "iteration 2932, current loss: 24178.033512\n",
      "iteration 2933, current loss: 24332.847460\n",
      "iteration 2934, current loss: 24140.435278\n",
      "iteration 2935, current loss: 24235.595958\n",
      "iteration 2936, current loss: 24081.044306\n",
      "iteration 2937, current loss: 24106.329903\n",
      "iteration 2938, current loss: 23960.776274\n",
      "iteration 2939, current loss: 23937.098180\n",
      "iteration 2940, current loss: 23798.455285\n",
      "iteration 2941, current loss: 23731.676093\n",
      "iteration 2942, current loss: 23652.052785\n",
      "iteration 2943, current loss: 23604.033598\n",
      "iteration 2944, current loss: 23562.885128\n",
      "iteration 2945, current loss: 23552.885996\n",
      "iteration 2946, current loss: 23531.224731\n",
      "iteration 2947, current loss: 23539.870886\n",
      "iteration 2948, current loss: 23531.059285\n",
      "iteration 2949, current loss: 23558.530775\n",
      "iteration 2950, current loss: 23572.658890\n",
      "iteration 2951, current loss: 23591.104045\n",
      "iteration 2952, current loss: 23629.478810\n",
      "iteration 2953, current loss: 23588.163802\n",
      "iteration 2954, current loss: 23639.711155\n",
      "iteration 2955, current loss: 23548.289105\n",
      "iteration 2956, current loss: 23627.517668\n",
      "iteration 2957, current loss: 23518.554167\n",
      "iteration 2958, current loss: 23617.905844\n",
      "iteration 2959, current loss: 23496.718481\n",
      "iteration 2960, current loss: 23605.158317\n",
      "iteration 2961, current loss: 23473.581153\n",
      "iteration 2962, current loss: 23581.544055\n",
      "iteration 2963, current loss: 23447.808556\n",
      "iteration 2964, current loss: 23552.567085\n",
      "iteration 2965, current loss: 23430.220127\n",
      "iteration 2966, current loss: 23551.956953\n",
      "iteration 2967, current loss: 23503.711900\n",
      "iteration 2968, current loss: 23692.157696\n",
      "iteration 2969, current loss: 23701.534553\n",
      "iteration 2970, current loss: 23889.803564\n",
      "iteration 2971, current loss: 23881.105251\n",
      "iteration 2972, current loss: 24026.112293\n",
      "iteration 2973, current loss: 23968.984696\n",
      "iteration 2974, current loss: 24047.277128\n",
      "iteration 2975, current loss: 23958.303543\n",
      "iteration 2976, current loss: 23965.490766\n",
      "iteration 2977, current loss: 23802.963820\n",
      "iteration 2978, current loss: 23758.558977\n",
      "iteration 2979, current loss: 23608.040349\n",
      "iteration 2980, current loss: 23585.450384\n",
      "iteration 2981, current loss: 23475.855497\n",
      "iteration 2982, current loss: 23469.053074\n",
      "iteration 2983, current loss: 23407.156795\n",
      "iteration 2984, current loss: 23413.733720\n",
      "iteration 2985, current loss: 23396.181836\n",
      "iteration 2986, current loss: 23408.656145\n",
      "iteration 2987, current loss: 23452.131155\n",
      "iteration 2988, current loss: 23475.028087\n",
      "iteration 2989, current loss: 23565.281135\n",
      "iteration 2990, current loss: 23552.645422\n",
      "iteration 2991, current loss: 23615.634558\n",
      "iteration 2992, current loss: 23596.425960\n",
      "iteration 2993, current loss: 23673.535039\n",
      "iteration 2994, current loss: 23704.280169\n",
      "iteration 2995, current loss: 23763.137213\n",
      "iteration 2996, current loss: 23750.199427\n",
      "iteration 2997, current loss: 23810.078475\n",
      "iteration 2998, current loss: 23727.822740\n",
      "iteration 2999, current loss: 23780.349864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e60fa072b0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(3000, model)\n",
    "costs = model['costs']\n",
    "plt.plot(costs[-3000:])\n",
    "#print(W[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22ace2235c0>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs[-3000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3, Inspect Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pred, real):\n",
    "    right = np.sum(pred==real)\n",
    "    acc = right/real.shape[1]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy: 0.895159\n",
      "test set accuracy: 0.822143\n"
     ]
    }
   ],
   "source": [
    "predict = model['predict']\n",
    "pred = predict(model, train_X)\n",
    "pred = Y_to_labels(pred)\n",
    "acc = get_accuracy(pred, train_labels)\n",
    "print(\"training set accuracy: %f\" % acc)\n",
    "pred = predict(model, test_X)\n",
    "pred = Y_to_labels(pred)\n",
    "acc = get_accuracy(pred, test_labels)\n",
    "print(\"test set accuracy: %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"W-four-layer\", *W)\n",
    "np.savez(\"b-four-layer\", *b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 28000)\n"
     ]
    }
   ],
   "source": [
    "test_X = np.genfromtxt(\"test.csv\", delimiter=',', skip_header=1)\n",
    "test_X = test_X.T\n",
    "test_X = test_X/255\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28000)\n"
     ]
    }
   ],
   "source": [
    "test_Y = predict(test_X)\n",
    "test_labels = Y_to_labels(test_Y)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.zeros((test_labels.shape[1], 2), dtype=int)\n",
    "for i in range(output.shape[0]):\n",
    "    output[i, 0] = i+1\n",
    "    output[i, 1] = test_labels[0, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"4layer-submission.csv\", output, fmt=\"%d\", delimiter=',', header='ImageId,Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17346840320>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_images = X_to_images(test_X)\n",
    "plt.imshow(test_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
