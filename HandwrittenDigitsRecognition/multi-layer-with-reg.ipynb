{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digits Recognition Using Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data comes from the \"MNIST\" data set, you can download it from [Kaggle](https://www.kaggle.com/c/digit-recognizer/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1, Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"train.csv\", delimiter=',', skip_header=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "[7. 9. 4. 7. 1. 2. 9. 4. 9. 7.]\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "print(data.shape)\n",
    "print(data[:10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 42000)\n",
      "(1, 42000)\n"
     ]
    }
   ],
   "source": [
    "features = data[:, 1:].T\n",
    "labels = data[:, 0]\n",
    "labels = np.reshape(labels, (1, -1))\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = features.shape[1]\n",
    "nx = features.shape[0]\n",
    "ny = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_Y(labels):\n",
    "    Y = np.zeros((ny, m))\n",
    "    for i in range(m):\n",
    "        Y[int(labels[0, i]), i] = 1\n",
    "    return Y\n",
    "\n",
    "def Y_to_labels(Y):\n",
    "    labels = np.argmax(Y, axis=0).astype(float)\n",
    "    labels = np.reshape(labels, (1, -1))\n",
    "    return labels\n",
    "\n",
    "def X_to_images(X):\n",
    "    images = [np.reshape(X[:, i], (28, 28)) for i in range(X.shape[1])]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 42000)\n",
      "7.0\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAADS1JREFUeJzt3XuMXOV5x/HvA1nsYC61FS6WAzG3RiDUOu3WJHVU0SJSaKNAokJDJeoqKI7UIIHEH0VupaBWlWibS1HaRDLFjamAJOISLBVRkFuJ0gQLQyiGUMIlLji2bJBJDBSML0//8DhazM7Z9dzOrp/vR7Jm5jznzHl87N+emX3PzBuZiaR6jmi7AUntMPxSUYZfKsrwS0UZfqkowy8VZfilogy/VJThl4p63yh3dlTMybnMG+UupVLe5k3eyV0xnXX7Cn9EXATcBBwJ/FNm3ti0/lzmcV5c0M8uJTVYn+umvW7PL/sj4kjgH4GLgXOAKyLinF6fT9Jo9fOefynwfGa+mJnvAN8GLhlMW5KGrZ/wLwJenvB4c2fZu0TEiojYEBEbdrOrj91JGqR+wj/ZLxXe8/ngzFyVmeOZOT7GnD52J2mQ+gn/ZuCUCY8/CGzprx1Jo9JP+B8FzoqI0yLiKOCzwNrBtCVp2Hoe6svMPRFxNfBv7B/qW52ZTw+sM0lD1dc4f2beB9w3oF4kjZCX90pFGX6pKMMvFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilogy/VJThl4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1RUX7P0RsQm4HVgL7AnM8cH0ZSk4esr/B2/nZmvDuB5JI2QL/ulovoNfwIPRMRjEbFiEA1JGo1+X/Yvy8wtEXEi8GBE/E9mPjRxhc4PhRUAczm6z91JGpS+zvyZuaVzux24B1g6yTqrMnM8M8fHmNPP7iQNUM/hj4h5EXHsgfvAJ4CnBtWYpOHq52X/ScA9EXHgeW7PzPsH0pWkoes5/Jn5IvCrA+xF0gg51CcVZfilogy/VJThl4oy/FJRhl8qahCf6huZn993Ztfa5077fuO29/7+ey4+fJc9L27qpSVp1vLMLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFzapx/keW3Nm1tjv3Nm77S/e/2Vj/87v/qLF+5l892bW2783m59ZwvPB3H2usn/DD7Fo77vZHBt3OrOOZXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeKiszuY6GDdlwsyPPigp63f+4fzutaW33xzY3bLpu7u+f9Ary6962utbvfOLtx2y8/fFFj/ahtzZdbnH7Xzxvr+574UWN9tnrf6Ysb65+7/98b6xcf3X3y6KVfv7Zx20V/0/z9EDPV+lzHztwR01nXM79UlOGXijL8UlGGXyrK8EtFGX6pKMMvFTXlOH9ErAY+CWzPzHM7yxYA3wEWA5uAyzPztal21u84f5OpxoSf/dOTG+trPvONxvrSOaO7HuJg+9jXWL/yJ92vI3j6Xz/cuO37X23v73X8H/60sf6Xp3+vsd7Pv8m5/3x1Y33xX/yg5+du06DH+b8FHPy/63pgXWaeBazrPJY0i0wZ/sx8CNhx0OJLgDWd+2uASwfcl6Qh6/U9/0mZuRWgc3vi4FqSNApD/w6/iFgBrACYy9HD3p2kaer1zL8tIhYCdG63d1sxM1dl5nhmjo8xp8fdSRq0XsO/Fljeub8cuHcw7UgalSnDHxF3AD8APhwRmyPiKuBG4MKIeA64sPNY0iwyqz7PP0xHnnBCY/2Fa87sWrvgd3/YuO2n5jfXp/KxuT9rrB8dR/X1/IerldvGu9Y2njfWuG3ufmfQ7YyEn+eXNCXDLxVl+KWiDL9UlOGXijL8UlEO9c0C//eZ7l9ZDrDr2O4/w9/81M7GbT+0YMpPYg/Ns/99amP9+NObe1v/67c31q/Zsqxr7YXfeLtx29nKoT5JUzL8UlGGXyrK8EtFGX6pKMMvFWX4paKG/jVe6t/Rd69vrjfU5q9pKAJ7D72dgTmTLY31l+88d0Sd1OSZXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeKcpxfM9bx895qu4XDmmd+qSjDLxVl+KWiDL9UlOGXijL8UlGGXypqynH+iFgNfBLYnpnndpbdAHweeKWz2srMvG9YTUq9eODHZ3etnUF/06YfDqZz5v8WcNEky7+WmUs6fwy+NMtMGf7MfAjYMYJeJI1QP+/5r46IJyNidUTMH1hHkkai1/B/EzgDWAJsBb7SbcWIWBERGyJiw2529bg7SYPWU/gzc1tm7s3MfcDNwNKGdVdl5nhmjo8xp9c+JQ1YT+GPiIUTHn4aeGow7UgalekM9d0BnA98ICI2A18Czo+IJUACm4AvDLFHSUMwZfgz84pJFt8yhF6kgTpm/fvbbmFG8wo/qSjDLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilogy/VJThl4oy/FJRU351tzQsR8yb11j/zRN/0lh/bd/bjfVFazd3re1p3LIGz/xSUYZfKsrwS0UZfqkowy8VZfilogy/VNSU4/wRcQpwK3AysA9YlZk3RcQC4DvAYmATcHlmvja8VnW4OWLB/Mb6jSc/1Fjfujcb63s2vXTIPVUynTP/HuC6zDwb+CjwxYg4B7geWJeZZwHrOo8lzRJThj8zt2bm4537rwPPAIuAS4A1ndXWAJcOq0lJg3dI7/kjYjHwEWA9cFJmboX9PyCAEwfdnKThmXb4I+IY4C7g2szceQjbrYiIDRGxYTe7eulR0hBMK/wRMcb+4N+WmXd3Fm+LiIWd+kJg+2TbZuaqzBzPzPEx5gyiZ0kDMGX4IyKAW4BnMvOrE0prgeWd+8uBewffnqRhmc5HepcBVwIbI+KJzrKVwI3AdyPiKuAl4LLhtKjD1eY/OLWv7e/c+SsD6qSmKcOfmQ8D0aV8wWDbkTQqXuEnFWX4paIMv1SU4ZeKMvxSUYZfKsqv7lZrxl5v/kjuVL7+X80jzb/Mo309/+HOM79UlOGXijL8UlGGXyrK8EtFGX6pKMMvFeU4v1rzs995q+0WSvPMLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFOc6v1nxj6W1tt1CaZ36pKMMvFWX4paIMv1SU4ZeKMvxSUYZfKmrKcf6IOAW4FTgZ2AesysybIuIG4PPAK51VV2bmfcNqVIef6zZe1lh/fOm/NNb/+KPfb6w/wtgh91TJdC7y2QNcl5mPR8SxwGMR8WCn9rXM/PLw2pM0LFOGPzO3Als791+PiGeARcNuTNJwHdJ7/ohYDHwEWN9ZdHVEPBkRqyNifpdtVkTEhojYsJtdfTUraXCmHf6IOAa4C7g2M3cC3wTOAJaw/5XBVybbLjNXZeZ4Zo6PMWcALUsahGmFPyLG2B/82zLzboDM3JaZezNzH3AzsHR4bUoatCnDHxEB3AI8k5lfnbB84YTVPg08Nfj2JA3LdH7bvwy4EtgYEU90lq0EroiIJUACm4AvDKVDHbbeeuG45hWmeC151fz1jfVH+PghdlTLdH7b/zAQk5Qc05dmMa/wk4oy/FJRhl8qyvBLRRl+qSjDLxUVmTmynR0XC/K8uGBk+5OqWZ/r2Jk7Jhuafw/P/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9U1EjH+SPiFeB/Jyz6APDqyBo4NDO1t5naF9hbrwbZ24cy84TprDjS8L9n5xEbMnO8tQYazNTeZmpfYG+9aqs3X/ZLRRl+qai2w7+q5f03mam9zdS+wN561Upvrb7nl9Sets/8klrSSvgj4qKIeDYino+I69vooZuI2BQRGyPiiYjY0HIvqyNie0Q8NWHZgoh4MCKe69xOOk1aS73dEBE/7Ry7JyLi91rq7ZSI+I+IeCYino6IazrLWz12DX21ctxG/rI/Io4EfgxcCGwGHgWuyMwfjbSRLiJiEzCema2PCUfEbwFvALdm5rmdZX8L7MjMGzs/OOdn5p/NkN5uAN5oe+bmzoQyCyfOLA1cCvwJLR67hr4up4Xj1saZfynwfGa+mJnvAN8GLmmhjxkvMx8Cdhy0+BJgTef+Gvb/5xm5Lr3NCJm5NTMf79x/HTgws3Srx66hr1a0Ef5FwMsTHm9mZk35ncADEfFYRKxou5lJnNSZNv3A9OknttzPwaacuXmUDppZesYcu15mvB60NsI/2VcMzaQhh2WZ+WvAxcAXOy9vNT3Tmrl5VCaZWXpG6HXG60FrI/ybgVMmPP4gsKWFPiaVmVs6t9uBe5h5sw9vOzBJaud2e8v9/MJMmrl5spmlmQHHbibNeN1G+B8FzoqI0yLiKOCzwNoW+niPiJjX+UUMETEP+AQzb/bhtcDyzv3lwL0t9vIuM2Xm5m4zS9PysZtpM163cpFPZyjj74EjgdWZ+dcjb2ISEXE6+8/2sH8S09vb7C0i7gDOZ/+nvrYBXwK+B3wXOBV4CbgsM0f+i7cuvZ3P/peuv5i5+cB77BH39nHgP4GNwL7O4pXsf3/d2rFr6OsKWjhuXuEnFeUVflJRhl8qyvBLRRl+qSjDLxVl+KWiDL9UlOGXivp/2uHEmvuZ2R0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = features / 255\n",
    "Y = labels_to_Y(labels)\n",
    "images = X_to_images(X)\n",
    "print(Y.shape)\n",
    "plt.imshow(images[0])\n",
    "print(labels[0, 0])\n",
    "print(Y[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 37800)\n",
      "(10, 37800)\n",
      "(784, 4200)\n",
      "(10, 4200)\n",
      "(1, 37800)\n"
     ]
    }
   ],
   "source": [
    "test_ratio = 1 - train_ratio\n",
    "train_m = int(m * train_ratio)\n",
    "test_m = m - train_m\n",
    "train_X = X[:, :train_m]\n",
    "test_X = X[:, train_m:]\n",
    "train_Y = Y[:, :train_m]\n",
    "test_Y = Y[:, train_m:]\n",
    "train_labels = Y_to_labels(train_Y)\n",
    "test_labels = Y_to_labels(test_Y)\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2, Design Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    a = 1/ (1 + np.exp(-z))\n",
    "    return a\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    p1 = np.exp(-z)\n",
    "    a = p1/((1+p1)**2)\n",
    "    return a\n",
    "\n",
    "def relu(z):\n",
    "    a = np.maximum(z, 0.01*z)\n",
    "    return a\n",
    "\n",
    "def relu_prime(z):\n",
    "    a = np.where(z > 0, 1, 0.01)\n",
    "    return a\n",
    "\n",
    "def tanh(z):\n",
    "    p1 = np.exp(z)\n",
    "    p2 = np.exp(-z)\n",
    "    a = (p1-p2)/(p1+p2)\n",
    "    return a\n",
    "    \n",
    "def tanh_prime(z):\n",
    "    p1 = tanh(z)\n",
    "    a = 1-p1**2\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_for__config_multi_layer_classifier_without_regularization(Y_hat, Y):\n",
    "    delta = 1e-10\n",
    "    l = -((Y+delta)*np.log(Y_hat+delta) + (1-Y+delta)*np.log(1-Y_hat+delta))\n",
    "    return l\n",
    "\n",
    "def forward_propagation_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    X = model['X']\n",
    "    Y = model['Y']\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f = model['f']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    costs = model['costs']\n",
    "    loss_function = model['loss_function']\n",
    "    A[0] = X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    costs.append(np.sum(loss_function(A[L], Y)))\n",
    "\n",
    "def back_propagation_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f_prime = model['f_prime']\n",
    "    m = model['m']\n",
    "    W = model['W']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    Y = model['Y']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    dA = model['dA']\n",
    "    dZ = model['dZ']\n",
    "    dZ[L] = A[L] - Y\n",
    "    dW[L] = np.matmul(dZ[L], A[L-1].T)/m\n",
    "    db[L] = np.sum(dZ[L], axis=1, keepdims=True)/m\n",
    "    for i in reversed(range(1, L)):\n",
    "        dZ[i] = np.matmul(W[i+1].T, dZ[i+1]) * f_prime[i](Z[i])\n",
    "        dW[i] = np.matmul(dZ[i], A[i-1].T)/m\n",
    "        db[i] = np.sum(dZ[i], axis=1, keepdims=True)/m\n",
    "        \n",
    "def update_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    L = model['L']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    learning_rate = model['learning_rate']\n",
    "    for i in range(1, L+1):\n",
    "        W[i] -= learning_rate*dW[i]\n",
    "        b[i] -= learning_rate*db[i]\n",
    "    \n",
    "    \n",
    "def predict_for_config_multi_layer_classifier_without_regularization(model, test_X):\n",
    "    L = model['L']\n",
    "    A = model['A']\n",
    "    Z = model['Z']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    f = model['f']\n",
    "    A[0] = test_X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    return A[L]\n",
    "    \n",
    "    \n",
    "def config_multi_layer_classifier_without_regularization(X, Y, neuron_of_hidden_layer, learning_rate):\n",
    "    n_input_layer = X.shape[0]\n",
    "    n_output_layer = Y.shape[0]\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = [n_input_layer] + neuron_of_hidden_layer + [n_output_layer]\n",
    "    L = len(n)-1\n",
    "    f = [tanh if i != 0 else None for i in range(L)]\n",
    "    f.append(sigmoid)\n",
    "    f_prime = [tanh_prime if i != 0 else None for i in range(L)]\n",
    "    f_prime.append(sigmoid_prime)\n",
    "    W = [np.random.normal(0, 1, (n[i], n[i-1])) if i != 0 else None for i in range(L+1)]\n",
    "    b = [np.random.normal(0, 1, (n[i], 1)) if i != 0 else None for i in range(L+1)]\n",
    "    Z = [None for i in range(L+1)]\n",
    "    A = [None for i in range(L+1)]\n",
    "    dW = [None for i in range(L+1)]\n",
    "    db = [None for i in range(L+1)]\n",
    "    dA = [None for i in range(L+1)]\n",
    "    dZ = [None for i in range(L+1)]\n",
    "    costs = []\n",
    "    \n",
    "    model = dict()\n",
    "    model['X'] = X\n",
    "    model['Y'] = Y\n",
    "    model['m'] = m\n",
    "    model['n'] = n\n",
    "    model['L'] = L\n",
    "    model['f'] = f\n",
    "    model['f_prime'] = f_prime\n",
    "    model['W'] = W\n",
    "    model['b'] = b\n",
    "    model['Z'] = Z\n",
    "    model['A'] = A\n",
    "    model['dW'] = dW\n",
    "    model['db'] = db\n",
    "    model['dA'] = dA\n",
    "    model['dZ'] = dZ\n",
    "    model['loss_function'] = loss_function_for__config_multi_layer_classifier_without_regularization\n",
    "    model['costs'] = costs\n",
    "    model['learning_rate'] = learning_rate\n",
    "    model['forwardprop'] = forward_propagation_for_config_multi_layer_classifier_without_regularization\n",
    "    model['backprop'] = back_propagation_for_config_multi_layer_classifier_without_regularization\n",
    "    model['update'] = update_for_config_multi_layer_classifier_without_regularization\n",
    "    model['predict'] = predict_for_config_multi_layer_classifier_without_regularization\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_for__config_multi_layer_classifier_with_regularization(model, Y_hat):\n",
    "    n = model['n']\n",
    "    Y = model['Y']\n",
    "    W = model['W']\n",
    "    L = model['L']\n",
    "    m = model['m']\n",
    "    penalty = model['penalty']\n",
    "    delta = 1e-10\n",
    "    l = -((Y+delta)*np.log(Y_hat+delta) + (1-Y+delta)*np.log(1-Y_hat+delta))\n",
    "    p = 0\n",
    "    for i in range(1, L+1):\n",
    "        p = p + np.sum(W[i]**2)*penalty/(2*m)\n",
    "    l = l + p/n[L]\n",
    "    return l\n",
    "\n",
    "def forward_propagation_for_config_multi_layer_classifier_with_regularization(model):\n",
    "    X = model['X']\n",
    "    Y = model['Y']\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f = model['f']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    costs = model['costs']\n",
    "    loss_function = model['loss_function']\n",
    "    A[0] = X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    costs.append(np.sum(loss_function(model, A[L])))\n",
    "\n",
    "def back_propagation_for_config_multi_layer_classifier_with_regularization(model):\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f_prime = model['f_prime']\n",
    "    m = model['m']\n",
    "    W = model['W']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    Y = model['Y']\n",
    "    penalty = model['penalty']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    dA = model['dA']\n",
    "    dZ = model['dZ']\n",
    "    dZ[L] = A[L] - Y\n",
    "    dW[L] = np.matmul(dZ[L], A[L-1].T)/m\n",
    "    db[L] = np.sum(dZ[L], axis=1, keepdims=True)/m\n",
    "    for i in reversed(range(1, L)):\n",
    "        dZ[i] = np.matmul(W[i+1].T, dZ[i+1]) * f_prime[i](Z[i])\n",
    "        dW[i] = np.matmul(dZ[i], A[i-1].T)/m + W[i]*penalty/m\n",
    "        db[i] = np.sum(dZ[i], axis=1, keepdims=True)/m\n",
    "        \n",
    "def update_for_config_multi_layer_classifier_with_regularization(model):\n",
    "    L = model['L']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    learning_rate = model['learning_rate']\n",
    "    for i in range(1, L+1):\n",
    "        W[i] -= learning_rate*dW[i]\n",
    "        b[i] -= learning_rate*db[i]\n",
    "    \n",
    "    \n",
    "def predict_for_config_multi_layer_classifier_with_regularization(model, test_X):\n",
    "    L = model['L']\n",
    "    A = model['A']\n",
    "    Z = model['Z']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    f = model['f']\n",
    "    A[0] = test_X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    return A[L]\n",
    "    \n",
    "    \n",
    "def config_multi_layer_classifier_with_regularization(X, Y, neuron_of_hidden_layer, learning_rate, penalty):\n",
    "    n_input_layer = X.shape[0]\n",
    "    n_output_layer = Y.shape[0]\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = [n_input_layer] + neuron_of_hidden_layer + [n_output_layer]\n",
    "    L = len(n)-1\n",
    "    f = [tanh if i != 0 else None for i in range(L)]\n",
    "    f.append(sigmoid)\n",
    "    f_prime = [tanh_prime if i != 0 else None for i in range(L)]\n",
    "    f_prime.append(sigmoid_prime)\n",
    "    W = [np.random.normal(0, 1, (n[i], n[i-1])) if i != 0 else None for i in range(L+1)]\n",
    "    b = [np.random.normal(0, 1, (n[i], 1)) if i != 0 else None for i in range(L+1)]\n",
    "    Z = [None for i in range(L+1)]\n",
    "    A = [None for i in range(L+1)]\n",
    "    dW = [None for i in range(L+1)]\n",
    "    db = [None for i in range(L+1)]\n",
    "    dA = [None for i in range(L+1)]\n",
    "    dZ = [None for i in range(L+1)]\n",
    "    costs = []\n",
    "    \n",
    "    model = dict()\n",
    "    model['X'] = X\n",
    "    model['Y'] = Y\n",
    "    model['m'] = m\n",
    "    model['n'] = n\n",
    "    model['L'] = L\n",
    "    model['f'] = f\n",
    "    model['f_prime'] = f_prime\n",
    "    model['W'] = W\n",
    "    model['b'] = b\n",
    "    model['Z'] = Z\n",
    "    model['A'] = A\n",
    "    model['dW'] = dW\n",
    "    model['db'] = db\n",
    "    model['dA'] = dA\n",
    "    model['dZ'] = dZ\n",
    "    model['loss_function'] = loss_function_for__config_multi_layer_classifier_with_regularization\n",
    "    model['costs'] = costs\n",
    "    model['learning_rate'] = learning_rate\n",
    "    model['penalty'] = penalty\n",
    "    model['forwardprop'] = forward_propagation_for_config_multi_layer_classifier_with_regularization\n",
    "    model['backprop'] = back_propagation_for_config_multi_layer_classifier_with_regularization\n",
    "    model['update'] = update_for_config_multi_layer_classifier_with_regularization\n",
    "    model['predict'] = predict_for_config_multi_layer_classifier_with_regularization\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(iteration_times, model):\n",
    "    forwardprop = model['forwardprop']\n",
    "    backprop = model['backprop']\n",
    "    update = model['update']\n",
    "    costs = model['costs']\n",
    "    for i in range(iteration_times):\n",
    "        forwardprop(model)\n",
    "        backprop(model)\n",
    "        update(model)\n",
    "        print(\"iteration %d, current loss: %f\" % (i, costs[len(costs)-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = config_multi_layer_classifier_with_regularization(train_X, train_Y, [128,32], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, current loss: 68945.718264\n",
      "iteration 1, current loss: 68934.390653\n",
      "iteration 2, current loss: 68923.083615\n",
      "iteration 3, current loss: 68911.797592\n",
      "iteration 4, current loss: 68900.533033\n",
      "iteration 5, current loss: 68889.290350\n",
      "iteration 6, current loss: 68878.069885\n",
      "iteration 7, current loss: 68866.871870\n",
      "iteration 8, current loss: 68855.696399\n",
      "iteration 9, current loss: 68844.543415\n",
      "iteration 10, current loss: 68833.412708\n",
      "iteration 11, current loss: 68822.303921\n",
      "iteration 12, current loss: 68811.216569\n",
      "iteration 13, current loss: 68800.150063\n",
      "iteration 14, current loss: 68789.103730\n",
      "iteration 15, current loss: 68778.076840\n",
      "iteration 16, current loss: 68767.068626\n",
      "iteration 17, current loss: 68756.078305\n",
      "iteration 18, current loss: 68745.105089\n",
      "iteration 19, current loss: 68734.148206\n",
      "iteration 20, current loss: 68723.206901\n",
      "iteration 21, current loss: 68712.280454\n",
      "iteration 22, current loss: 68701.368179\n",
      "iteration 23, current loss: 68690.469435\n",
      "iteration 24, current loss: 68679.583627\n",
      "iteration 25, current loss: 68668.710212\n",
      "iteration 26, current loss: 68657.848698\n",
      "iteration 27, current loss: 68646.998649\n",
      "iteration 28, current loss: 68636.159678\n",
      "iteration 29, current loss: 68625.331452\n",
      "iteration 30, current loss: 68614.513683\n",
      "iteration 31, current loss: 68603.706122\n",
      "iteration 32, current loss: 68592.908556\n",
      "iteration 33, current loss: 68582.120790\n",
      "iteration 34, current loss: 68571.342647\n",
      "iteration 35, current loss: 68560.573944\n",
      "iteration 36, current loss: 68549.814491\n",
      "iteration 37, current loss: 68539.064068\n",
      "iteration 38, current loss: 68528.322426\n",
      "iteration 39, current loss: 68517.589269\n",
      "iteration 40, current loss: 68506.864257\n",
      "iteration 41, current loss: 68496.147005\n",
      "iteration 42, current loss: 68485.437099\n",
      "iteration 43, current loss: 68474.734119\n",
      "iteration 44, current loss: 68464.037683\n",
      "iteration 45, current loss: 68453.347512\n",
      "iteration 46, current loss: 68442.663513\n",
      "iteration 47, current loss: 68431.985875\n",
      "iteration 48, current loss: 68421.315176\n",
      "iteration 49, current loss: 68410.652460\n",
      "iteration 50, current loss: 68399.999259\n",
      "iteration 51, current loss: 68389.357540\n",
      "iteration 52, current loss: 68378.729553\n",
      "iteration 53, current loss: 68368.117609\n",
      "iteration 54, current loss: 68357.523841\n",
      "iteration 55, current loss: 68346.949994\n",
      "iteration 56, current loss: 68336.397312\n",
      "iteration 57, current loss: 68325.866513\n",
      "iteration 58, current loss: 68315.357840\n",
      "iteration 59, current loss: 68304.871159\n",
      "iteration 60, current loss: 68294.406064\n",
      "iteration 61, current loss: 68283.961974\n",
      "iteration 62, current loss: 68273.538208\n",
      "iteration 63, current loss: 68263.134040\n",
      "iteration 64, current loss: 68252.748739\n",
      "iteration 65, current loss: 68242.381588\n",
      "iteration 66, current loss: 68232.031904\n",
      "iteration 67, current loss: 68221.699040\n",
      "iteration 68, current loss: 68211.382393\n",
      "iteration 69, current loss: 68201.081400\n",
      "iteration 70, current loss: 68190.795540\n",
      "iteration 71, current loss: 68180.524332\n",
      "iteration 72, current loss: 68170.267331\n",
      "iteration 73, current loss: 68160.024130\n",
      "iteration 74, current loss: 68149.794353\n",
      "iteration 75, current loss: 68139.577656\n",
      "iteration 76, current loss: 68129.373724\n",
      "iteration 77, current loss: 68119.182271\n",
      "iteration 78, current loss: 68109.003037\n",
      "iteration 79, current loss: 68098.835790\n",
      "iteration 80, current loss: 68088.680322\n",
      "iteration 81, current loss: 68078.536447\n",
      "iteration 82, current loss: 68068.404008\n",
      "iteration 83, current loss: 68058.282865\n",
      "iteration 84, current loss: 68048.172907\n",
      "iteration 85, current loss: 68038.074038\n",
      "iteration 86, current loss: 68027.986189\n",
      "iteration 87, current loss: 68017.909309\n",
      "iteration 88, current loss: 68007.843363\n",
      "iteration 89, current loss: 67997.788339\n",
      "iteration 90, current loss: 67987.744238\n",
      "iteration 91, current loss: 67977.711075\n",
      "iteration 92, current loss: 67967.688879\n",
      "iteration 93, current loss: 67957.677690\n",
      "iteration 94, current loss: 67947.677553\n",
      "iteration 95, current loss: 67937.688522\n",
      "iteration 96, current loss: 67927.710652\n",
      "iteration 97, current loss: 67917.743999\n",
      "iteration 98, current loss: 67907.788619\n",
      "iteration 99, current loss: 67897.844561\n",
      "iteration 100, current loss: 67887.911871\n",
      "iteration 101, current loss: 67877.990585\n",
      "iteration 102, current loss: 67868.080729\n",
      "iteration 103, current loss: 67858.182319\n",
      "iteration 104, current loss: 67848.295361\n",
      "iteration 105, current loss: 67838.419846\n",
      "iteration 106, current loss: 67828.555754\n",
      "iteration 107, current loss: 67818.703052\n",
      "iteration 108, current loss: 67808.861696\n",
      "iteration 109, current loss: 67799.031631\n",
      "iteration 110, current loss: 67789.212794\n",
      "iteration 111, current loss: 67779.405111\n",
      "iteration 112, current loss: 67769.608503\n",
      "iteration 113, current loss: 67759.822885\n",
      "iteration 114, current loss: 67750.048171\n",
      "iteration 115, current loss: 67740.284269\n",
      "iteration 116, current loss: 67730.531092\n",
      "iteration 117, current loss: 67720.788553\n",
      "iteration 118, current loss: 67711.056566\n",
      "iteration 119, current loss: 67701.335053\n",
      "iteration 120, current loss: 67691.623939\n",
      "iteration 121, current loss: 67681.923156\n",
      "iteration 122, current loss: 67672.232642\n",
      "iteration 123, current loss: 67662.552339\n",
      "iteration 124, current loss: 67652.882195\n",
      "iteration 125, current loss: 67643.222162\n",
      "iteration 126, current loss: 67633.572191\n",
      "iteration 127, current loss: 67623.932233\n",
      "iteration 128, current loss: 67614.302236\n",
      "iteration 129, current loss: 67604.682142\n",
      "iteration 130, current loss: 67595.071884\n",
      "iteration 131, current loss: 67585.471382\n",
      "iteration 132, current loss: 67575.880544\n",
      "iteration 133, current loss: 67566.299263\n",
      "iteration 134, current loss: 67556.727413\n",
      "iteration 135, current loss: 67547.164851\n",
      "iteration 136, current loss: 67537.611418\n",
      "iteration 137, current loss: 67528.066933\n",
      "iteration 138, current loss: 67518.531200\n",
      "iteration 139, current loss: 67509.004006\n",
      "iteration 140, current loss: 67499.485124\n",
      "iteration 141, current loss: 67489.974315\n",
      "iteration 142, current loss: 67480.471329\n",
      "iteration 143, current loss: 67470.975907\n",
      "iteration 144, current loss: 67461.487789\n",
      "iteration 145, current loss: 67452.006708\n",
      "iteration 146, current loss: 67442.532402\n",
      "iteration 147, current loss: 67433.064611\n",
      "iteration 148, current loss: 67423.603080\n",
      "iteration 149, current loss: 67414.147565\n",
      "iteration 150, current loss: 67404.697832\n",
      "iteration 151, current loss: 67395.253660\n",
      "iteration 152, current loss: 67385.814841\n",
      "iteration 153, current loss: 67376.381186\n",
      "iteration 154, current loss: 67366.952520\n",
      "iteration 155, current loss: 67357.528686\n",
      "iteration 156, current loss: 67348.109545\n",
      "iteration 157, current loss: 67338.694976\n",
      "iteration 158, current loss: 67329.284876\n",
      "iteration 159, current loss: 67319.879162\n",
      "iteration 160, current loss: 67310.477768\n",
      "iteration 161, current loss: 67301.080648\n",
      "iteration 162, current loss: 67291.687775\n",
      "iteration 163, current loss: 67282.299140\n",
      "iteration 164, current loss: 67272.914750\n",
      "iteration 165, current loss: 67263.534632\n",
      "iteration 166, current loss: 67254.158824\n",
      "iteration 167, current loss: 67244.787377\n",
      "iteration 168, current loss: 67235.420353\n",
      "iteration 169, current loss: 67226.057817\n",
      "iteration 170, current loss: 67216.699835\n",
      "iteration 171, current loss: 67207.346469\n",
      "iteration 172, current loss: 67197.997772\n",
      "iteration 173, current loss: 67188.653783\n",
      "iteration 174, current loss: 67179.314524\n",
      "iteration 175, current loss: 67169.979991\n",
      "iteration 176, current loss: 67160.650155\n",
      "iteration 177, current loss: 67151.324954\n",
      "iteration 178, current loss: 67142.004295\n",
      "iteration 179, current loss: 67132.688046\n",
      "iteration 180, current loss: 67123.376038\n",
      "iteration 181, current loss: 67114.068064\n",
      "iteration 182, current loss: 67104.763875\n",
      "iteration 183, current loss: 67095.463181\n",
      "iteration 184, current loss: 67086.165651\n",
      "iteration 185, current loss: 67076.870916\n",
      "iteration 186, current loss: 67067.578564\n",
      "iteration 187, current loss: 67058.288152\n",
      "iteration 188, current loss: 67048.999204\n",
      "iteration 189, current loss: 67039.711219\n",
      "iteration 190, current loss: 67030.423685\n",
      "iteration 191, current loss: 67021.136086\n",
      "iteration 192, current loss: 67011.847930\n",
      "iteration 193, current loss: 67002.558767\n",
      "iteration 194, current loss: 66993.268228\n",
      "iteration 195, current loss: 66983.976055\n",
      "iteration 196, current loss: 66974.682153\n",
      "iteration 197, current loss: 66965.386631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 198, current loss: 66956.089847\n",
      "iteration 199, current loss: 66946.792440\n",
      "iteration 200, current loss: 66937.495341\n",
      "iteration 201, current loss: 66928.199760\n",
      "iteration 202, current loss: 66918.907134\n",
      "iteration 203, current loss: 66909.619051\n",
      "iteration 204, current loss: 66900.337142\n",
      "iteration 205, current loss: 66891.062967\n",
      "iteration 206, current loss: 66881.797913\n",
      "iteration 207, current loss: 66872.543112\n",
      "iteration 208, current loss: 66863.299397\n",
      "iteration 209, current loss: 66854.067293\n",
      "iteration 210, current loss: 66844.847035\n",
      "iteration 211, current loss: 66835.638617\n",
      "iteration 212, current loss: 66826.441838\n",
      "iteration 213, current loss: 66817.256363\n",
      "iteration 214, current loss: 66808.081767\n",
      "iteration 215, current loss: 66798.917579\n",
      "iteration 216, current loss: 66789.763318\n",
      "iteration 217, current loss: 66780.618515\n",
      "iteration 218, current loss: 66771.482730\n",
      "iteration 219, current loss: 66762.355567\n",
      "iteration 220, current loss: 66753.236681\n",
      "iteration 221, current loss: 66744.125782\n",
      "iteration 222, current loss: 66735.022640\n",
      "iteration 223, current loss: 66725.927088\n",
      "iteration 224, current loss: 66716.839020\n",
      "iteration 225, current loss: 66707.758396\n",
      "iteration 226, current loss: 66698.685238\n",
      "iteration 227, current loss: 66689.619629\n",
      "iteration 228, current loss: 66680.561707\n",
      "iteration 229, current loss: 66671.511655\n",
      "iteration 230, current loss: 66662.469690\n",
      "iteration 231, current loss: 66653.436045\n",
      "iteration 232, current loss: 66644.410945\n",
      "iteration 233, current loss: 66635.394594\n",
      "iteration 234, current loss: 66626.387149\n",
      "iteration 235, current loss: 66617.388704\n",
      "iteration 236, current loss: 66608.399282\n",
      "iteration 237, current loss: 66599.418827\n",
      "iteration 238, current loss: 66590.447206\n",
      "iteration 239, current loss: 66581.484212\n",
      "iteration 240, current loss: 66572.529574\n",
      "iteration 241, current loss: 66563.582970\n",
      "iteration 242, current loss: 66554.644037\n",
      "iteration 243, current loss: 66545.712385\n",
      "iteration 244, current loss: 66536.787612\n",
      "iteration 245, current loss: 66527.869313\n",
      "iteration 246, current loss: 66518.957092\n",
      "iteration 247, current loss: 66510.050576\n",
      "iteration 248, current loss: 66501.149424\n",
      "iteration 249, current loss: 66492.253337\n",
      "iteration 250, current loss: 66483.362072\n",
      "iteration 251, current loss: 66474.475452\n",
      "iteration 252, current loss: 66465.593376\n",
      "iteration 253, current loss: 66456.715832\n",
      "iteration 254, current loss: 66447.842907\n",
      "iteration 255, current loss: 66438.974790\n",
      "iteration 256, current loss: 66430.111776\n",
      "iteration 257, current loss: 66421.254261\n",
      "iteration 258, current loss: 66412.402724\n",
      "iteration 259, current loss: 66403.557714\n",
      "iteration 260, current loss: 66394.719813\n",
      "iteration 261, current loss: 66385.889601\n",
      "iteration 262, current loss: 66377.067628\n",
      "iteration 263, current loss: 66368.254371\n",
      "iteration 264, current loss: 66359.450216\n",
      "iteration 265, current loss: 66350.655434\n",
      "iteration 266, current loss: 66341.870179\n",
      "iteration 267, current loss: 66333.094487\n",
      "iteration 268, current loss: 66324.328283\n",
      "iteration 269, current loss: 66315.571395\n",
      "iteration 270, current loss: 66306.823569\n",
      "iteration 271, current loss: 66298.084483\n",
      "iteration 272, current loss: 66289.353761\n",
      "iteration 273, current loss: 66280.630989\n",
      "iteration 274, current loss: 66271.915729\n",
      "iteration 275, current loss: 66263.207528\n",
      "iteration 276, current loss: 66254.505929\n",
      "iteration 277, current loss: 66245.810489\n",
      "iteration 278, current loss: 66237.120784\n",
      "iteration 279, current loss: 66228.436423\n",
      "iteration 280, current loss: 66219.757062\n",
      "iteration 281, current loss: 66211.082412\n",
      "iteration 282, current loss: 66202.412254\n",
      "iteration 283, current loss: 66193.746447\n",
      "iteration 284, current loss: 66185.084936\n",
      "iteration 285, current loss: 66176.427757\n",
      "iteration 286, current loss: 66167.775039\n",
      "iteration 287, current loss: 66159.126997\n",
      "iteration 288, current loss: 66150.483927\n",
      "iteration 289, current loss: 66141.846189\n",
      "iteration 290, current loss: 66133.214191\n",
      "iteration 291, current loss: 66124.588363\n",
      "iteration 292, current loss: 66115.969143\n",
      "iteration 293, current loss: 66107.356947\n",
      "iteration 294, current loss: 66098.752153\n",
      "iteration 295, current loss: 66090.155085\n",
      "iteration 296, current loss: 66081.565998\n",
      "iteration 297, current loss: 66072.985076\n",
      "iteration 298, current loss: 66064.412426\n",
      "iteration 299, current loss: 66055.848078\n",
      "iteration 300, current loss: 66047.291993\n",
      "iteration 301, current loss: 66038.744069\n",
      "iteration 302, current loss: 66030.204144\n",
      "iteration 303, current loss: 66021.672013\n",
      "iteration 304, current loss: 66013.147428\n",
      "iteration 305, current loss: 66004.630113\n",
      "iteration 306, current loss: 65996.119765\n",
      "iteration 307, current loss: 65987.616067\n",
      "iteration 308, current loss: 65979.118690\n",
      "iteration 309, current loss: 65970.627300\n",
      "iteration 310, current loss: 65962.141563\n",
      "iteration 311, current loss: 65953.661147\n",
      "iteration 312, current loss: 65945.185727\n",
      "iteration 313, current loss: 65936.714988\n",
      "iteration 314, current loss: 65928.248629\n",
      "iteration 315, current loss: 65919.786360\n",
      "iteration 316, current loss: 65911.327909\n",
      "iteration 317, current loss: 65902.873021\n",
      "iteration 318, current loss: 65894.421455\n",
      "iteration 319, current loss: 65885.972987\n",
      "iteration 320, current loss: 65877.527407\n",
      "iteration 321, current loss: 65869.084515\n",
      "iteration 322, current loss: 65860.644122\n",
      "iteration 323, current loss: 65852.206041\n",
      "iteration 324, current loss: 65843.770088\n",
      "iteration 325, current loss: 65835.336074\n",
      "iteration 326, current loss: 65826.903803\n",
      "iteration 327, current loss: 65818.473069\n",
      "iteration 328, current loss: 65810.043655\n",
      "iteration 329, current loss: 65801.615331\n",
      "iteration 330, current loss: 65793.187854\n",
      "iteration 331, current loss: 65784.760975\n",
      "iteration 332, current loss: 65776.334443\n",
      "iteration 333, current loss: 65767.908008\n",
      "iteration 334, current loss: 65759.481431\n",
      "iteration 335, current loss: 65751.054491\n",
      "iteration 336, current loss: 65742.626984\n",
      "iteration 337, current loss: 65734.198724\n",
      "iteration 338, current loss: 65725.769538\n",
      "iteration 339, current loss: 65717.339255\n",
      "iteration 340, current loss: 65708.907688\n",
      "iteration 341, current loss: 65700.474632\n",
      "iteration 342, current loss: 65692.039859\n",
      "iteration 343, current loss: 65683.603146\n",
      "iteration 344, current loss: 65675.164344\n",
      "iteration 345, current loss: 65666.723484\n",
      "iteration 346, current loss: 65658.280943\n",
      "iteration 347, current loss: 65649.837613\n",
      "iteration 348, current loss: 65641.395043\n",
      "iteration 349, current loss: 65632.955456\n",
      "iteration 350, current loss: 65624.521582\n",
      "iteration 351, current loss: 65616.096332\n",
      "iteration 352, current loss: 65607.682392\n",
      "iteration 353, current loss: 65599.281909\n",
      "iteration 354, current loss: 65590.896353\n",
      "iteration 355, current loss: 65582.526553\n",
      "iteration 356, current loss: 65574.172834\n",
      "iteration 357, current loss: 65565.835180\n",
      "iteration 358, current loss: 65557.513364\n",
      "iteration 359, current loss: 65549.207041\n",
      "iteration 360, current loss: 65540.915806\n",
      "iteration 361, current loss: 65532.639223\n",
      "iteration 362, current loss: 65524.376846\n",
      "iteration 363, current loss: 65516.128226\n",
      "iteration 364, current loss: 65507.892915\n",
      "iteration 365, current loss: 65499.670469\n",
      "iteration 366, current loss: 65491.460451\n",
      "iteration 367, current loss: 65483.262431\n",
      "iteration 368, current loss: 65475.075993\n",
      "iteration 369, current loss: 65466.900734\n",
      "iteration 370, current loss: 65458.736270\n",
      "iteration 371, current loss: 65450.582238\n",
      "iteration 372, current loss: 65442.438301\n",
      "iteration 373, current loss: 65434.304154\n",
      "iteration 374, current loss: 65426.179525\n",
      "iteration 375, current loss: 65418.064183\n",
      "iteration 376, current loss: 65409.957938\n",
      "iteration 377, current loss: 65401.860649\n",
      "iteration 378, current loss: 65393.772222\n",
      "iteration 379, current loss: 65385.692614\n",
      "iteration 380, current loss: 65377.621827\n",
      "iteration 381, current loss: 65369.559909\n",
      "iteration 382, current loss: 65361.506942\n",
      "iteration 383, current loss: 65353.463033\n",
      "iteration 384, current loss: 65345.428306\n",
      "iteration 385, current loss: 65337.402883\n",
      "iteration 386, current loss: 65329.386877\n",
      "iteration 387, current loss: 65321.380374\n",
      "iteration 388, current loss: 65313.383426\n",
      "iteration 389, current loss: 65305.396044\n",
      "iteration 390, current loss: 65297.418190\n",
      "iteration 391, current loss: 65289.449779\n",
      "iteration 392, current loss: 65281.490681\n",
      "iteration 393, current loss: 65273.540719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 394, current loss: 65265.599681\n",
      "iteration 395, current loss: 65257.667324\n",
      "iteration 396, current loss: 65249.743376\n",
      "iteration 397, current loss: 65241.827552\n",
      "iteration 398, current loss: 65233.919551\n",
      "iteration 399, current loss: 65226.019065\n",
      "iteration 400, current loss: 65218.125789\n",
      "iteration 401, current loss: 65210.239419\n",
      "iteration 402, current loss: 65202.359660\n",
      "iteration 403, current loss: 65194.486228\n",
      "iteration 404, current loss: 65186.618860\n",
      "iteration 405, current loss: 65178.757307\n",
      "iteration 406, current loss: 65170.901349\n",
      "iteration 407, current loss: 65163.050790\n",
      "iteration 408, current loss: 65155.205464\n",
      "iteration 409, current loss: 65147.365241\n",
      "iteration 410, current loss: 65139.530025\n",
      "iteration 411, current loss: 65131.699757\n",
      "iteration 412, current loss: 65123.874420\n",
      "iteration 413, current loss: 65116.054038\n",
      "iteration 414, current loss: 65108.238673\n",
      "iteration 415, current loss: 65100.428431\n",
      "iteration 416, current loss: 65092.623456\n",
      "iteration 417, current loss: 65084.823927\n",
      "iteration 418, current loss: 65077.030059\n",
      "iteration 419, current loss: 65069.242095\n",
      "iteration 420, current loss: 65061.460300\n",
      "iteration 421, current loss: 65053.684961\n",
      "iteration 422, current loss: 65045.916374\n",
      "iteration 423, current loss: 65038.154842\n",
      "iteration 424, current loss: 65030.400667\n",
      "iteration 425, current loss: 65022.654140\n",
      "iteration 426, current loss: 65014.915540\n",
      "iteration 427, current loss: 65007.185121\n",
      "iteration 428, current loss: 64999.463113\n",
      "iteration 429, current loss: 64991.749711\n",
      "iteration 430, current loss: 64984.045075\n",
      "iteration 431, current loss: 64976.349326\n",
      "iteration 432, current loss: 64968.662547\n",
      "iteration 433, current loss: 64960.984781\n",
      "iteration 434, current loss: 64953.316032\n",
      "iteration 435, current loss: 64945.656274\n",
      "iteration 436, current loss: 64938.005447\n",
      "iteration 437, current loss: 64930.363468\n",
      "iteration 438, current loss: 64922.730235\n",
      "iteration 439, current loss: 64915.105636\n",
      "iteration 440, current loss: 64907.489548\n",
      "iteration 441, current loss: 64899.881853\n",
      "iteration 442, current loss: 64892.282436\n",
      "iteration 443, current loss: 64884.691192\n",
      "iteration 444, current loss: 64877.108032\n",
      "iteration 445, current loss: 64869.532880\n",
      "iteration 446, current loss: 64861.965678\n",
      "iteration 447, current loss: 64854.406378\n",
      "iteration 448, current loss: 64846.854942\n",
      "iteration 449, current loss: 64839.311336\n",
      "iteration 450, current loss: 64831.775520\n",
      "iteration 451, current loss: 64824.247442\n",
      "iteration 452, current loss: 64816.727029\n",
      "iteration 453, current loss: 64809.214176\n",
      "iteration 454, current loss: 64801.708744\n",
      "iteration 455, current loss: 64794.210551\n",
      "iteration 456, current loss: 64786.719367\n",
      "iteration 457, current loss: 64779.234917\n",
      "iteration 458, current loss: 64771.756879\n",
      "iteration 459, current loss: 64764.284889\n",
      "iteration 460, current loss: 64756.818548\n",
      "iteration 461, current loss: 64749.357431\n",
      "iteration 462, current loss: 64741.901099\n",
      "iteration 463, current loss: 64734.449117\n",
      "iteration 464, current loss: 64727.001069\n",
      "iteration 465, current loss: 64719.556585\n",
      "iteration 466, current loss: 64712.115359\n",
      "iteration 467, current loss: 64704.677177\n",
      "iteration 468, current loss: 64697.241938\n",
      "iteration 469, current loss: 64689.809678\n",
      "iteration 470, current loss: 64682.380576\n",
      "iteration 471, current loss: 64674.954957\n",
      "iteration 472, current loss: 64667.533287\n",
      "iteration 473, current loss: 64660.116139\n",
      "iteration 474, current loss: 64652.704164\n",
      "iteration 475, current loss: 64645.298045\n",
      "iteration 476, current loss: 64637.898454\n",
      "iteration 477, current loss: 64630.506013\n",
      "iteration 478, current loss: 64623.121264\n",
      "iteration 479, current loss: 64615.744657\n",
      "iteration 480, current loss: 64608.376549\n",
      "iteration 481, current loss: 64601.017210\n",
      "iteration 482, current loss: 64593.666845\n",
      "iteration 483, current loss: 64586.325611\n",
      "iteration 484, current loss: 64578.993633\n",
      "iteration 485, current loss: 64571.671022\n",
      "iteration 486, current loss: 64564.357875\n",
      "iteration 487, current loss: 64557.054279\n",
      "iteration 488, current loss: 64549.760305\n",
      "iteration 489, current loss: 64542.475998\n",
      "iteration 490, current loss: 64535.201362\n",
      "iteration 491, current loss: 64527.936356\n",
      "iteration 492, current loss: 64520.680885\n",
      "iteration 493, current loss: 64513.434792\n",
      "iteration 494, current loss: 64506.197866\n",
      "iteration 495, current loss: 64498.969845\n",
      "iteration 496, current loss: 64491.750419\n",
      "iteration 497, current loss: 64484.539249\n",
      "iteration 498, current loss: 64477.335972\n",
      "iteration 499, current loss: 64470.140218\n",
      "iteration 500, current loss: 64462.951617\n",
      "iteration 501, current loss: 64455.769815\n",
      "iteration 502, current loss: 64448.594478\n",
      "iteration 503, current loss: 64441.425309\n",
      "iteration 504, current loss: 64434.262047\n",
      "iteration 505, current loss: 64427.104477\n",
      "iteration 506, current loss: 64419.952433\n",
      "iteration 507, current loss: 64412.805800\n",
      "iteration 508, current loss: 64405.664513\n",
      "iteration 509, current loss: 64398.528553\n",
      "iteration 510, current loss: 64391.397945\n",
      "iteration 511, current loss: 64384.272749\n",
      "iteration 512, current loss: 64377.153050\n",
      "iteration 513, current loss: 64370.038954\n",
      "iteration 514, current loss: 64362.930574\n",
      "iteration 515, current loss: 64355.828021\n",
      "iteration 516, current loss: 64348.731400\n",
      "iteration 517, current loss: 64341.640802\n",
      "iteration 518, current loss: 64334.556298\n",
      "iteration 519, current loss: 64327.477939\n",
      "iteration 520, current loss: 64320.405755\n",
      "iteration 521, current loss: 64313.339753\n",
      "iteration 522, current loss: 64306.279923\n",
      "iteration 523, current loss: 64299.226236\n",
      "iteration 524, current loss: 64292.178649\n",
      "iteration 525, current loss: 64285.137105\n",
      "iteration 526, current loss: 64278.101540\n",
      "iteration 527, current loss: 64271.071879\n",
      "iteration 528, current loss: 64264.048042\n",
      "iteration 529, current loss: 64257.029942\n",
      "iteration 530, current loss: 64250.017489\n",
      "iteration 531, current loss: 64243.010590\n",
      "iteration 532, current loss: 64236.009148\n",
      "iteration 533, current loss: 64229.013063\n",
      "iteration 534, current loss: 64222.022236\n",
      "iteration 535, current loss: 64215.036567\n",
      "iteration 536, current loss: 64208.055955\n",
      "iteration 537, current loss: 64201.080305\n",
      "iteration 538, current loss: 64194.109525\n",
      "iteration 539, current loss: 64187.143532\n",
      "iteration 540, current loss: 64180.182253\n",
      "iteration 541, current loss: 64173.225631\n",
      "iteration 542, current loss: 64166.273627\n",
      "iteration 543, current loss: 64159.326224\n",
      "iteration 544, current loss: 64152.383428\n",
      "iteration 545, current loss: 64145.445276\n",
      "iteration 546, current loss: 64138.511832\n",
      "iteration 547, current loss: 64131.583190\n",
      "iteration 548, current loss: 64124.659471\n",
      "iteration 549, current loss: 64117.740820\n",
      "iteration 550, current loss: 64110.827400\n",
      "iteration 551, current loss: 64103.919386\n",
      "iteration 552, current loss: 64097.016958\n",
      "iteration 553, current loss: 64090.120295\n",
      "iteration 554, current loss: 64083.229567\n",
      "iteration 555, current loss: 64076.344930\n",
      "iteration 556, current loss: 64069.466523\n",
      "iteration 557, current loss: 64062.594468\n",
      "iteration 558, current loss: 64055.728866\n",
      "iteration 559, current loss: 64048.869801\n",
      "iteration 560, current loss: 64042.017339\n",
      "iteration 561, current loss: 64035.171534\n",
      "iteration 562, current loss: 64028.332427\n",
      "iteration 563, current loss: 64021.500049\n",
      "iteration 564, current loss: 64014.674423\n",
      "iteration 565, current loss: 64007.855563\n",
      "iteration 566, current loss: 64001.043480\n",
      "iteration 567, current loss: 63994.238173\n",
      "iteration 568, current loss: 63987.439639\n",
      "iteration 569, current loss: 63980.647864\n",
      "iteration 570, current loss: 63973.862826\n",
      "iteration 571, current loss: 63967.084495\n",
      "iteration 572, current loss: 63960.312828\n",
      "iteration 573, current loss: 63953.547776\n",
      "iteration 574, current loss: 63946.789276\n",
      "iteration 575, current loss: 63940.037256\n",
      "iteration 576, current loss: 63933.291631\n",
      "iteration 577, current loss: 63926.552309\n",
      "iteration 578, current loss: 63919.819188\n",
      "iteration 579, current loss: 63913.092156\n",
      "iteration 580, current loss: 63906.371098\n",
      "iteration 581, current loss: 63899.655888\n",
      "iteration 582, current loss: 63892.946397\n",
      "iteration 583, current loss: 63886.242492\n",
      "iteration 584, current loss: 63879.544037\n",
      "iteration 585, current loss: 63872.850893\n",
      "iteration 586, current loss: 63866.162918\n",
      "iteration 587, current loss: 63859.479971\n",
      "iteration 588, current loss: 63852.801907\n",
      "iteration 589, current loss: 63846.128581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 590, current loss: 63839.459847\n",
      "iteration 591, current loss: 63832.795557\n",
      "iteration 592, current loss: 63826.135560\n",
      "iteration 593, current loss: 63819.479704\n",
      "iteration 594, current loss: 63812.827834\n",
      "iteration 595, current loss: 63806.179790\n",
      "iteration 596, current loss: 63799.535409\n",
      "iteration 597, current loss: 63792.894524\n",
      "iteration 598, current loss: 63786.256962\n",
      "iteration 599, current loss: 63779.622545\n",
      "iteration 600, current loss: 63772.991090\n",
      "iteration 601, current loss: 63766.362409\n",
      "iteration 602, current loss: 63759.736308\n",
      "iteration 603, current loss: 63753.112586\n",
      "iteration 604, current loss: 63746.491039\n",
      "iteration 605, current loss: 63739.871455\n",
      "iteration 606, current loss: 63733.253619\n",
      "iteration 607, current loss: 63726.637309\n",
      "iteration 608, current loss: 63720.022300\n",
      "iteration 609, current loss: 63713.408363\n",
      "iteration 610, current loss: 63706.795263\n",
      "iteration 611, current loss: 63700.182765\n",
      "iteration 612, current loss: 63693.570630\n",
      "iteration 613, current loss: 63686.958621\n",
      "iteration 614, current loss: 63680.346501\n",
      "iteration 615, current loss: 63673.734037\n",
      "iteration 616, current loss: 63667.121002\n",
      "iteration 617, current loss: 63660.507180\n",
      "iteration 618, current loss: 63653.892372\n",
      "iteration 619, current loss: 63647.276397\n",
      "iteration 620, current loss: 63640.659102\n",
      "iteration 621, current loss: 63634.040368\n",
      "iteration 622, current loss: 63627.420118\n",
      "iteration 623, current loss: 63620.798327\n",
      "iteration 624, current loss: 63614.175027\n",
      "iteration 625, current loss: 63607.550316\n",
      "iteration 626, current loss: 63600.924364\n",
      "iteration 627, current loss: 63594.297415\n",
      "iteration 628, current loss: 63587.669786\n",
      "iteration 629, current loss: 63581.041862\n",
      "iteration 630, current loss: 63574.414088\n",
      "iteration 631, current loss: 63567.786949\n",
      "iteration 632, current loss: 63561.160958\n",
      "iteration 633, current loss: 63554.536633\n",
      "iteration 634, current loss: 63547.914476\n",
      "iteration 635, current loss: 63541.294955\n",
      "iteration 636, current loss: 63534.678488\n",
      "iteration 637, current loss: 63528.065433\n",
      "iteration 638, current loss: 63521.456077\n",
      "iteration 639, current loss: 63514.850641\n",
      "iteration 640, current loss: 63508.249278\n",
      "iteration 641, current loss: 63501.652076\n",
      "iteration 642, current loss: 63495.059071\n",
      "iteration 643, current loss: 63488.470247\n",
      "iteration 644, current loss: 63481.885549\n",
      "iteration 645, current loss: 63475.304890\n",
      "iteration 646, current loss: 63468.728153\n",
      "iteration 647, current loss: 63462.155202\n",
      "iteration 648, current loss: 63455.585886\n",
      "iteration 649, current loss: 63449.020039\n",
      "iteration 650, current loss: 63442.457491\n",
      "iteration 651, current loss: 63435.898064\n",
      "iteration 652, current loss: 63429.341578\n",
      "iteration 653, current loss: 63422.787855\n",
      "iteration 654, current loss: 63416.236715\n",
      "iteration 655, current loss: 63409.687983\n",
      "iteration 656, current loss: 63403.141486\n",
      "iteration 657, current loss: 63396.597058\n",
      "iteration 658, current loss: 63390.054536\n",
      "iteration 659, current loss: 63383.513764\n",
      "iteration 660, current loss: 63376.974591\n",
      "iteration 661, current loss: 63370.436872\n",
      "iteration 662, current loss: 63363.900469\n",
      "iteration 663, current loss: 63357.365250\n",
      "iteration 664, current loss: 63350.831088\n",
      "iteration 665, current loss: 63344.297863\n",
      "iteration 666, current loss: 63337.765459\n",
      "iteration 667, current loss: 63331.233767\n",
      "iteration 668, current loss: 63324.702682\n",
      "iteration 669, current loss: 63318.172104\n",
      "iteration 670, current loss: 63311.641937\n",
      "iteration 671, current loss: 63305.112091\n",
      "iteration 672, current loss: 63298.582478\n",
      "iteration 673, current loss: 63292.053014\n",
      "iteration 674, current loss: 63285.523621\n",
      "iteration 675, current loss: 63278.994221\n",
      "iteration 676, current loss: 63272.464741\n",
      "iteration 677, current loss: 63265.935110\n",
      "iteration 678, current loss: 63259.405260\n",
      "iteration 679, current loss: 63252.875125\n",
      "iteration 680, current loss: 63246.344642\n",
      "iteration 681, current loss: 63239.813747\n",
      "iteration 682, current loss: 63233.282378\n",
      "iteration 683, current loss: 63226.750477\n",
      "iteration 684, current loss: 63220.217982\n",
      "iteration 685, current loss: 63213.684833\n",
      "iteration 686, current loss: 63207.150970\n",
      "iteration 687, current loss: 63200.616331\n",
      "iteration 688, current loss: 63194.080855\n",
      "iteration 689, current loss: 63187.544478\n",
      "iteration 690, current loss: 63181.007134\n",
      "iteration 691, current loss: 63174.468758\n",
      "iteration 692, current loss: 63167.929279\n",
      "iteration 693, current loss: 63161.388628\n",
      "iteration 694, current loss: 63154.846731\n",
      "iteration 695, current loss: 63148.303512\n",
      "iteration 696, current loss: 63141.758893\n",
      "iteration 697, current loss: 63135.212794\n",
      "iteration 698, current loss: 63128.665130\n",
      "iteration 699, current loss: 63122.115817\n",
      "iteration 700, current loss: 63115.564764\n",
      "iteration 701, current loss: 63109.011881\n",
      "iteration 702, current loss: 63102.457071\n",
      "iteration 703, current loss: 63095.900238\n",
      "iteration 704, current loss: 63089.341281\n",
      "iteration 705, current loss: 63082.780096\n",
      "iteration 706, current loss: 63076.216578\n",
      "iteration 707, current loss: 63069.650617\n",
      "iteration 708, current loss: 63063.082105\n",
      "iteration 709, current loss: 63056.510932\n",
      "iteration 710, current loss: 63049.936987\n",
      "iteration 711, current loss: 63043.360161\n",
      "iteration 712, current loss: 63036.780351\n",
      "iteration 713, current loss: 63030.197455\n",
      "iteration 714, current loss: 63023.611380\n",
      "iteration 715, current loss: 63017.022045\n",
      "iteration 716, current loss: 63010.429380\n",
      "iteration 717, current loss: 63003.833331\n",
      "iteration 718, current loss: 62997.233865\n",
      "iteration 719, current loss: 62990.630972\n",
      "iteration 720, current loss: 62984.024667\n",
      "iteration 721, current loss: 62977.414999\n",
      "iteration 722, current loss: 62970.802049\n",
      "iteration 723, current loss: 62964.185934\n",
      "iteration 724, current loss: 62957.566813\n",
      "iteration 725, current loss: 62950.944882\n",
      "iteration 726, current loss: 62944.320383\n",
      "iteration 727, current loss: 62937.693595\n",
      "iteration 728, current loss: 62931.064838\n",
      "iteration 729, current loss: 62924.434469\n",
      "iteration 730, current loss: 62917.802876\n",
      "iteration 731, current loss: 62911.170475\n",
      "iteration 732, current loss: 62904.537704\n",
      "iteration 733, current loss: 62897.905012\n",
      "iteration 734, current loss: 62891.272858\n",
      "iteration 735, current loss: 62884.641698\n",
      "iteration 736, current loss: 62878.011981\n",
      "iteration 737, current loss: 62871.384138\n",
      "iteration 738, current loss: 62864.758580\n",
      "iteration 739, current loss: 62858.135689\n",
      "iteration 740, current loss: 62851.515817\n",
      "iteration 741, current loss: 62844.899278\n",
      "iteration 742, current loss: 62838.286351\n",
      "iteration 743, current loss: 62831.677274\n",
      "iteration 744, current loss: 62825.072249\n",
      "iteration 745, current loss: 62818.471439\n",
      "iteration 746, current loss: 62811.874972\n",
      "iteration 747, current loss: 62805.282942\n",
      "iteration 748, current loss: 62798.695410\n",
      "iteration 749, current loss: 62792.112412\n",
      "iteration 750, current loss: 62785.533953\n",
      "iteration 751, current loss: 62778.960018\n",
      "iteration 752, current loss: 62772.390569\n",
      "iteration 753, current loss: 62765.825549\n",
      "iteration 754, current loss: 62759.264883\n",
      "iteration 755, current loss: 62752.708481\n",
      "iteration 756, current loss: 62746.156238\n",
      "iteration 757, current loss: 62739.608036\n",
      "iteration 758, current loss: 62733.063745\n",
      "iteration 759, current loss: 62726.523224\n",
      "iteration 760, current loss: 62719.986323\n",
      "iteration 761, current loss: 62713.452883\n",
      "iteration 762, current loss: 62706.922736\n",
      "iteration 763, current loss: 62700.395709\n",
      "iteration 764, current loss: 62693.871624\n",
      "iteration 765, current loss: 62687.350299\n",
      "iteration 766, current loss: 62680.831551\n",
      "iteration 767, current loss: 62674.315195\n",
      "iteration 768, current loss: 62667.801050\n",
      "iteration 769, current loss: 62661.288937\n",
      "iteration 770, current loss: 62654.778681\n",
      "iteration 771, current loss: 62648.270117\n",
      "iteration 772, current loss: 62641.763086\n",
      "iteration 773, current loss: 62635.257439\n",
      "iteration 774, current loss: 62628.753038\n",
      "iteration 775, current loss: 62622.249756\n",
      "iteration 776, current loss: 62615.747475\n",
      "iteration 777, current loss: 62609.246087\n",
      "iteration 778, current loss: 62602.745492\n",
      "iteration 779, current loss: 62596.245594\n",
      "iteration 780, current loss: 62589.746301\n",
      "iteration 781, current loss: 62583.247517\n",
      "iteration 782, current loss: 62576.749142\n",
      "iteration 783, current loss: 62570.251068\n",
      "iteration 784, current loss: 62563.753171\n",
      "iteration 785, current loss: 62557.255313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 786, current loss: 62550.757338\n",
      "iteration 787, current loss: 62544.259068\n",
      "iteration 788, current loss: 62537.760307\n",
      "iteration 789, current loss: 62531.260839\n",
      "iteration 790, current loss: 62524.760433\n",
      "iteration 791, current loss: 62518.258845\n",
      "iteration 792, current loss: 62511.755826\n",
      "iteration 793, current loss: 62505.251128\n",
      "iteration 794, current loss: 62498.744513\n",
      "iteration 795, current loss: 62492.235763\n",
      "iteration 796, current loss: 62485.724693\n",
      "iteration 797, current loss: 62479.211156\n",
      "iteration 798, current loss: 62472.695062\n",
      "iteration 799, current loss: 62466.176378\n",
      "iteration 800, current loss: 62459.655145\n",
      "iteration 801, current loss: 62453.131471\n",
      "iteration 802, current loss: 62446.605539\n",
      "iteration 803, current loss: 62440.077594\n",
      "iteration 804, current loss: 62433.547932\n",
      "iteration 805, current loss: 62427.016885\n",
      "iteration 806, current loss: 62420.484793\n",
      "iteration 807, current loss: 62413.951983\n",
      "iteration 808, current loss: 62407.418737\n",
      "iteration 809, current loss: 62400.885271\n",
      "iteration 810, current loss: 62394.351706\n",
      "iteration 811, current loss: 62387.818047\n",
      "iteration 812, current loss: 62381.284169\n",
      "iteration 813, current loss: 62374.749795\n",
      "iteration 814, current loss: 62368.214490\n",
      "iteration 815, current loss: 62361.677649\n",
      "iteration 816, current loss: 62355.138497\n",
      "iteration 817, current loss: 62348.596099\n",
      "iteration 818, current loss: 62342.049393\n",
      "iteration 819, current loss: 62335.497245\n",
      "iteration 820, current loss: 62328.938564\n",
      "iteration 821, current loss: 62322.372469\n",
      "iteration 822, current loss: 62315.798529\n",
      "iteration 823, current loss: 62309.217048\n",
      "iteration 824, current loss: 62302.629335\n",
      "iteration 825, current loss: 62296.037848\n",
      "iteration 826, current loss: 62289.446083\n",
      "iteration 827, current loss: 62282.858159\n",
      "iteration 828, current loss: 62276.278188\n",
      "iteration 829, current loss: 62269.709650\n",
      "iteration 830, current loss: 62263.155000\n",
      "iteration 831, current loss: 62256.615582\n",
      "iteration 832, current loss: 62250.091792\n",
      "iteration 833, current loss: 62243.583352\n",
      "iteration 834, current loss: 62237.089578\n",
      "iteration 835, current loss: 62230.609590\n",
      "iteration 836, current loss: 62224.142455\n",
      "iteration 837, current loss: 62217.687274\n",
      "iteration 838, current loss: 62211.243227\n",
      "iteration 839, current loss: 62204.809595\n",
      "iteration 840, current loss: 62198.385759\n",
      "iteration 841, current loss: 62191.971202\n",
      "iteration 842, current loss: 62185.565495\n",
      "iteration 843, current loss: 62179.168289\n",
      "iteration 844, current loss: 62172.779296\n",
      "iteration 845, current loss: 62166.398280\n",
      "iteration 846, current loss: 62160.025043\n",
      "iteration 847, current loss: 62153.659412\n",
      "iteration 848, current loss: 62147.301228\n",
      "iteration 849, current loss: 62140.950341\n",
      "iteration 850, current loss: 62134.606600\n",
      "iteration 851, current loss: 62128.269845\n",
      "iteration 852, current loss: 62121.939912\n",
      "iteration 853, current loss: 62115.616623\n",
      "iteration 854, current loss: 62109.299788\n",
      "iteration 855, current loss: 62102.989207\n",
      "iteration 856, current loss: 62096.684666\n",
      "iteration 857, current loss: 62090.385946\n",
      "iteration 858, current loss: 62084.092816\n",
      "iteration 859, current loss: 62077.805041\n",
      "iteration 860, current loss: 62071.522382\n",
      "iteration 861, current loss: 62065.244599\n",
      "iteration 862, current loss: 62058.971449\n",
      "iteration 863, current loss: 62052.702692\n",
      "iteration 864, current loss: 62046.438091\n",
      "iteration 865, current loss: 62040.177415\n",
      "iteration 866, current loss: 62033.920437\n",
      "iteration 867, current loss: 62027.666940\n",
      "iteration 868, current loss: 62021.416716\n",
      "iteration 869, current loss: 62015.169565\n",
      "iteration 870, current loss: 62008.925304\n",
      "iteration 871, current loss: 62002.683761\n",
      "iteration 872, current loss: 61996.444780\n",
      "iteration 873, current loss: 61990.208221\n",
      "iteration 874, current loss: 61983.973963\n",
      "iteration 875, current loss: 61977.741905\n",
      "iteration 876, current loss: 61971.511965\n",
      "iteration 877, current loss: 61965.284086\n",
      "iteration 878, current loss: 61959.058229\n",
      "iteration 879, current loss: 61952.834381\n",
      "iteration 880, current loss: 61946.612549\n",
      "iteration 881, current loss: 61940.392763\n",
      "iteration 882, current loss: 61934.175075\n",
      "iteration 883, current loss: 61927.959553\n",
      "iteration 884, current loss: 61921.746282\n",
      "iteration 885, current loss: 61915.535362\n",
      "iteration 886, current loss: 61909.326900\n",
      "iteration 887, current loss: 61903.121012\n",
      "iteration 888, current loss: 61896.917812\n",
      "iteration 889, current loss: 61890.717415\n",
      "iteration 890, current loss: 61884.519925\n",
      "iteration 891, current loss: 61878.325438\n",
      "iteration 892, current loss: 61872.134034\n",
      "iteration 893, current loss: 61865.945775\n",
      "iteration 894, current loss: 61859.760703\n",
      "iteration 895, current loss: 61853.578838\n",
      "iteration 896, current loss: 61847.400178\n",
      "iteration 897, current loss: 61841.224699\n",
      "iteration 898, current loss: 61835.052354\n",
      "iteration 899, current loss: 61828.883078\n",
      "iteration 900, current loss: 61822.716786\n",
      "iteration 901, current loss: 61816.553379\n",
      "iteration 902, current loss: 61810.392746\n",
      "iteration 903, current loss: 61804.234765\n",
      "iteration 904, current loss: 61798.079310\n",
      "iteration 905, current loss: 61791.926251\n",
      "iteration 906, current loss: 61785.775460\n",
      "iteration 907, current loss: 61779.626811\n",
      "iteration 908, current loss: 61773.480183\n",
      "iteration 909, current loss: 61767.335464\n",
      "iteration 910, current loss: 61761.192550\n",
      "iteration 911, current loss: 61755.051349\n",
      "iteration 912, current loss: 61748.911778\n",
      "iteration 913, current loss: 61742.773769\n",
      "iteration 914, current loss: 61736.637263\n",
      "iteration 915, current loss: 61730.502212\n",
      "iteration 916, current loss: 61724.368582\n",
      "iteration 917, current loss: 61718.236345\n",
      "iteration 918, current loss: 61712.105485\n",
      "iteration 919, current loss: 61705.975991\n",
      "iteration 920, current loss: 61699.847861\n",
      "iteration 921, current loss: 61693.721096\n",
      "iteration 922, current loss: 61687.595703\n",
      "iteration 923, current loss: 61681.471690\n",
      "iteration 924, current loss: 61675.349068\n",
      "iteration 925, current loss: 61669.227847\n",
      "iteration 926, current loss: 61663.108038\n",
      "iteration 927, current loss: 61656.989647\n",
      "iteration 928, current loss: 61650.872682\n",
      "iteration 929, current loss: 61644.757146\n",
      "iteration 930, current loss: 61638.643037\n",
      "iteration 931, current loss: 61632.530354\n",
      "iteration 932, current loss: 61626.419089\n",
      "iteration 933, current loss: 61620.309232\n",
      "iteration 934, current loss: 61614.200769\n",
      "iteration 935, current loss: 61608.093684\n",
      "iteration 936, current loss: 61601.987958\n",
      "iteration 937, current loss: 61595.883572\n",
      "iteration 938, current loss: 61589.780505\n",
      "iteration 939, current loss: 61583.678735\n",
      "iteration 940, current loss: 61577.578241\n",
      "iteration 941, current loss: 61571.479003\n",
      "iteration 942, current loss: 61565.381003\n",
      "iteration 943, current loss: 61559.284225\n",
      "iteration 944, current loss: 61553.188657\n",
      "iteration 945, current loss: 61547.094290\n",
      "iteration 946, current loss: 61541.001122\n",
      "iteration 947, current loss: 61534.909154\n",
      "iteration 948, current loss: 61528.818395\n",
      "iteration 949, current loss: 61522.728861\n",
      "iteration 950, current loss: 61516.640574\n",
      "iteration 951, current loss: 61510.553564\n",
      "iteration 952, current loss: 61504.467872\n",
      "iteration 953, current loss: 61498.383542\n",
      "iteration 954, current loss: 61492.300632\n",
      "iteration 955, current loss: 61486.219203\n",
      "iteration 956, current loss: 61480.139325\n",
      "iteration 957, current loss: 61474.061074\n",
      "iteration 958, current loss: 61467.984530\n",
      "iteration 959, current loss: 61461.909780\n",
      "iteration 960, current loss: 61455.836908\n",
      "iteration 961, current loss: 61449.766003\n",
      "iteration 962, current loss: 61443.697149\n",
      "iteration 963, current loss: 61437.630430\n",
      "iteration 964, current loss: 61431.565923\n",
      "iteration 965, current loss: 61425.503697\n",
      "iteration 966, current loss: 61419.443815\n",
      "iteration 967, current loss: 61413.386328\n",
      "iteration 968, current loss: 61407.331277\n",
      "iteration 969, current loss: 61401.278688\n",
      "iteration 970, current loss: 61395.228576\n",
      "iteration 971, current loss: 61389.180938\n",
      "iteration 972, current loss: 61383.135758\n",
      "iteration 973, current loss: 61377.093005\n",
      "iteration 974, current loss: 61371.052630\n",
      "iteration 975, current loss: 61365.014568\n",
      "iteration 976, current loss: 61358.978741\n",
      "iteration 977, current loss: 61352.945054\n",
      "iteration 978, current loss: 61346.913400\n",
      "iteration 979, current loss: 61340.883659\n",
      "iteration 980, current loss: 61334.855701\n",
      "iteration 981, current loss: 61328.829390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 982, current loss: 61322.804583\n",
      "iteration 983, current loss: 61316.781138\n",
      "iteration 984, current loss: 61310.758918\n",
      "iteration 985, current loss: 61304.737790\n",
      "iteration 986, current loss: 61298.717639\n",
      "iteration 987, current loss: 61292.698364\n",
      "iteration 988, current loss: 61286.679891\n",
      "iteration 989, current loss: 61280.662172\n",
      "iteration 990, current loss: 61274.645192\n",
      "iteration 991, current loss: 61268.628972\n",
      "iteration 992, current loss: 61262.613572\n",
      "iteration 993, current loss: 61256.599087\n",
      "iteration 994, current loss: 61250.585654\n",
      "iteration 995, current loss: 61244.573442\n",
      "iteration 996, current loss: 61238.562653\n",
      "iteration 997, current loss: 61232.553516\n",
      "iteration 998, current loss: 61226.546283\n",
      "iteration 999, current loss: 61220.541223\n",
      "iteration 1000, current loss: 61214.538620\n",
      "iteration 1001, current loss: 61208.538767\n",
      "iteration 1002, current loss: 61202.541964\n",
      "iteration 1003, current loss: 61196.548517\n",
      "iteration 1004, current loss: 61190.558734\n",
      "iteration 1005, current loss: 61184.572925\n",
      "iteration 1006, current loss: 61178.591397\n",
      "iteration 1007, current loss: 61172.614456\n",
      "iteration 1008, current loss: 61166.642397\n",
      "iteration 1009, current loss: 61160.675503\n",
      "iteration 1010, current loss: 61154.714043\n",
      "iteration 1011, current loss: 61148.758264\n",
      "iteration 1012, current loss: 61142.808388\n",
      "iteration 1013, current loss: 61136.864612\n",
      "iteration 1014, current loss: 61130.927105\n",
      "iteration 1015, current loss: 61124.996013\n",
      "iteration 1016, current loss: 61119.071456\n",
      "iteration 1017, current loss: 61113.153535\n",
      "iteration 1018, current loss: 61107.242337\n",
      "iteration 1019, current loss: 61101.337941\n",
      "iteration 1020, current loss: 61095.440421\n",
      "iteration 1021, current loss: 61089.549848\n",
      "iteration 1022, current loss: 61083.666299\n",
      "iteration 1023, current loss: 61077.789848\n",
      "iteration 1024, current loss: 61071.920571\n",
      "iteration 1025, current loss: 61066.058541\n",
      "iteration 1026, current loss: 61060.203821\n",
      "iteration 1027, current loss: 61054.356457\n",
      "iteration 1028, current loss: 61048.516477\n",
      "iteration 1029, current loss: 61042.683879\n",
      "iteration 1030, current loss: 61036.858628\n",
      "iteration 1031, current loss: 61031.040654\n",
      "iteration 1032, current loss: 61025.229851\n",
      "iteration 1033, current loss: 61019.426075\n",
      "iteration 1034, current loss: 61013.629146\n",
      "iteration 1035, current loss: 61007.838851\n",
      "iteration 1036, current loss: 61002.054951\n",
      "iteration 1037, current loss: 60996.277179\n",
      "iteration 1038, current loss: 60990.505248\n",
      "iteration 1039, current loss: 60984.738855\n",
      "iteration 1040, current loss: 60978.977681\n",
      "iteration 1041, current loss: 60973.221396\n",
      "iteration 1042, current loss: 60967.469662\n",
      "iteration 1043, current loss: 60961.722133\n",
      "iteration 1044, current loss: 60955.978455\n",
      "iteration 1045, current loss: 60950.238273\n",
      "iteration 1046, current loss: 60944.501226\n",
      "iteration 1047, current loss: 60938.766951\n",
      "iteration 1048, current loss: 60933.035084\n",
      "iteration 1049, current loss: 60927.305263\n",
      "iteration 1050, current loss: 60921.577129\n",
      "iteration 1051, current loss: 60915.850329\n",
      "iteration 1052, current loss: 60910.124520\n",
      "iteration 1053, current loss: 60904.399377\n",
      "iteration 1054, current loss: 60898.674592\n",
      "iteration 1055, current loss: 60892.949888\n",
      "iteration 1056, current loss: 60887.225021\n",
      "iteration 1057, current loss: 60881.499790\n",
      "iteration 1058, current loss: 60875.774045\n",
      "iteration 1059, current loss: 60870.047698\n",
      "iteration 1060, current loss: 60864.320729\n",
      "iteration 1061, current loss: 60858.593190\n",
      "iteration 1062, current loss: 60852.865215\n",
      "iteration 1063, current loss: 60847.137016\n",
      "iteration 1064, current loss: 60841.408884\n",
      "iteration 1065, current loss: 60835.681185\n",
      "iteration 1066, current loss: 60829.954342\n",
      "iteration 1067, current loss: 60824.228832\n",
      "iteration 1068, current loss: 60818.505158\n",
      "iteration 1069, current loss: 60812.783841\n",
      "iteration 1070, current loss: 60807.065395\n",
      "iteration 1071, current loss: 60801.350314\n",
      "iteration 1072, current loss: 60795.639055\n",
      "iteration 1073, current loss: 60789.932032\n",
      "iteration 1074, current loss: 60784.229604\n",
      "iteration 1075, current loss: 60778.532075\n",
      "iteration 1076, current loss: 60772.839694\n",
      "iteration 1077, current loss: 60767.152658\n",
      "iteration 1078, current loss: 60761.471116\n",
      "iteration 1079, current loss: 60755.795174\n",
      "iteration 1080, current loss: 60750.124902\n",
      "iteration 1081, current loss: 60744.460340\n",
      "iteration 1082, current loss: 60738.801503\n",
      "iteration 1083, current loss: 60733.148384\n",
      "iteration 1084, current loss: 60727.500963\n",
      "iteration 1085, current loss: 60721.859203\n",
      "iteration 1086, current loss: 60716.223060\n",
      "iteration 1087, current loss: 60710.592479\n",
      "iteration 1088, current loss: 60704.967400\n",
      "iteration 1089, current loss: 60699.347755\n",
      "iteration 1090, current loss: 60693.733471\n",
      "iteration 1091, current loss: 60688.124470\n",
      "iteration 1092, current loss: 60682.520670\n",
      "iteration 1093, current loss: 60676.921982\n",
      "iteration 1094, current loss: 60671.328316\n",
      "iteration 1095, current loss: 60665.739574\n",
      "iteration 1096, current loss: 60660.155657\n",
      "iteration 1097, current loss: 60654.576460\n",
      "iteration 1098, current loss: 60649.001875\n",
      "iteration 1099, current loss: 60643.431791\n",
      "iteration 1100, current loss: 60637.866093\n",
      "iteration 1101, current loss: 60632.304664\n",
      "iteration 1102, current loss: 60626.747383\n",
      "iteration 1103, current loss: 60621.194130\n",
      "iteration 1104, current loss: 60615.644780\n",
      "iteration 1105, current loss: 60610.099208\n",
      "iteration 1106, current loss: 60604.557289\n",
      "iteration 1107, current loss: 60599.018894\n",
      "iteration 1108, current loss: 60593.483898\n",
      "iteration 1109, current loss: 60587.952173\n",
      "iteration 1110, current loss: 60582.423592\n",
      "iteration 1111, current loss: 60576.898028\n",
      "iteration 1112, current loss: 60571.375355\n",
      "iteration 1113, current loss: 60565.855448\n",
      "iteration 1114, current loss: 60560.338183\n",
      "iteration 1115, current loss: 60554.823436\n",
      "iteration 1116, current loss: 60549.311088\n",
      "iteration 1117, current loss: 60543.801017\n",
      "iteration 1118, current loss: 60538.293106\n",
      "iteration 1119, current loss: 60532.787238\n",
      "iteration 1120, current loss: 60527.283301\n",
      "iteration 1121, current loss: 60521.781182\n",
      "iteration 1122, current loss: 60516.280774\n",
      "iteration 1123, current loss: 60510.781968\n",
      "iteration 1124, current loss: 60505.284664\n",
      "iteration 1125, current loss: 60499.788762\n",
      "iteration 1126, current loss: 60494.294167\n",
      "iteration 1127, current loss: 60488.800789\n",
      "iteration 1128, current loss: 60483.308544\n",
      "iteration 1129, current loss: 60477.817355\n",
      "iteration 1130, current loss: 60472.327152\n",
      "iteration 1131, current loss: 60466.837877\n",
      "iteration 1132, current loss: 60461.349483\n",
      "iteration 1133, current loss: 60455.861939\n",
      "iteration 1134, current loss: 60450.375232\n",
      "iteration 1135, current loss: 60444.889370\n",
      "iteration 1136, current loss: 60439.404390\n",
      "iteration 1137, current loss: 60433.920357\n",
      "iteration 1138, current loss: 60428.437369\n",
      "iteration 1139, current loss: 60422.955566\n",
      "iteration 1140, current loss: 60417.475124\n",
      "iteration 1141, current loss: 60411.996263\n",
      "iteration 1142, current loss: 60406.519242\n",
      "iteration 1143, current loss: 60401.044355\n",
      "iteration 1144, current loss: 60395.571924\n",
      "iteration 1145, current loss: 60390.102291\n",
      "iteration 1146, current loss: 60384.635803\n",
      "iteration 1147, current loss: 60379.172798\n",
      "iteration 1148, current loss: 60373.713594\n",
      "iteration 1149, current loss: 60368.258473\n",
      "iteration 1150, current loss: 60362.807672\n",
      "iteration 1151, current loss: 60357.361372\n",
      "iteration 1152, current loss: 60351.919700\n",
      "iteration 1153, current loss: 60346.482722\n",
      "iteration 1154, current loss: 60341.050446\n",
      "iteration 1155, current loss: 60335.622829\n",
      "iteration 1156, current loss: 60330.199781\n",
      "iteration 1157, current loss: 60324.781168\n",
      "iteration 1158, current loss: 60319.366823\n",
      "iteration 1159, current loss: 60313.956551\n",
      "iteration 1160, current loss: 60308.550129\n",
      "iteration 1161, current loss: 60303.147319\n",
      "iteration 1162, current loss: 60297.747863\n",
      "iteration 1163, current loss: 60292.351493\n",
      "iteration 1164, current loss: 60286.957928\n",
      "iteration 1165, current loss: 60281.566878\n",
      "iteration 1166, current loss: 60276.178048\n",
      "iteration 1167, current loss: 60270.791133\n",
      "iteration 1168, current loss: 60265.405826\n",
      "iteration 1169, current loss: 60260.021817\n",
      "iteration 1170, current loss: 60254.638795\n",
      "iteration 1171, current loss: 60249.256453\n",
      "iteration 1172, current loss: 60243.874490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1173, current loss: 60238.492616\n",
      "iteration 1174, current loss: 60233.110561\n",
      "iteration 1175, current loss: 60227.728083\n",
      "iteration 1176, current loss: 60222.344972\n",
      "iteration 1177, current loss: 60216.961069\n",
      "iteration 1178, current loss: 60211.576268\n",
      "iteration 1179, current loss: 60206.190527\n",
      "iteration 1180, current loss: 60200.803879\n",
      "iteration 1181, current loss: 60195.416430\n",
      "iteration 1182, current loss: 60190.028359\n",
      "iteration 1183, current loss: 60184.639907\n",
      "iteration 1184, current loss: 60179.251363\n",
      "iteration 1185, current loss: 60173.863042\n",
      "iteration 1186, current loss: 60168.475259\n",
      "iteration 1187, current loss: 60163.088301\n",
      "iteration 1188, current loss: 60157.702406\n",
      "iteration 1189, current loss: 60152.317737\n",
      "iteration 1190, current loss: 60146.934376\n",
      "iteration 1191, current loss: 60141.552309\n",
      "iteration 1192, current loss: 60136.171435\n",
      "iteration 1193, current loss: 60130.791567\n",
      "iteration 1194, current loss: 60125.412446\n",
      "iteration 1195, current loss: 60120.033753\n",
      "iteration 1196, current loss: 60114.655123\n",
      "iteration 1197, current loss: 60109.276162\n",
      "iteration 1198, current loss: 60103.896460\n",
      "iteration 1199, current loss: 60098.515605\n",
      "iteration 1200, current loss: 60093.133195\n",
      "iteration 1201, current loss: 60087.748854\n",
      "iteration 1202, current loss: 60082.362239\n",
      "iteration 1203, current loss: 60076.973052\n",
      "iteration 1204, current loss: 60071.581052\n",
      "iteration 1205, current loss: 60066.186064\n",
      "iteration 1206, current loss: 60060.787987\n",
      "iteration 1207, current loss: 60055.386796\n",
      "iteration 1208, current loss: 60049.982557\n",
      "iteration 1209, current loss: 60044.575417\n",
      "iteration 1210, current loss: 60039.165610\n",
      "iteration 1211, current loss: 60033.753450\n",
      "iteration 1212, current loss: 60028.339326\n",
      "iteration 1213, current loss: 60022.923684\n",
      "iteration 1214, current loss: 60017.507020\n",
      "iteration 1215, current loss: 60012.089864\n",
      "iteration 1216, current loss: 60006.672759\n",
      "iteration 1217, current loss: 60001.256253\n",
      "iteration 1218, current loss: 59995.840878\n",
      "iteration 1219, current loss: 59990.427143\n",
      "iteration 1220, current loss: 59985.015522\n",
      "iteration 1221, current loss: 59979.606451\n",
      "iteration 1222, current loss: 59974.200319\n",
      "iteration 1223, current loss: 59968.797475\n",
      "iteration 1224, current loss: 59963.398224\n",
      "iteration 1225, current loss: 59958.002829\n",
      "iteration 1226, current loss: 59952.611520\n",
      "iteration 1227, current loss: 59947.224492\n",
      "iteration 1228, current loss: 59941.841913\n",
      "iteration 1229, current loss: 59936.463927\n",
      "iteration 1230, current loss: 59931.090658\n",
      "iteration 1231, current loss: 59925.722213\n",
      "iteration 1232, current loss: 59920.358680\n",
      "iteration 1233, current loss: 59915.000137\n",
      "iteration 1234, current loss: 59909.646648\n",
      "iteration 1235, current loss: 59904.298264\n",
      "iteration 1236, current loss: 59898.955024\n",
      "iteration 1237, current loss: 59893.616955\n",
      "iteration 1238, current loss: 59888.284068\n",
      "iteration 1239, current loss: 59882.956363\n",
      "iteration 1240, current loss: 59877.633823\n",
      "iteration 1241, current loss: 59872.316417\n",
      "iteration 1242, current loss: 59867.004098\n",
      "iteration 1243, current loss: 59861.696801\n",
      "iteration 1244, current loss: 59856.394447\n",
      "iteration 1245, current loss: 59851.096941\n",
      "iteration 1246, current loss: 59845.804172\n",
      "iteration 1247, current loss: 59840.516015\n",
      "iteration 1248, current loss: 59835.232331\n",
      "iteration 1249, current loss: 59829.952971\n",
      "iteration 1250, current loss: 59824.677773\n",
      "iteration 1251, current loss: 59819.406570\n",
      "iteration 1252, current loss: 59814.139187\n",
      "iteration 1253, current loss: 59808.875446\n",
      "iteration 1254, current loss: 59803.615166\n",
      "iteration 1255, current loss: 59798.358169\n",
      "iteration 1256, current loss: 59793.104280\n",
      "iteration 1257, current loss: 59787.853331\n",
      "iteration 1258, current loss: 59782.605163\n",
      "iteration 1259, current loss: 59777.359630\n",
      "iteration 1260, current loss: 59772.116603\n",
      "iteration 1261, current loss: 59766.875970\n",
      "iteration 1262, current loss: 59761.637641\n",
      "iteration 1263, current loss: 59756.401552\n",
      "iteration 1264, current loss: 59751.167668\n",
      "iteration 1265, current loss: 59745.935982\n",
      "iteration 1266, current loss: 59740.706523\n",
      "iteration 1267, current loss: 59735.479353\n",
      "iteration 1268, current loss: 59730.254571\n",
      "iteration 1269, current loss: 59725.032311\n",
      "iteration 1270, current loss: 59719.812745\n",
      "iteration 1271, current loss: 59714.596075\n",
      "iteration 1272, current loss: 59709.382535\n",
      "iteration 1273, current loss: 59704.172384\n",
      "iteration 1274, current loss: 59698.965898\n",
      "iteration 1275, current loss: 59693.763365\n",
      "iteration 1276, current loss: 59688.565076\n",
      "iteration 1277, current loss: 59683.371315\n",
      "iteration 1278, current loss: 59678.182352\n",
      "iteration 1279, current loss: 59672.998431\n",
      "iteration 1280, current loss: 59667.819767\n",
      "iteration 1281, current loss: 59662.646539\n",
      "iteration 1282, current loss: 59657.478882\n",
      "iteration 1283, current loss: 59652.316889\n",
      "iteration 1284, current loss: 59647.160605\n",
      "iteration 1285, current loss: 59642.010033\n",
      "iteration 1286, current loss: 59636.865134\n",
      "iteration 1287, current loss: 59631.725828\n",
      "iteration 1288, current loss: 59626.592002\n",
      "iteration 1289, current loss: 59621.463512\n",
      "iteration 1290, current loss: 59616.340192\n",
      "iteration 1291, current loss: 59611.221853\n",
      "iteration 1292, current loss: 59606.108297\n",
      "iteration 1293, current loss: 59600.999314\n",
      "iteration 1294, current loss: 59595.894693\n",
      "iteration 1295, current loss: 59590.794225\n",
      "iteration 1296, current loss: 59585.697706\n",
      "iteration 1297, current loss: 59580.604943\n",
      "iteration 1298, current loss: 59575.515759\n",
      "iteration 1299, current loss: 59570.429991\n",
      "iteration 1300, current loss: 59565.347499\n",
      "iteration 1301, current loss: 59560.268164\n",
      "iteration 1302, current loss: 59555.191886\n",
      "iteration 1303, current loss: 59550.118593\n",
      "iteration 1304, current loss: 59545.048230\n",
      "iteration 1305, current loss: 59539.980763\n",
      "iteration 1306, current loss: 59534.916174\n",
      "iteration 1307, current loss: 59529.854457\n",
      "iteration 1308, current loss: 59524.795614\n",
      "iteration 1309, current loss: 59519.739653\n",
      "iteration 1310, current loss: 59514.686576\n",
      "iteration 1311, current loss: 59509.636384\n",
      "iteration 1312, current loss: 59504.589067\n",
      "iteration 1313, current loss: 59499.544603\n",
      "iteration 1314, current loss: 59494.502954\n",
      "iteration 1315, current loss: 59489.464070\n",
      "iteration 1316, current loss: 59484.427882\n",
      "iteration 1317, current loss: 59479.394304\n",
      "iteration 1318, current loss: 59474.363236\n",
      "iteration 1319, current loss: 59469.334563\n",
      "iteration 1320, current loss: 59464.308155\n",
      "iteration 1321, current loss: 59459.283871\n",
      "iteration 1322, current loss: 59454.261557\n",
      "iteration 1323, current loss: 59449.241050\n",
      "iteration 1324, current loss: 59444.222179\n",
      "iteration 1325, current loss: 59439.204762\n",
      "iteration 1326, current loss: 59434.188613\n",
      "iteration 1327, current loss: 59429.173538\n",
      "iteration 1328, current loss: 59424.159340\n",
      "iteration 1329, current loss: 59419.145816\n",
      "iteration 1330, current loss: 59414.132758\n",
      "iteration 1331, current loss: 59409.119958\n",
      "iteration 1332, current loss: 59404.107205\n",
      "iteration 1333, current loss: 59399.094287\n",
      "iteration 1334, current loss: 59394.080995\n",
      "iteration 1335, current loss: 59389.067119\n",
      "iteration 1336, current loss: 59384.052455\n",
      "iteration 1337, current loss: 59379.036805\n",
      "iteration 1338, current loss: 59374.019981\n",
      "iteration 1339, current loss: 59369.001801\n",
      "iteration 1340, current loss: 59363.982101\n",
      "iteration 1341, current loss: 59358.960731\n",
      "iteration 1342, current loss: 59353.937559\n",
      "iteration 1343, current loss: 59348.912471\n",
      "iteration 1344, current loss: 59343.885377\n",
      "iteration 1345, current loss: 59338.856207\n",
      "iteration 1346, current loss: 59333.824911\n",
      "iteration 1347, current loss: 59328.791458\n",
      "iteration 1348, current loss: 59323.755832\n",
      "iteration 1349, current loss: 59318.718024\n",
      "iteration 1350, current loss: 59313.678034\n",
      "iteration 1351, current loss: 59308.635853\n",
      "iteration 1352, current loss: 59303.591468\n",
      "iteration 1353, current loss: 59298.544848\n",
      "iteration 1354, current loss: 59293.495948\n",
      "iteration 1355, current loss: 59288.444702\n",
      "iteration 1356, current loss: 59283.391034\n",
      "iteration 1357, current loss: 59278.334867\n",
      "iteration 1358, current loss: 59273.276136\n",
      "iteration 1359, current loss: 59268.214811\n",
      "iteration 1360, current loss: 59263.150924\n",
      "iteration 1361, current loss: 59258.084596\n",
      "iteration 1362, current loss: 59253.016065\n",
      "iteration 1363, current loss: 59247.945710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1364, current loss: 59242.874063\n",
      "iteration 1365, current loss: 59237.801809\n",
      "iteration 1366, current loss: 59232.729771\n",
      "iteration 1367, current loss: 59227.658871\n",
      "iteration 1368, current loss: 59222.590086\n",
      "iteration 1369, current loss: 59217.524386\n",
      "iteration 1370, current loss: 59212.462683\n",
      "iteration 1371, current loss: 59207.405775\n",
      "iteration 1372, current loss: 59202.354315\n",
      "iteration 1373, current loss: 59197.308792\n",
      "iteration 1374, current loss: 59192.269523\n",
      "iteration 1375, current loss: 59187.236667\n",
      "iteration 1376, current loss: 59182.210242\n",
      "iteration 1377, current loss: 59177.190146\n",
      "iteration 1378, current loss: 59172.176185\n",
      "iteration 1379, current loss: 59167.168090\n",
      "iteration 1380, current loss: 59162.165539\n",
      "iteration 1381, current loss: 59157.168178\n",
      "iteration 1382, current loss: 59152.175627\n",
      "iteration 1383, current loss: 59147.187494\n",
      "iteration 1384, current loss: 59142.203385\n",
      "iteration 1385, current loss: 59137.222906\n",
      "iteration 1386, current loss: 59132.245667\n",
      "iteration 1387, current loss: 59127.271286\n",
      "iteration 1388, current loss: 59122.299391\n",
      "iteration 1389, current loss: 59117.329619\n",
      "iteration 1390, current loss: 59112.361616\n",
      "iteration 1391, current loss: 59107.395040\n",
      "iteration 1392, current loss: 59102.429554\n",
      "iteration 1393, current loss: 59097.464833\n",
      "iteration 1394, current loss: 59092.500556\n",
      "iteration 1395, current loss: 59087.536410\n",
      "iteration 1396, current loss: 59082.572084\n",
      "iteration 1397, current loss: 59077.607274\n",
      "iteration 1398, current loss: 59072.641677\n",
      "iteration 1399, current loss: 59067.674991\n",
      "iteration 1400, current loss: 59062.706916\n",
      "iteration 1401, current loss: 59057.737149\n",
      "iteration 1402, current loss: 59052.765390\n",
      "iteration 1403, current loss: 59047.791333\n",
      "iteration 1404, current loss: 59042.814674\n",
      "iteration 1405, current loss: 59037.835106\n",
      "iteration 1406, current loss: 59032.852322\n",
      "iteration 1407, current loss: 59027.866018\n",
      "iteration 1408, current loss: 59022.875893\n",
      "iteration 1409, current loss: 59017.881656\n",
      "iteration 1410, current loss: 59012.883031\n",
      "iteration 1411, current loss: 59007.879764\n",
      "iteration 1412, current loss: 59002.871636\n",
      "iteration 1413, current loss: 58997.858474\n",
      "iteration 1414, current loss: 58992.840169\n",
      "iteration 1415, current loss: 58987.816692\n",
      "iteration 1416, current loss: 58982.788114\n",
      "iteration 1417, current loss: 58977.754627\n",
      "iteration 1418, current loss: 58972.716556\n",
      "iteration 1419, current loss: 58967.674374\n",
      "iteration 1420, current loss: 58962.628694\n",
      "iteration 1421, current loss: 58957.580263\n",
      "iteration 1422, current loss: 58952.529931\n",
      "iteration 1423, current loss: 58947.478611\n",
      "iteration 1424, current loss: 58942.427238\n",
      "iteration 1425, current loss: 58937.376718\n",
      "iteration 1426, current loss: 58932.327891\n",
      "iteration 1427, current loss: 58927.281505\n",
      "iteration 1428, current loss: 58922.238196\n",
      "iteration 1429, current loss: 58917.198495\n",
      "iteration 1430, current loss: 58912.162830\n",
      "iteration 1431, current loss: 58907.131541\n",
      "iteration 1432, current loss: 58902.104898\n",
      "iteration 1433, current loss: 58897.083109\n",
      "iteration 1434, current loss: 58892.066338\n",
      "iteration 1435, current loss: 58887.054705\n",
      "iteration 1436, current loss: 58882.048303\n",
      "iteration 1437, current loss: 58877.047192\n",
      "iteration 1438, current loss: 58872.051411\n",
      "iteration 1439, current loss: 58867.060974\n",
      "iteration 1440, current loss: 58862.075875\n",
      "iteration 1441, current loss: 58857.096094\n",
      "iteration 1442, current loss: 58852.121591\n",
      "iteration 1443, current loss: 58847.152317\n",
      "iteration 1444, current loss: 58842.188211\n",
      "iteration 1445, current loss: 58837.229206\n",
      "iteration 1446, current loss: 58832.275228\n",
      "iteration 1447, current loss: 58827.326200\n",
      "iteration 1448, current loss: 58822.382042\n",
      "iteration 1449, current loss: 58817.442672\n",
      "iteration 1450, current loss: 58812.508007\n",
      "iteration 1451, current loss: 58807.577963\n",
      "iteration 1452, current loss: 58802.652454\n",
      "iteration 1453, current loss: 58797.731391\n",
      "iteration 1454, current loss: 58792.814682\n",
      "iteration 1455, current loss: 58787.902230\n",
      "iteration 1456, current loss: 58782.993933\n",
      "iteration 1457, current loss: 58778.089682\n",
      "iteration 1458, current loss: 58773.189362\n",
      "iteration 1459, current loss: 58768.292848\n",
      "iteration 1460, current loss: 58763.400011\n",
      "iteration 1461, current loss: 58758.510709\n",
      "iteration 1462, current loss: 58753.624796\n",
      "iteration 1463, current loss: 58748.742117\n",
      "iteration 1464, current loss: 58743.862509\n",
      "iteration 1465, current loss: 58738.985804\n",
      "iteration 1466, current loss: 58734.111829\n",
      "iteration 1467, current loss: 58729.240403\n",
      "iteration 1468, current loss: 58724.371344\n",
      "iteration 1469, current loss: 58719.504465\n",
      "iteration 1470, current loss: 58714.639578\n",
      "iteration 1471, current loss: 58709.776489\n",
      "iteration 1472, current loss: 58704.915008\n",
      "iteration 1473, current loss: 58700.054938\n",
      "iteration 1474, current loss: 58695.196086\n",
      "iteration 1475, current loss: 58690.338257\n",
      "iteration 1476, current loss: 58685.481254\n",
      "iteration 1477, current loss: 58680.624882\n",
      "iteration 1478, current loss: 58675.768947\n",
      "iteration 1479, current loss: 58670.913253\n",
      "iteration 1480, current loss: 58666.057606\n",
      "iteration 1481, current loss: 58661.201815\n",
      "iteration 1482, current loss: 58656.345688\n",
      "iteration 1483, current loss: 58651.489039\n",
      "iteration 1484, current loss: 58646.631684\n",
      "iteration 1485, current loss: 58641.773447\n",
      "iteration 1486, current loss: 58636.914158\n",
      "iteration 1487, current loss: 58632.053661\n",
      "iteration 1488, current loss: 58627.191813\n",
      "iteration 1489, current loss: 58622.328490\n",
      "iteration 1490, current loss: 58617.463591\n",
      "iteration 1491, current loss: 58612.597044\n",
      "iteration 1492, current loss: 58607.728810\n",
      "iteration 1493, current loss: 58602.858886\n",
      "iteration 1494, current loss: 58597.987315\n",
      "iteration 1495, current loss: 58593.114183\n",
      "iteration 1496, current loss: 58588.239622\n",
      "iteration 1497, current loss: 58583.363814\n",
      "iteration 1498, current loss: 58578.486979\n",
      "iteration 1499, current loss: 58573.609380\n",
      "iteration 1500, current loss: 58568.731304\n",
      "iteration 1501, current loss: 58563.853060\n",
      "iteration 1502, current loss: 58558.974962\n",
      "iteration 1503, current loss: 58554.097318\n",
      "iteration 1504, current loss: 58549.220422\n",
      "iteration 1505, current loss: 58544.344537\n",
      "iteration 1506, current loss: 58539.469894\n",
      "iteration 1507, current loss: 58534.596682\n",
      "iteration 1508, current loss: 58529.725051\n",
      "iteration 1509, current loss: 58524.855106\n",
      "iteration 1510, current loss: 58519.986914\n",
      "iteration 1511, current loss: 58515.120510\n",
      "iteration 1512, current loss: 58510.255898\n",
      "iteration 1513, current loss: 58505.393061\n",
      "iteration 1514, current loss: 58500.531965\n",
      "iteration 1515, current loss: 58495.672567\n",
      "iteration 1516, current loss: 58490.814823\n",
      "iteration 1517, current loss: 58485.958690\n",
      "iteration 1518, current loss: 58481.104130\n",
      "iteration 1519, current loss: 58476.251119\n",
      "iteration 1520, current loss: 58471.399646\n",
      "iteration 1521, current loss: 58466.549718\n",
      "iteration 1522, current loss: 58461.701362\n",
      "iteration 1523, current loss: 58456.854621\n",
      "iteration 1524, current loss: 58452.009563\n",
      "iteration 1525, current loss: 58447.166272\n",
      "iteration 1526, current loss: 58442.324851\n",
      "iteration 1527, current loss: 58437.485420\n",
      "iteration 1528, current loss: 58432.648112\n",
      "iteration 1529, current loss: 58427.813070\n",
      "iteration 1530, current loss: 58422.980447\n",
      "iteration 1531, current loss: 58418.150394\n",
      "iteration 1532, current loss: 58413.323066\n",
      "iteration 1533, current loss: 58408.498612\n",
      "iteration 1534, current loss: 58403.677173\n",
      "iteration 1535, current loss: 58398.858880\n",
      "iteration 1536, current loss: 58394.043850\n",
      "iteration 1537, current loss: 58389.232185\n",
      "iteration 1538, current loss: 58384.423966\n",
      "iteration 1539, current loss: 58379.619260\n",
      "iteration 1540, current loss: 58374.818111\n",
      "iteration 1541, current loss: 58370.020544\n",
      "iteration 1542, current loss: 58365.226564\n",
      "iteration 1543, current loss: 58360.436158\n",
      "iteration 1544, current loss: 58355.649293\n",
      "iteration 1545, current loss: 58350.865918\n",
      "iteration 1546, current loss: 58346.085967\n",
      "iteration 1547, current loss: 58341.309357\n",
      "iteration 1548, current loss: 58336.535991\n",
      "iteration 1549, current loss: 58331.765760\n",
      "iteration 1550, current loss: 58326.998545\n",
      "iteration 1551, current loss: 58322.234217\n",
      "iteration 1552, current loss: 58317.472637\n",
      "iteration 1553, current loss: 58312.713662\n",
      "iteration 1554, current loss: 58307.957146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1555, current loss: 58303.202939\n",
      "iteration 1556, current loss: 58298.450891\n",
      "iteration 1557, current loss: 58293.700859\n",
      "iteration 1558, current loss: 58288.952701\n",
      "iteration 1559, current loss: 58284.206288\n",
      "iteration 1560, current loss: 58279.461502\n",
      "iteration 1561, current loss: 58274.718241\n",
      "iteration 1562, current loss: 58269.976424\n",
      "iteration 1563, current loss: 58265.235993\n",
      "iteration 1564, current loss: 58260.496917\n",
      "iteration 1565, current loss: 58255.759196\n",
      "iteration 1566, current loss: 58251.022860\n",
      "iteration 1567, current loss: 58246.287972\n",
      "iteration 1568, current loss: 58241.554624\n",
      "iteration 1569, current loss: 58236.822937\n",
      "iteration 1570, current loss: 58232.093054\n",
      "iteration 1571, current loss: 58227.365132\n",
      "iteration 1572, current loss: 58222.639338\n",
      "iteration 1573, current loss: 58217.915837\n",
      "iteration 1574, current loss: 58213.194785\n",
      "iteration 1575, current loss: 58208.476319\n",
      "iteration 1576, current loss: 58203.760553\n",
      "iteration 1577, current loss: 58199.047574\n",
      "iteration 1578, current loss: 58194.337437\n",
      "iteration 1579, current loss: 58189.630166\n",
      "iteration 1580, current loss: 58184.925758\n",
      "iteration 1581, current loss: 58180.224181\n",
      "iteration 1582, current loss: 58175.525385\n",
      "iteration 1583, current loss: 58170.829300\n",
      "iteration 1584, current loss: 58166.135848\n",
      "iteration 1585, current loss: 58161.444942\n",
      "iteration 1586, current loss: 58156.756494\n",
      "iteration 1587, current loss: 58152.070417\n",
      "iteration 1588, current loss: 58147.386629\n",
      "iteration 1589, current loss: 58142.705056\n",
      "iteration 1590, current loss: 58138.025633\n",
      "iteration 1591, current loss: 58133.348307\n",
      "iteration 1592, current loss: 58128.673034\n",
      "iteration 1593, current loss: 58123.999783\n",
      "iteration 1594, current loss: 58119.328533\n",
      "iteration 1595, current loss: 58114.659274\n",
      "iteration 1596, current loss: 58109.992003\n",
      "iteration 1597, current loss: 58105.326726\n",
      "iteration 1598, current loss: 58100.663456\n",
      "iteration 1599, current loss: 58096.002210\n",
      "iteration 1600, current loss: 58091.343007\n",
      "iteration 1601, current loss: 58086.685870\n",
      "iteration 1602, current loss: 58082.030822\n",
      "iteration 1603, current loss: 58077.377885\n",
      "iteration 1604, current loss: 58072.727079\n",
      "iteration 1605, current loss: 58068.078424\n",
      "iteration 1606, current loss: 58063.431935\n",
      "iteration 1607, current loss: 58058.787626\n",
      "iteration 1608, current loss: 58054.145509\n",
      "iteration 1609, current loss: 58049.505589\n",
      "iteration 1610, current loss: 58044.867872\n",
      "iteration 1611, current loss: 58040.232359\n",
      "iteration 1612, current loss: 58035.599051\n",
      "iteration 1613, current loss: 58030.967945\n",
      "iteration 1614, current loss: 58026.339036\n",
      "iteration 1615, current loss: 58021.712319\n",
      "iteration 1616, current loss: 58017.087786\n",
      "iteration 1617, current loss: 58012.465429\n",
      "iteration 1618, current loss: 58007.845238\n",
      "iteration 1619, current loss: 58003.227202\n",
      "iteration 1620, current loss: 57998.611308\n",
      "iteration 1621, current loss: 57993.997545\n",
      "iteration 1622, current loss: 57989.385895\n",
      "iteration 1623, current loss: 57984.776344\n",
      "iteration 1624, current loss: 57980.168871\n",
      "iteration 1625, current loss: 57975.563457\n",
      "iteration 1626, current loss: 57970.960077\n",
      "iteration 1627, current loss: 57966.358704\n",
      "iteration 1628, current loss: 57961.759309\n",
      "iteration 1629, current loss: 57957.161856\n",
      "iteration 1630, current loss: 57952.566307\n",
      "iteration 1631, current loss: 57947.972618\n",
      "iteration 1632, current loss: 57943.380740\n",
      "iteration 1633, current loss: 57938.790618\n",
      "iteration 1634, current loss: 57934.202191\n",
      "iteration 1635, current loss: 57929.615389\n",
      "iteration 1636, current loss: 57925.030139\n",
      "iteration 1637, current loss: 57920.446358\n",
      "iteration 1638, current loss: 57915.863956\n",
      "iteration 1639, current loss: 57911.282837\n",
      "iteration 1640, current loss: 57906.702899\n",
      "iteration 1641, current loss: 57902.124034\n",
      "iteration 1642, current loss: 57897.546131\n",
      "iteration 1643, current loss: 57892.969078\n",
      "iteration 1644, current loss: 57888.392767\n",
      "iteration 1645, current loss: 57883.817094\n",
      "iteration 1646, current loss: 57879.241970\n",
      "iteration 1647, current loss: 57874.667323\n",
      "iteration 1648, current loss: 57870.093106\n",
      "iteration 1649, current loss: 57865.519307\n",
      "iteration 1650, current loss: 57860.945951\n",
      "iteration 1651, current loss: 57856.373111\n",
      "iteration 1652, current loss: 57851.800909\n",
      "iteration 1653, current loss: 57847.229516\n",
      "iteration 1654, current loss: 57842.659145\n",
      "iteration 1655, current loss: 57838.090045\n",
      "iteration 1656, current loss: 57833.522480\n",
      "iteration 1657, current loss: 57828.956717\n",
      "iteration 1658, current loss: 57824.393001\n",
      "iteration 1659, current loss: 57819.831536\n",
      "iteration 1660, current loss: 57815.272475\n",
      "iteration 1661, current loss: 57810.715904\n",
      "iteration 1662, current loss: 57806.161837\n",
      "iteration 1663, current loss: 57801.610225\n",
      "iteration 1664, current loss: 57797.060950\n",
      "iteration 1665, current loss: 57792.513844\n",
      "iteration 1666, current loss: 57787.968687\n",
      "iteration 1667, current loss: 57783.425225\n",
      "iteration 1668, current loss: 57778.883170\n",
      "iteration 1669, current loss: 57774.342213\n",
      "iteration 1670, current loss: 57769.802022\n",
      "iteration 1671, current loss: 57765.262250\n",
      "iteration 1672, current loss: 57760.722534\n",
      "iteration 1673, current loss: 57756.182498\n",
      "iteration 1674, current loss: 57751.641752\n",
      "iteration 1675, current loss: 57747.099894\n",
      "iteration 1676, current loss: 57742.556508\n",
      "iteration 1677, current loss: 57738.011168\n",
      "iteration 1678, current loss: 57733.463441\n",
      "iteration 1679, current loss: 57728.912888\n",
      "iteration 1680, current loss: 57724.359076\n",
      "iteration 1681, current loss: 57719.801588\n",
      "iteration 1682, current loss: 57715.240039\n",
      "iteration 1683, current loss: 57710.674097\n",
      "iteration 1684, current loss: 57706.103514\n",
      "iteration 1685, current loss: 57701.528154\n",
      "iteration 1686, current loss: 57696.948034\n",
      "iteration 1687, current loss: 57692.363353\n",
      "iteration 1688, current loss: 57687.774523\n",
      "iteration 1689, current loss: 57683.182178\n",
      "iteration 1690, current loss: 57678.587165\n",
      "iteration 1691, current loss: 57673.990499\n",
      "iteration 1692, current loss: 57669.393298\n",
      "iteration 1693, current loss: 57664.796698\n",
      "iteration 1694, current loss: 57660.201758\n",
      "iteration 1695, current loss: 57655.609389\n",
      "iteration 1696, current loss: 57651.020296\n",
      "iteration 1697, current loss: 57646.434964\n",
      "iteration 1698, current loss: 57641.853662\n",
      "iteration 1699, current loss: 57637.276473\n",
      "iteration 1700, current loss: 57632.703332\n",
      "iteration 1701, current loss: 57628.134063\n",
      "iteration 1702, current loss: 57623.568417\n",
      "iteration 1703, current loss: 57619.006099\n",
      "iteration 1704, current loss: 57614.446790\n",
      "iteration 1705, current loss: 57609.890164\n",
      "iteration 1706, current loss: 57605.335899\n",
      "iteration 1707, current loss: 57600.783683\n",
      "iteration 1708, current loss: 57596.233222\n",
      "iteration 1709, current loss: 57591.684239\n",
      "iteration 1710, current loss: 57587.136479\n",
      "iteration 1711, current loss: 57582.589708\n",
      "iteration 1712, current loss: 57578.043715\n",
      "iteration 1713, current loss: 57573.498309\n",
      "iteration 1714, current loss: 57568.953325\n",
      "iteration 1715, current loss: 57564.408619\n",
      "iteration 1716, current loss: 57559.864068\n",
      "iteration 1717, current loss: 57555.319577\n",
      "iteration 1718, current loss: 57550.775069\n",
      "iteration 1719, current loss: 57546.230494\n",
      "iteration 1720, current loss: 57541.685823\n",
      "iteration 1721, current loss: 57537.141052\n",
      "iteration 1722, current loss: 57532.596199\n",
      "iteration 1723, current loss: 57528.051304\n",
      "iteration 1724, current loss: 57523.506427\n",
      "iteration 1725, current loss: 57518.961648\n",
      "iteration 1726, current loss: 57514.417064\n",
      "iteration 1727, current loss: 57509.872787\n",
      "iteration 1728, current loss: 57505.328940\n",
      "iteration 1729, current loss: 57500.785656\n",
      "iteration 1730, current loss: 57496.243070\n",
      "iteration 1731, current loss: 57491.701323\n",
      "iteration 1732, current loss: 57487.160550\n",
      "iteration 1733, current loss: 57482.620885\n",
      "iteration 1734, current loss: 57478.082450\n",
      "iteration 1735, current loss: 57473.545360\n",
      "iteration 1736, current loss: 57469.009716\n",
      "iteration 1737, current loss: 57464.475604\n",
      "iteration 1738, current loss: 57459.943096\n",
      "iteration 1739, current loss: 57455.412248\n",
      "iteration 1740, current loss: 57450.883101\n",
      "iteration 1741, current loss: 57446.355679\n",
      "iteration 1742, current loss: 57441.829994\n",
      "iteration 1743, current loss: 57437.306041\n",
      "iteration 1744, current loss: 57432.783805\n",
      "iteration 1745, current loss: 57428.263260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1746, current loss: 57423.744368\n",
      "iteration 1747, current loss: 57419.227084\n",
      "iteration 1748, current loss: 57414.711354\n",
      "iteration 1749, current loss: 57410.197120\n",
      "iteration 1750, current loss: 57405.684317\n",
      "iteration 1751, current loss: 57401.172876\n",
      "iteration 1752, current loss: 57396.662726\n",
      "iteration 1753, current loss: 57392.153792\n",
      "iteration 1754, current loss: 57387.645999\n",
      "iteration 1755, current loss: 57383.139270\n",
      "iteration 1756, current loss: 57378.633528\n",
      "iteration 1757, current loss: 57374.128695\n",
      "iteration 1758, current loss: 57369.624696\n",
      "iteration 1759, current loss: 57365.121453\n",
      "iteration 1760, current loss: 57360.618894\n",
      "iteration 1761, current loss: 57356.116947\n",
      "iteration 1762, current loss: 57351.615541\n",
      "iteration 1763, current loss: 57347.114609\n",
      "iteration 1764, current loss: 57342.614090\n",
      "iteration 1765, current loss: 57338.113924\n",
      "iteration 1766, current loss: 57333.614058\n",
      "iteration 1767, current loss: 57329.114443\n",
      "iteration 1768, current loss: 57324.615039\n",
      "iteration 1769, current loss: 57320.115812\n",
      "iteration 1770, current loss: 57315.616738\n",
      "iteration 1771, current loss: 57311.117801\n",
      "iteration 1772, current loss: 57306.618997\n",
      "iteration 1773, current loss: 57302.120331\n",
      "iteration 1774, current loss: 57297.621822\n",
      "iteration 1775, current loss: 57293.123496\n",
      "iteration 1776, current loss: 57288.625392\n",
      "iteration 1777, current loss: 57284.127558\n",
      "iteration 1778, current loss: 57279.630050\n",
      "iteration 1779, current loss: 57275.132926\n",
      "iteration 1780, current loss: 57270.636249\n",
      "iteration 1781, current loss: 57266.140081\n",
      "iteration 1782, current loss: 57261.644480\n",
      "iteration 1783, current loss: 57257.149494\n",
      "iteration 1784, current loss: 57252.655164\n",
      "iteration 1785, current loss: 57248.161515\n",
      "iteration 1786, current loss: 57243.668556\n",
      "iteration 1787, current loss: 57239.176281\n",
      "iteration 1788, current loss: 57234.684663\n",
      "iteration 1789, current loss: 57230.193660\n",
      "iteration 1790, current loss: 57225.703209\n",
      "iteration 1791, current loss: 57221.213232\n",
      "iteration 1792, current loss: 57216.723636\n",
      "iteration 1793, current loss: 57212.234314\n",
      "iteration 1794, current loss: 57207.745147\n",
      "iteration 1795, current loss: 57203.256007\n",
      "iteration 1796, current loss: 57198.766759\n",
      "iteration 1797, current loss: 57194.277262\n",
      "iteration 1798, current loss: 57189.787373\n",
      "iteration 1799, current loss: 57185.296947\n",
      "iteration 1800, current loss: 57180.805837\n",
      "iteration 1801, current loss: 57176.313899\n",
      "iteration 1802, current loss: 57171.820992\n",
      "iteration 1803, current loss: 57167.326978\n",
      "iteration 1804, current loss: 57162.831723\n",
      "iteration 1805, current loss: 57158.335099\n",
      "iteration 1806, current loss: 57153.836983\n",
      "iteration 1807, current loss: 57149.337260\n",
      "iteration 1808, current loss: 57144.835820\n",
      "iteration 1809, current loss: 57140.332560\n",
      "iteration 1810, current loss: 57135.827382\n",
      "iteration 1811, current loss: 57131.320197\n",
      "iteration 1812, current loss: 57126.810921\n",
      "iteration 1813, current loss: 57122.299478\n",
      "iteration 1814, current loss: 57117.785797\n",
      "iteration 1815, current loss: 57113.269812\n",
      "iteration 1816, current loss: 57108.751465\n",
      "iteration 1817, current loss: 57104.230703\n",
      "iteration 1818, current loss: 57099.707481\n",
      "iteration 1819, current loss: 57095.181758\n",
      "iteration 1820, current loss: 57090.653502\n",
      "iteration 1821, current loss: 57086.122688\n",
      "iteration 1822, current loss: 57081.589298\n",
      "iteration 1823, current loss: 57077.053323\n",
      "iteration 1824, current loss: 57072.514766\n",
      "iteration 1825, current loss: 57067.973635\n",
      "iteration 1826, current loss: 57063.429953\n",
      "iteration 1827, current loss: 57058.883752\n",
      "iteration 1828, current loss: 57054.335074\n",
      "iteration 1829, current loss: 57049.783973\n",
      "iteration 1830, current loss: 57045.230514\n",
      "iteration 1831, current loss: 57040.674770\n",
      "iteration 1832, current loss: 57036.116823\n",
      "iteration 1833, current loss: 57031.556764\n",
      "iteration 1834, current loss: 57026.994687\n",
      "iteration 1835, current loss: 57022.430693\n",
      "iteration 1836, current loss: 57017.864886\n",
      "iteration 1837, current loss: 57013.297371\n",
      "iteration 1838, current loss: 57008.728257\n",
      "iteration 1839, current loss: 57004.157654\n",
      "iteration 1840, current loss: 56999.585679\n",
      "iteration 1841, current loss: 56995.012451\n",
      "iteration 1842, current loss: 56990.438100\n",
      "iteration 1843, current loss: 56985.862769\n",
      "iteration 1844, current loss: 56981.286617\n",
      "iteration 1845, current loss: 56976.709826\n",
      "iteration 1846, current loss: 56972.132603\n",
      "iteration 1847, current loss: 56967.555186\n",
      "iteration 1848, current loss: 56962.977843\n",
      "iteration 1849, current loss: 56958.400874\n",
      "iteration 1850, current loss: 56953.824610\n",
      "iteration 1851, current loss: 56949.249404\n",
      "iteration 1852, current loss: 56944.675628\n",
      "iteration 1853, current loss: 56940.103659\n",
      "iteration 1854, current loss: 56935.533873\n",
      "iteration 1855, current loss: 56930.966627\n",
      "iteration 1856, current loss: 56926.402253\n",
      "iteration 1857, current loss: 56921.841044\n",
      "iteration 1858, current loss: 56917.283249\n",
      "iteration 1859, current loss: 56912.729065\n",
      "iteration 1860, current loss: 56908.178639\n",
      "iteration 1861, current loss: 56903.632064\n",
      "iteration 1862, current loss: 56899.089390\n",
      "iteration 1863, current loss: 56894.550623\n",
      "iteration 1864, current loss: 56890.015738\n",
      "iteration 1865, current loss: 56885.484688\n",
      "iteration 1866, current loss: 56880.957412\n",
      "iteration 1867, current loss: 56876.433851\n",
      "iteration 1868, current loss: 56871.913956\n",
      "iteration 1869, current loss: 56867.397697\n",
      "iteration 1870, current loss: 56862.885078\n",
      "iteration 1871, current loss: 56858.376142\n",
      "iteration 1872, current loss: 56853.870974\n",
      "iteration 1873, current loss: 56849.369707\n",
      "iteration 1874, current loss: 56844.872517\n",
      "iteration 1875, current loss: 56840.379618\n",
      "iteration 1876, current loss: 56835.891251\n",
      "iteration 1877, current loss: 56831.407672\n",
      "iteration 1878, current loss: 56826.929134\n",
      "iteration 1879, current loss: 56822.455876\n",
      "iteration 1880, current loss: 56817.988102\n",
      "iteration 1881, current loss: 56813.525975\n",
      "iteration 1882, current loss: 56809.069609\n",
      "iteration 1883, current loss: 56804.619061\n",
      "iteration 1884, current loss: 56800.174338\n",
      "iteration 1885, current loss: 56795.735395\n",
      "iteration 1886, current loss: 56791.302142\n",
      "iteration 1887, current loss: 56786.874452\n",
      "iteration 1888, current loss: 56782.452170\n",
      "iteration 1889, current loss: 56778.035116\n",
      "iteration 1890, current loss: 56773.623095\n",
      "iteration 1891, current loss: 56769.215901\n",
      "iteration 1892, current loss: 56764.813324\n",
      "iteration 1893, current loss: 56760.415149\n",
      "iteration 1894, current loss: 56756.021165\n",
      "iteration 1895, current loss: 56751.631164\n",
      "iteration 1896, current loss: 56747.244942\n",
      "iteration 1897, current loss: 56742.862303\n",
      "iteration 1898, current loss: 56738.483055\n",
      "iteration 1899, current loss: 56734.107019\n",
      "iteration 1900, current loss: 56729.734020\n",
      "iteration 1901, current loss: 56725.363893\n",
      "iteration 1902, current loss: 56720.996481\n",
      "iteration 1903, current loss: 56716.631635\n",
      "iteration 1904, current loss: 56712.269217\n",
      "iteration 1905, current loss: 56707.909094\n",
      "iteration 1906, current loss: 56703.551144\n",
      "iteration 1907, current loss: 56699.195250\n",
      "iteration 1908, current loss: 56694.841307\n",
      "iteration 1909, current loss: 56690.489215\n",
      "iteration 1910, current loss: 56686.138882\n",
      "iteration 1911, current loss: 56681.790224\n",
      "iteration 1912, current loss: 56677.443165\n",
      "iteration 1913, current loss: 56673.097634\n",
      "iteration 1914, current loss: 56668.753568\n",
      "iteration 1915, current loss: 56664.410912\n",
      "iteration 1916, current loss: 56660.069614\n",
      "iteration 1917, current loss: 56655.729631\n",
      "iteration 1918, current loss: 56651.390924\n",
      "iteration 1919, current loss: 56647.053462\n",
      "iteration 1920, current loss: 56642.717215\n",
      "iteration 1921, current loss: 56638.382160\n",
      "iteration 1922, current loss: 56634.048279\n",
      "iteration 1923, current loss: 56629.715557\n",
      "iteration 1924, current loss: 56625.383983\n",
      "iteration 1925, current loss: 56621.053547\n",
      "iteration 1926, current loss: 56616.724244\n",
      "iteration 1927, current loss: 56612.396071\n",
      "iteration 1928, current loss: 56608.069026\n",
      "iteration 1929, current loss: 56603.743108\n",
      "iteration 1930, current loss: 56599.418318\n",
      "iteration 1931, current loss: 56595.094657\n",
      "iteration 1932, current loss: 56590.772126\n",
      "iteration 1933, current loss: 56586.450725\n",
      "iteration 1934, current loss: 56582.130456\n",
      "iteration 1935, current loss: 56577.811317\n",
      "iteration 1936, current loss: 56573.493307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1937, current loss: 56569.176422\n",
      "iteration 1938, current loss: 56564.860657\n",
      "iteration 1939, current loss: 56560.546005\n",
      "iteration 1940, current loss: 56556.232457\n",
      "iteration 1941, current loss: 56551.920001\n",
      "iteration 1942, current loss: 56547.608624\n",
      "iteration 1943, current loss: 56543.298308\n",
      "iteration 1944, current loss: 56538.989035\n",
      "iteration 1945, current loss: 56534.680783\n",
      "iteration 1946, current loss: 56530.373528\n",
      "iteration 1947, current loss: 56526.067243\n",
      "iteration 1948, current loss: 56521.761898\n",
      "iteration 1949, current loss: 56517.457461\n",
      "iteration 1950, current loss: 56513.153897\n",
      "iteration 1951, current loss: 56508.851170\n",
      "iteration 1952, current loss: 56504.549241\n",
      "iteration 1953, current loss: 56500.248067\n",
      "iteration 1954, current loss: 56495.947607\n",
      "iteration 1955, current loss: 56491.647816\n",
      "iteration 1956, current loss: 56487.348648\n",
      "iteration 1957, current loss: 56483.050057\n",
      "iteration 1958, current loss: 56478.751996\n",
      "iteration 1959, current loss: 56474.454419\n",
      "iteration 1960, current loss: 56470.157280\n",
      "iteration 1961, current loss: 56465.860534\n",
      "iteration 1962, current loss: 56461.564139\n",
      "iteration 1963, current loss: 56457.268056\n",
      "iteration 1964, current loss: 56452.972248\n",
      "iteration 1965, current loss: 56448.676686\n",
      "iteration 1966, current loss: 56444.381344\n",
      "iteration 1967, current loss: 56440.086204\n",
      "iteration 1968, current loss: 56435.791257\n",
      "iteration 1969, current loss: 56431.496501\n",
      "iteration 1970, current loss: 56427.201947\n",
      "iteration 1971, current loss: 56422.907616\n",
      "iteration 1972, current loss: 56418.613540\n",
      "iteration 1973, current loss: 56414.319766\n",
      "iteration 1974, current loss: 56410.026353\n",
      "iteration 1975, current loss: 56405.733375\n",
      "iteration 1976, current loss: 56401.440917\n",
      "iteration 1977, current loss: 56397.149079\n",
      "iteration 1978, current loss: 56392.857971\n",
      "iteration 1979, current loss: 56388.567714\n",
      "iteration 1980, current loss: 56384.278440\n",
      "iteration 1981, current loss: 56379.990284\n",
      "iteration 1982, current loss: 56375.703388\n",
      "iteration 1983, current loss: 56371.417897\n",
      "iteration 1984, current loss: 56367.133952\n",
      "iteration 1985, current loss: 56362.851696\n",
      "iteration 1986, current loss: 56358.571264\n",
      "iteration 1987, current loss: 56354.292786\n",
      "iteration 1988, current loss: 56350.016384\n",
      "iteration 1989, current loss: 56345.742170\n",
      "iteration 1990, current loss: 56341.470244\n",
      "iteration 1991, current loss: 56337.200697\n",
      "iteration 1992, current loss: 56332.933605\n",
      "iteration 1993, current loss: 56328.669035\n",
      "iteration 1994, current loss: 56324.407039\n",
      "iteration 1995, current loss: 56320.147658\n",
      "iteration 1996, current loss: 56315.890921\n",
      "iteration 1997, current loss: 56311.636843\n",
      "iteration 1998, current loss: 56307.385431\n",
      "iteration 1999, current loss: 56303.136678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22aed8976d8>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3Xd4VWW6/vHvk4TQQweBAAFEERUpoYqgMiKgDhZGBQsiihXb+DujxzMz50zXsaKIFAWxjCJjYcahjQIiEiDSO6EIoUa6dMjz+2MvZjImwA4kWSn357r2xc6bd+08a+2w76z2vubuiIiIZBUTdgEiIlL4KBxERCQbhYOIiGSjcBARkWwUDiIiko3CQUREslE4iIhINgoHERHJRuEgIiLZxIVdwJmqXr26JyUlhV2GiEiR8u23337v7jVO16/IhkNSUhKpqalhlyEiUqSY2XfR9NNhJRERyUbhICIi2SgcREQkG4WDiIhko3AQEZFsFA4iIpKNwkFERLIpceHwwZwNTF25PewyREQKtRIVDkeOZTJm1nc89N48FqfvCbscEZFCq0SFQ3xcDKP6t6FKuXj6j57Lxp0Hwi5JRKRQKlHhAFAroQxv392GI8eO02/UHHbtPxJ2SSIihU6JCweAc2tWZGS/NqTvPMg9Y1I5dPR42CWJiBQqJTIcANo2rMpLt7Rg3oZdPPrBfI5netgliYgUGiU2HACuaV6b/7mmGZOWbuM3f1uKuwJCRASK8JDdeWVAp4Zs2X2QkV+vo1qF0jzStUnYJYmIhK7EhwPAf/e8gF0HjvLilFVUKVeKOzokhV2SiEioFA5ATIzx7E0Xs+fgUX41fikJZUvRq0XdsMsSEQlNiT7nkFVcbAyv9W1J26Sq/HzsQqau0F3UIlJyKRyyKFMqlpH9kmlauyIPvPctqet3hl2SiEgoogoHM6tsZuPMbIWZLTezDmbWwsxSzGyBmaWaWdugr5nZYDNLM7NFZtYqy+v0M7PVwaNflvbWZrY4WGawmVner2p0KpYpxej+balTqSx3j57L8i17wypFRCQ00e45vAJMdPemwCXAcuA54P/cvQXwq+BrgB5Ak+AxEBgKYGZVgV8D7YC2wK/NrEqwzNCg74nlup/dap2d6hVK88497ShfOo473pzDuu/3h1mOiEiBO204mFkC0Bl4E8Ddj7j7bsCBhKBbJWBz8LwXMMYjUoDKZlYbuBqY4u473X0XMAXoHnwvwd1neeRGgzHA9Xm3imembuWyvDOgLe5O3xEpbNihcZhEpOSIZs+hEZABjDKz+WY20szKA48BfzazjcDzwNNB/7rAxizLpwdtp2pPz6E9dOfWrMi797Tj4NHj9BmRQvouBYSIlAzRhEMc0AoY6u4tgf3AU8ADwOPuXg94nGDPAsjpfIGfQXs2ZjYwOL+RmpGREUXpZ++C2gm8O6Ad+w4dpc+IFDbvPlggP1dEJEzRhEM6kO7us4OvxxEJi37Ax0HbR0TOI5zoXy/L8olEDjmdqj0xh/Zs3H24uye7e3KNGjWiKD1vXFS3Eu8MaMfu/UfpOyKFbXsPFdjPFhEJw2nDwd23AhvN7PygqSuwjMgHeJeg7UpgdfB8PHBncNVSe2CPu28BJgHdzKxKcCK6GzAp+N4+M2sfXKV0J/BZHq1fnrmkXmVG392WjH2H6TMihe37FBAiUnxFe7XSIOA9M1sEtAD+ANwLvGBmC4OvBwZ9/wGsBdKAEcCDAO6+E/gtMDd4/CZog8ghqpHBMmuACWe3WvmjdYMqjL67LVt2H+K2EbPZ8cPhsEsSEckXVlRHIk1OTvbU1NRQfvasNTvoP3oODaqW571721G9QulQ6hARyS0z+9bdk0/XT3dIn4EOjavxVr82fLdzP7cOT2G7zkGISDGjcDhDHc+tzuj+bdm8+yC3DE9hyx5dxSQixYfC4Sy0b1SNMcFJ6luG6T4IESk+FA5nKTmpKu8MaMuuA0e4ZVgKG3cqIESk6FM45IGW9avw/j3t+eHwMW4eNov1GotJRIo4hUMeuTixEn+5tz2Hj2Vy87BZpG3/IeySRETOmMIhDzWrk8Bf7m1Ppju3Dk9h5dZ9YZckInJGFA557PxzKvLBwA7ExsAtw2excOPusEsSEck1hUM+OLdmBT66ryMVy8Rx28jZzF67I+ySRERyReGQT+pXK8dH93WkVkJp7nxrDtNWak5qESk6FA756JxKZRh7Xwca16jAvWNSmbB4S9gliYhEReGQz6pVKM1fBraneWJlHnp/HuO+TT/9QiIiIVM4FIBKZUvxzoC2dGhcjSc/WsiYWevDLklE5JQUDgWkXHwcb/Zrw1XNavGrz5by+rS0sEsSETkphUMBKlMqltdva0WvFnV4buJKnpu4gqI6ZLqIFG9xYRdQ0pSKjeHFm1tQLj6O16etYf/hY/z6uguJiclpKm0RkXAoHEIQG2P84YaLqFA6lhEz1rHv0DGe7d2cUrHakRORwkHhEBIz4797XkBCmVK8MGUVOw8c4fXbWlEuXm+JiIRPf6qGyMwY1LUJf7zxYr5alUGfEbPZuf9I2GWJiCgcCoM+bevzxu2tWbFlL72HfqM5IUQkdAqHQqLbhefw7j3t+P6Hw9w09BuWb9kbdkkiUoIpHAqRNklV+ej+jsSYcfOwWaRowD4RCYnCoZA5/5yK/PXBjtSsGBmwb+ISjcckIgUvqnAws8pmNs7MVpjZcjPrELQPMrOVZrbUzJ7L0v9pM0sLvnd1lvbuQVuamT2Vpb2hmc02s9Vm9qGZxeflShY1dSuXZdz9HbmwTgIPvjePd1K+C7skESlhot1zeAWY6O5NgUuA5WZ2BdALaO7uFwLPA5hZM+BW4EKgO/C6mcWaWSwwBOgBNAP6BH0BngVecvcmwC5gQJ6sXRFWpXw879/TnivOr8kvP13CH/+xnMxM3U0tIgXjtOFgZglAZ+BNAHc/4u67gQeAP7n74aD9xIQFvYAP3P2wu68D0oC2wSPN3de6+xHgA6CXmRlwJTAuWP5t4Pq8WsGirGx8LMPuaM3t7esz7Ku1DPpgPoeOHg+7LBEpAaLZc2gEZACjzGy+mY00s/LAecBlweGg6WbWJuhfF9iYZfn0oO1k7dWA3e5+7Eft2ZjZQDNLNbPUjIyMKFexaIuLjeG3vS7i6R5N+XzRFm4fOZtduhdCRPJZNOEQB7QChrp7S2A/8FTQXgVoD/w/YGywF5DTIEF+Bu3ZG92Hu3uyuyfXqFEjitKLBzPjvi6Nea1vSxZt2sONQ7/hux37wy5LRIqxaMIhHUh399nB1+OIhEU68LFHzAEygepBe70syycCm0/R/j1Q2cziftQuP3Jt8zq8f087dh84wg2vf8O8DbvCLklEiqnThoO7bwU2mtn5QVNXYBnwKZFzBZjZeUA8kQ/68cCtZlbazBoCTYA5wFygSXBlUjyRk9bjPTJm9VSgd/D6/YDP8mj9ip3kpKp8/OClVCwTR5/hKbrUVUTyRbRXKw0C3jOzRUAL4A/AW0AjM1tC5ORyv2AvYikwlkiATAQecvfjwTmFh4FJwHJgbNAX4BfAE2aWRuQcxJt5s3rFU8Pq5fn4gY40q5PAA+/NY+SMtZoXQkTylBXVD5Xk5GRPTU0Nu4xQHTp6nMc/XMCEJVu5q2MSv7y2GbGaF0JETsHMvnX35NP10x3SRViZUrEM6duKezo1ZPQ367nn7bnsO3Q07LJEpBhQOBRxMTHG/1zbjN/fcBEzVn9P76GzNKqriJw1hUMxcVu7Brx9d1u27DnI9UNmkrp+Z9gliUgRpnAoRi49tzqfPHQpCWVL0XfEbD6elx52SSJSRCkcipnGNSrwyYMdad2gCk+MXchzE1dwXGMyiUguKRyKocrl4hkzoC192tbn9WlruOftuew5qBPVIhI9hUMxVSo2hj/ccBG/uz5yovr6ITNZvW1f2GWJSBGhcCjGzIzb2zfgLwPbs+/QMa4fMpOJS7aGXZaIFAEKhxKgTVJV/j6oE01qVeT+d7/lhckrNTeEiJySwqGEOKdSGT68rz03Jyfy6pdp3DMmVechROSkFA4lSOm4WJ69qTm/7XUhX63K0HkIETkphUMJY2bc0SHpX+cheg2ZyWcLNoVdlogUMgqHEqpNUlU+f6QTF9ZJ4NEPFvA/ny7m8DFNQSoiEQqHEqxWQhnev7c993VuxLspG/jZGxqXSUQiFA4lXKnYGJ7ueQHD72jNuu/3c83gGfxz2bawyxKRkCkcBIBuF57D54Muo17VctwzJpU/TVjBseOZYZclIiFROMi/1K9Wjr8+0JG+7erzxvQ19B05m+17D4VdloiEQOEg/6FMqVj+cMPFvHTLJSxO30P3V2YwdcX2sMsSkQKmcJAc3dAykb8NupSaFUvTf/Rcfvv3ZbqaSaQEUTjISZ1bsyKfPnQp/To04M2v13Hj69+wNuOHsMsSkQKgcJBTKlMqlv/rdREj7kxm0+6DXPvq13yUuhF3jc0kUpwpHCQqVzWrxcRHO9M8sRL/b9wiHv1gAfsOaWwmkeIqqnAws8pmNs7MVpjZcjPrkOV7T5qZm1n14Gszs8FmlmZmi8ysVZa+/cxsdfDol6W9tZktDpYZbGaWlyspeeOcSmV47572PNntPD5fvIWeg2cwf8OusMsSkXwQ7Z7DK8BEd28KXAIsBzCzesBVwIYsfXsATYLHQGBo0Lcq8GugHdAW+LWZVQmWGRr0PbFc9zNfJclPsTHGw1c2Yex97cnMhN5vzOLFKas4qnsiRIqV04aDmSUAnYE3Adz9iLvvDr79EvBfQNYD0L2AMR6RAlQ2s9rA1cAUd9/p7ruAKUD34HsJ7j7LIweyxwDX59H6ST5p3aAqEx67jF6X1GHwF6vpPfQb1uhktUixEc2eQyMgAxhlZvPNbKSZlTeznwKb3H3hj/rXBTZm+To9aDtVe3oO7VLIJZQpxYu3tGBI31Z8t/MA1wyewTuz1utktUgxEE04xAGtgKHu3hLYD/wv8Azwqxz653S+wM+gPfsLmw00s1QzS83IyIiidCkI1zSvzaTHOtO2YTV++dlS+o2ayzbdWS1SpEUTDulAurvPDr4eRyQsGgILzWw9kAjMM7Nzgv71siyfCGw+TXtiDu3ZuPtwd0929+QaNWpEUboUlFoJZXi7fxt+2+tC5qzbwdUvf8U/Fm8JuywROUOnDQd33wpsNLPzg6auwDx3r+nuSe6eROQDvlXQdzxwZ3DVUntgj7tvASYB3cysSnAiuhswKfjePjNrH1yldCfwWV6vqOS/ExMJff7IZTSoWo4H35vH4x8uYM8BXfIqUtTERdlvEPCemcUDa4H+p+j7D6AnkAYcONHX3Xea2W+BuUG/37j7zuD5A8BooCwwIXhIEdW4RgXGPdCR175M47WpaXyz5nuevak5l59fM+zSRCRKVlRPHiYnJ3tqamrYZchpLErfzc/HLmT19h+4tU09nrnmAiqWKRV2WSIllpl96+7Jp+unO6QlXzVPrMzfBnXi/i6NGZu6ke4vz+CbtO/DLktETkPhIPmuTKlYnurRlI/u70jpuBj6jpzNrz5bwoEjx8IuTUROQuEgBaZ1gyp8/shl3H1pQ95J+Y4er8xg7vqdp19QRAqcwkEKVNn4WH51XTM+uLc9me7cPGwWv/v7Mg4d1VwRIoWJwkFC0a5RNSY+2pnb2tVn5NfrNIifSCGjcJDQlC8dx++uv5h3B7Tj0JHj3DT0G56buEIzzokUAgoHCV2nJtWZ+HhnerdO5PVpa+j12kyWbNoTdlkiJZrCQQqFhDKleK73Jbx1VzI79x/h+iEzefmfGgpcJCwKBylUrmxai8mPd+a6S+rw8j9Xc8PrM1m5dV/YZYmUOAoHKXQql4vnpVta8Mbtrdmy+xDXvfo1Q6amcUx7ESIFRuEghVb3i85h8uOd+Umzmvx50kp6vzGLtO2aUEikICgcpFCrVqE0Q/q2YnCflqzfsZ9rBs9g5Iy1HM8smmOCiRQVCgcp9MyMn15Sh8mPd+ayJtX53efLuXX4LL7bsT/s0kSKLYWDFBk1K5ZhxJ3JPP+zS1ixdR/dX45MS5qpvQiRPKdwkCLFzOjdOpHJj3emTcOq/PKzpdzx1mw27jwQdmkixYrCQYqk2pXK8nb/NvzxxotZsGE3V7/8Fe+kfKe9CJE8onCQIsvM6NO2PpMe70zrBlX45adL6DsyhQ07tBchcrYUDlLkJVYpx5i72/LsTRezdNNern75K0bPXKe9CJGzoHCQYsHMuKVNfSY/0Zl2jaryv39bxq3DU1j3va5oEjkTCgcpVmpXKsuou9rw/M8uYfnWvfR45SvdFyFyBhQOUuycuKLpn0904dLGkfsifvbGN6zJ0N3VItFSOEixVSuhDCP7JfPSLZewJmM/PV6ZwbDpa7QXIRKFqMLBzCqb2TgzW2Fmy82sg5n9Ofh6kZl9YmaVs/R/2szSzGylmV2dpb170JZmZk9laW9oZrPNbLWZfWhm8Xm7mlJSmRk3tExkyhOdufy8GvxxwgpuGvoNq7dppFeRU4l2z+EVYKK7NwUuAZYDU4CL3L05sAp4GsDMmgG3AhcC3YHXzSzWzGKBIUAPoBnQJ+gL8Czwkrs3AXYBA/Ji5UROqFmxDMPuaM3gPi35bsd+rhmskV5FTuW04WBmCUBn4E0Adz/i7rvdfbK7Hwu6pQCJwfNewAfuftjd1wFpQNvgkebua939CPAB0MvMDLgSGBcs/zZwfd6snsi//XuMpi7/Gun1xqHfaL4IkRxEs+fQCMgARpnZfDMbaWblf9TnbmBC8LwusDHL99KDtpO1VwN2ZwmaE+0i+aJGxdK8fltrhvRtxaZdB7n21RkM/mK1Zp0TySKacIgDWgFD3b0lsB/Ier7gGeAY8N6Jphxew8+gPRszG2hmqWaWmpGREUXpIid3TfPaTH68M90vqs2LU1Zx3atfM2/DrrDLEikUogmHdCDd3WcHX48jEhaYWT/gWuA2d/cs/etlWT4R2HyK9u+BymYW96P2bNx9uLsnu3tyjRo1oihd5NSqVSjNq31aMuLOZPYcPMpNQ7/hl58uYe+ho2GXJhKq04aDu28FNprZ+UFTV2CZmXUHfgH81N2zDmYzHrjVzEqbWUOgCTAHmAs0Ca5Miidy0np8ECpTgd7B8v2Az/Jg3USidlWzWkx5ogv9OiTx7uzvuOrF6UxYvIV//80jUrJYNL/8ZtYCGAnEA2uB/kQ+7EsDO4JuKe5+f9D/GSLnIY4Bj7n7hKC9J/AyEAu85e6/D9obETlBXRWYD9zu7odPVVNycrKnpqbmamVForFw426e/ngxy7bs5ScX1OI3vS6kTuWyYZclkifM7Ft3Tz5tv6L6l5HCQfLTseOZvDVzHS9NWU2Mwc+7nU+/jknExuR0ikyk6Ig2HHSHtEgO4mJjGNi58b8mFfrN35dxw+szWbJpT9iliRQIhYPIKdSrWo5Rd7Xh1T4t2bz7EL2GzOQP/1jOgSPHTr+wSBGmcBA5DTPjukvq8MUTXbg5OZHhX63lqhe/YurK7WGXJpJvFA4iUapUrhR/vLE5Y+/rQNn4WPqPmsvD789j+95DYZcmkucUDiK51LZhVT5/pBNPXHUek5dt48oXpjNq5jqN0yTFisJB5AyUjovlka5NmPxYZ1o1qML//W0ZP31tpu6wlmJD4SByFpKql+ft/m14/bZW7Nx/hJuGfsPTHy9m94EjYZcmclYUDiJnyczoeXFt/vnzLtzTqSFjUzdy5QvTGZu6kUxNLCRFlMJBJI9UKB3HM9c04/NHOtGoenn+a9wibh42ixVb94ZdmkiuKRxE8ljTcxIYe18HnuvdnDUZP3DN4K/5/efL+OGw7o2QokPhIJIPYmKMm5Pr8eXPL+fm5ERGzFjHT16Yzt8WbtZgflIkKBxE8lGV8vH88cbmfPxgR6pViGfQX+Zzy7AUDcMhhZ7CQaQAtKpfhfEPd+KPN15MWsYPXPfa1zz98SK+/+GUgw+LhEbhIFJAYmOMPm3rM/XJy7n70oZ8lJrOFX+exsgZazlyTDfQSeGicBApYJXKluKX1zZjYnAD3e8+X073VzRWkxQuCgeRkJxbswJv392WUXe1AYf+o+bSf9Qc1mb8EHZpIgoHkbBd0bQmEx/rzDM9LyB1/S66vfQVv/98meaxllApHEQKgfi4GO7t3Igvn7yc3q0TGfn1Oq58fhrvzf5OA/pJKBQOIoVIjYql+dNNzfnbw51oVL0Cz3yyhB6vzGDqiu26P0IKlMJBpBC6qG4lPryvPW/c3pqjxzPpP3oud7w5h6WbdX+EFAyFg0ghZWZ0v+gcJj/ehV9f14wlm/dw7atf8+RHC9m6RxMMSf6yorqrmpyc7KmpqWGXIVJg9hw8ypCpaYyeuZ6YGBh4WSPu69KY8qXjwi5NihAz+9bdk0/XL6o9BzOrbGbjzGyFmS03sw5mVtXMppjZ6uDfKkFfM7PBZpZmZovMrFWW1+kX9F9tZv2ytLc2s8XBMoPNzM5kpUWKs0plS/HfPS/gi5934ScX1GLwl2l0+fM0/jJnA8c1NLjksWgPK70CTHT3psAlwHLgKeALd28CfBF8DdADaBI8BgJDAcysKvBroB3QFvj1iUAJ+gzMslz3s1stkeKrXtVyvNa3FR8/2JEG1crx9MeL6fnKDKbpJjrJQ6cNBzNLADoDbwK4+xF33w30At4Our0NXB887wWM8YgUoLKZ1QauBqa4+0533wVMAboH30tw91keOcY1JstrichJtKpfhXH3d2Doba04dOw4d42ayx1vzmb5Fs0fIWcvmj2HRkAGMMrM5pvZSDMrD9Ry9y0Awb81g/51gY1Zlk8P2k7Vnp5Du4ichpnR4+LaTHm8C7+8thmL0vfQc/AM/mvcQrbt1UlrOXPRhEMc0AoY6u4tgf38+xBSTnI6X+Bn0J79hc0GmlmqmaVmZGScumqREiQ+LoYBnRoy/f9FBvX7ZP4mLv/zNF6YvJJ9utNazkA04ZAOpLv77ODrcUTCYltwSIjg3+1Z+tfLsnwisPk07Yk5tGfj7sPdPdndk2vUqBFF6SIlS+Vy8fzy2mb884kudL2gJq8GJ61HzVynkV8lV04bDu6+FdhoZucHTV2BZcB44MQVR/2Az4Ln44E7g6uW2gN7gsNOk4BuZlYlOBHdDZgUfG+fmbUPrlK6M8tricgZaFCtPK/1bcX4hy+l6TkV+b+/LaPri9P4bMEmMnVlk0QhqvsczKwFMBKIB9YC/YkEy1igPrAB+Jm77ww+4F8jcsXRAaC/u6cGr3M38N/By/7e3UcF7cnAaKAsMAEY5KcpTPc5iETH3flq9ff8acIKlm/Zy0V1E3iq+wV0alI97NIkBNHe56Cb4ERKiMxM57OFm3h+0io27T7IZU2q84vuTbmobqWwS5MClKc3wYlI0RcTY9zQMpEvn+zC/1xzAYs3RYbjGPSX+ZpDQrLRnoNICbX30FGGTV/DqJnrOXwsk5ta1eWRrk1IrFIu7NIkH+mwkohE5fsfDvP61DW8O/s73J0+bevz8BXnUjOhTNilST5QOIhIrmzZc5DBX6TxUepG4mKNfh2SuL9LY6qUjw+7NMlDCgcROSPf7djPK/9czScLNlE+Po67OzXknssaklCmVNilSR5QOIjIWVm9bR8vTlnFhCVbqVyuFPd1bky/jg0oF68hwosyhYOI5Iklm/bwwuSVTF2ZQfUKpXn4isb0aVef0nGxYZcmZ0DhICJ5KnX9Tp6fvJKUtTupU6kMg7o2oXfrRErF6or4okThICJ5zt35Zs0O/jxpJQs27qZe1bIMurIJN7asS5xCokhQOIhIvnF3pq3K4KUpq1iUvoekauV4pGsTerWoS2yMJnIszHSHtIjkGzPjivNr8tlDlzLizmTKxsfxxNiFXPXSdD5bsEnTlhYDCgcROWNmxlXNavH5oE68cXsrSsXE8OgHC+j+8ld8vmiLRoAtwhQOInLWYmKM7hfVZsKjl/Fa35Y48ND78+g5eAaTlm6lqB6+LskUDiKSZ2JijGub12HSY5155dYWHD6WyX3vfMu1r37NF8u3KSSKEJ2QFpF8c+x4Jp8u2MzgL1azYecBLkmsxONXnUeX82oQmfpFCpquVhKRQuPo8Uw+npfO4C/S2LT7IK3qV+aJq87n0nOrKSQKmMJBRAqdI8cy+ejbjbz2ZRpb9hyibVJVHruqCR0aKSQKisJBRAqtw8eO8+HcSEhs33eYNklVeKRrEzqdW10hkc8UDiJS6B06GgmJodPWsHXvIVrWr8wjXZtwuc5J5BuFg4gUGYePHeej1HSGTlvDpt0HaZ5YiUeubELXC2oqJPKYwkFEipwjxyInrodMS2PjzoNcWCeBQVc2oVuzWsRoWI48oXAQkSLr6PFMPp2/iSFT01i/4wBNz6nIoCub0OOicxQSZ0nhICJF3rHjmfx90RZe/XI1azL206RmBR6+8lyubV5HA/ydoTwdeM/M1pvZYjNbYGapQVsLM0s50WZmbYN2M7PBZpZmZovMrFWW1+lnZquDR78s7a2D108LltW7LiLExcZwfcu6TH68C6/2aYkZPPrBAq56cTofpW7kyLHMsEsstqLaczCz9UCyu3+fpW0y8JK7TzCznsB/ufvlwfNBQE+gHfCKu7czs6pAKpAMOPAt0Nrdd5nZHOBRIAX4BzDY3SecqibtOYiUPJmZzqSlWxn8ZRrLt+ylTqUy3HNZI25tW0/Tl0apIIbsdiAheF4J2Bw87wWM8YgUoLKZ1QauBqa4+0533wVMAboH30tw91keSaoxwPVnUZeIFFMxMUaPi2vzj0c6MequNtStUpbf/H0ZnZ6dyuAvVrPnwNGwSyw2oo1aByabmQPD3H048BgwycyeJxIyHYO+dYGNWZZND9pO1Z6eQ3s2ZjYQGAhQv379KEsXkeLGzLiiaU2uaFqTuet3MnTaGl6csoph09dwW/sGDOjUkFoJZcIus0iLNhwudffNZlYTmGJmK4DewOPu/lczuxl4E/gJkNP5Aj+D9uyNkVAaDpHDSlHWLiLFWJukqrS5qyrLt+xl6LQ1jJyxltEz13NT67rc17kxSdXLh11ikRTVYSV33xz8ux2OH0g/AAALU0lEQVT4BGgL9AM+Drp8FLRB5C//elkWTyRyyOlU7Yk5tIuIRO2C2gkM7tOSqU9eTu/kRP767SaufGEaD78/j6Wb94RdXpFz2nAws/JmVvHEc6AbsITIB3iXoNuVwOrg+XjgzuCqpfbAHnffAkwCuplZFTOrErzOpOB7+8ysfXCV0p3AZ3m3iiJSkjSoVp4/3HAxX//iCu7t3IhpKzO4ZvDX3DVqDnPW7Qy7vCIjmsNKtYBPgqtL44D33X2imf0AvGJmccAhgnMBRK426gmkAQeA/gDuvtPMfgvMDfr9xt1PvFMPAKOBssCE4CEicsZqJpTh6R4X8ODl5/LOrPW8NXM9Nw+bResGVbi/S2O6Nq2pG+pOQTfBiUiJcPDIcT6cu4ERM9axafdBmtSswMDOjejVoi7xcSVnUkzdIS0ikoOjxzP5fNEW3pi+hhVb91G7UhkGdGrIrW3rU6F08b9XQuEgInIK7s70VRm8MX0NKWt3klAmjjs7JNGvYxI1KpYOu7x8o3AQEYnS/A27GDZ9LZOWbSU+NoberRMZ2LkRDaoVv8tgFQ4iIrm0JuMHRny1lo/nbeJYZiY9Lq7NA10ac1HdSmGXlmcUDiIiZ2jb3kO8NXMd76Vs4IfDx+h0bnUeuLwxHRsX/bmuFQ4iImdp76GjvJeygbdmriNj32EurluJ+7o0osdFtYvskOEKBxGRPHLo6HE+mb+J4V+tZd33+2lQrRz3XtaI3q0TKVMqNuzyckXhICKSx45nOpOXbuWN6WtYmL6H6hXi6X9pQ25v34BKZUuFXV5UFA4iIvnE3Zm1dgdvTF/LV6syKB8fS9929RnQqRHnVCrco8EqHERECsDSzXsYNn0tf1+0mdgY4/oWdbmvSyPOrVkx7NJypHAQESlAG3ceYMSMtYxN3ciho5lc1awW93dpTOsGVcIu7T8oHEREQrDjh8O8/c163p71HXsOHqVtUlXuv7wRV5xfs1BcBqtwEBEJ0f7Dx/hg7kbenLGWzXsOcX6titzXpRHXXVKHUrHhDfSncBARKQSOHs9k/ILNDPtqDau2/UCdSmW469IkbmlTP5QrnBQOIiKFSGamM3XldkbMWEvK2p2Uj4/lZ8n16H9pUoGO4aRwEBEppJZs2sNbX69j/MLNHHenW7NaDOjUiDZJVfL9vITCQUSkkNu29xBjZq3nvdkb2H3gKM0TKzGgU0N6Xlw7385LKBxERIqIg0eO89d56bw1cx1rM/ZzTkIZ+nVMom/b+lQql7fnJRQOIiJFTGamM23Vdt78eh0z03ZQtlQsP0tOpP+lDWlYPW/OSygcRESKsGWb9/LWzHWMX7CZo5mZdG1aiwGdGtK+UdWzOi+hcBARKQa27zvEuykbeDflO3buP0Kz2gmM7t+GmglnNoZTtOFQ/GfTFhEpwmpWLMMTV53Hg5c35tP5m5i6cjvVK+T/HNdRnQ43s/VmttjMFphZapb2QWa20syWmtlzWdqfNrO04HtXZ2nvHrSlmdlTWdobmtlsM1ttZh+aWXxeraCISHFQplQst7atz7A7kokpgImGcnOt1BXu3uLE7oiZXQH0Apq7+4XA80F7M+BW4EKgO/C6mcWaWSwwBOgBNAP6BH0BngVecvcmwC5gwNmvmoiInKmzuZD2AeBP7n4YwN23B+29gA/c/bC7rwPSgLbBI83d17r7EeADoJdFzqxcCYwLln8buP4s6hIRkbMUbTg4MNnMvjWzgUHbecBlweGg6WbWJmivC2zMsmx60Hay9mrAbnc/9qN2EREJSbQnpC91981mVhOYYmYrgmWrAO2BNsBYM2sE5HQwzMk5iPwU/bMJgmkgQP369aMsXUREciuqPQd33xz8ux34hMghonTgY4+YA2QC1YP2elkWTwQ2n6L9e6CymcX9qD2nOoa7e7K7J9eoUSO6NRQRkVw7bTiYWXkzq3jiOdANWAJ8SuRcAWZ2HhBP5IN+PHCrmZU2s4ZAE2AOMBdoElyZFE/kpPV4j9xoMRXoHfzIfsBnebeKIiKSW9EcVqoFfBLckRcHvO/uE4MP+LfMbAlwBOgXfNAvNbOxwDLgGPCQux8HMLOHgUlALPCWuy8NfsYvgA/M7HfAfODNPFtDERHJNd0hLSJSghT74TPMLAP47gwXr07kEFhho7pyR3XljurKneJaVwN3P+1J2yIbDmfDzFKjSc6CprpyR3XljurKnZJeV3izXIuISKGlcBARkWxKajgMD7uAk1BduaO6ckd15U6JrqtEnnMQEZFTK6l7DiIicgolKhxONp9EAf3semY21cyWB/NfPBq0/6+ZbQrmylhgZj2zLJPjvBj5UFu2+TrMrKqZTQnm2JhiZlWCdjOzwUFdi8ysVT7VdH6WbbLAzPaa2WNhbS8ze8vMtgc3fZ5oy/U2MrN+Qf/VZtYvn+r6s5mtCH72J2ZWOWhPMrODWbbdG1mWaR38DqQFtZ/VhAEnqSvX711e/589SV0fZqlpvZktCNoLcnud7PMhvN8xdy8RDyJ3Za8BGhEZ6mMh0KwAf35toFXwvCKwisi8Fv8LPJlD/2ZBjaWBhkHtsflU23qg+o/angOeCp4/BTwbPO8JTCAyYGJ7YHYBvXdbgQZhbS+gM9AKWHKm2wioCqwN/q0SPK+SD3V1A+KC589mqSspa78fvc4coENQ8wSgRz7Ulav3Lj/+z+ZU14++/wLwqxC218k+H0L7HStJew45zidRUD/c3be4+7zg+T5gOacemvxk82IUlF5E5taA/5xjoxcwxiNSiAyaWDufa+kKrHH3U930mK/by92/Anbm8DNzs42uBqa4+0533wVMITIhVp7W5e6T/d9D4KcQGczypILaEtx9lkc+YcZwlnOqnGR7nUyu5oDJr7qCv/5vBv5yqtfIp+11ss+H0H7HSlI4nGw+iQJnZklAS2B20PRwsGv41ondRgq23pzm66jl7lsg8osL1AyhrhNu5T//w4a9vU7I7TYKo8a7ifyFeUJDM5tvkTlYLgva6ga1FERduXnvCnp7XQZsc/fVWdoKfHv96PMhtN+xkhQOUc8bka9FmFUA/go85u57gaFAY6AFsIXIbi0UbL2XunsrIlO4PmRmnU/Rt0C3o0UGePwp8FHQVBi21+mcrJaC3nbPEBn88r2gaQtQ391bAk8A75tZQgHWldv3rqDf0z785x8hBb69cvh8OGnXk9SQZ7WVpHA42XwSBcbMShF5499z948B3H2bux9390xgBP8+FFJg9XrO83VsO3G4KPj3xDSwBb0dewDz3H1bUGPo2yuL3G6jAqsxOBF5LXBbcOiD4LDNjuD5t0SO558X1JX10FO+1HUG711Bbq844Ebgwyz1Fuj2yunzgRB/x0pSOOQ4n0RB/fDgeOabwHJ3fzFLe9bj9TcQmSsDTj4vRl7XdbL5OsYTmVsD/nOOjfHAncHVEu2BPSd2e/PJf/w1F/b2+pHcbqNJQDczqxIcUukWtOUpM+tOZBj8n7r7gSztNcwsNnjeiMg2WhvUts/M2ge/p3eSD3OqnMF7V5D/Z38CrHD3fx0uKsjtdbLPB8L8HTubM+xF7UHkDP8qIn8BPFPAP7sTkd27RcCC4NETeAdYHLSPB2pnWeaZoNaVnOXVEKeoqxGRq0AWAktPbBcic3t/AawO/q0atBswJKhrMZCcj9usHLADqJSlLZTtRSSgtgBHifx1NuBMthGRcwBpwaN/PtWVRuS484nfszeCvjcF7/FCYB5wXZbXSSbyYb0GeI3gBtk8rivX711e/5/Nqa6gfTRw/4/6FuT2OtnnQ2i/Y7pDWkREsilJh5VERCRKCgcREclG4SAiItkoHEREJBuFg4iIZKNwEBGRbBQOIiKSjcJBRESy+f8UxMsS6yEROwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(2000, model)\n",
    "costs = model['costs']\n",
    "plt.plot(costs[-2000:])\n",
    "#print(W[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22ace2235c0>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs[-3000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3, Inspect Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pred, real):\n",
    "    right = np.sum(pred==real)\n",
    "    acc = right/real.shape[1]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy: 0.958413\n",
      "test set accuracy: 0.881190\n"
     ]
    }
   ],
   "source": [
    "predict = model['predict']\n",
    "pred = predict(model, train_X)\n",
    "pred = Y_to_labels(pred)\n",
    "acc = get_accuracy(pred, train_labels)\n",
    "print(\"training set accuracy: %f\" % acc)\n",
    "pred = predict(model, test_X)\n",
    "pred = Y_to_labels(pred)\n",
    "acc = get_accuracy(pred, test_labels)\n",
    "print(\"test set accuracy: %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"W-four-layer\", *W)\n",
    "np.savez(\"b-four-layer\", *b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 28000)\n"
     ]
    }
   ],
   "source": [
    "test_X = np.genfromtxt(\"test.csv\", delimiter=',', skip_header=1)\n",
    "test_X = test_X.T\n",
    "test_X = test_X/255\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28000)\n"
     ]
    }
   ],
   "source": [
    "test_Y = predict(test_X)\n",
    "test_labels = Y_to_labels(test_Y)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.zeros((test_labels.shape[1], 2), dtype=int)\n",
    "for i in range(output.shape[0]):\n",
    "    output[i, 0] = i+1\n",
    "    output[i, 1] = test_labels[0, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"4layer-submission.csv\", output, fmt=\"%d\", delimiter=',', header='ImageId,Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17346840320>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_images = X_to_images(test_X)\n",
    "plt.imshow(test_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
