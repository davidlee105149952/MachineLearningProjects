{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digits Recognition Using Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data comes from the \"MNIST\" data set, you can download it from [Kaggle](https://www.kaggle.com/c/digit-recognizer/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1, Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"train.csv\", delimiter=',', skip_header=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "[9. 1. 3. 7. 8. 0. 3. 9. 7. 0.]\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "print(data.shape)\n",
    "print(data[:10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 42000)\n",
      "(1, 42000)\n"
     ]
    }
   ],
   "source": [
    "features = data[:, 1:].T\n",
    "labels = data[:, 0]\n",
    "labels = np.reshape(labels, (1, -1))\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = features.shape[1]\n",
    "nx = features.shape[0]\n",
    "ny = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_Y(labels):\n",
    "    Y = np.zeros((ny, m))\n",
    "    for i in range(m):\n",
    "        Y[int(labels[0, i]), i] = 1\n",
    "    return Y\n",
    "\n",
    "def Y_to_labels(Y):\n",
    "    labels = np.argmax(Y, axis=0).astype(float)\n",
    "    labels = np.reshape(labels, (1, -1))\n",
    "    return labels\n",
    "\n",
    "def X_to_images(X):\n",
    "    images = [np.reshape(X[:, i], (28, 28)) for i in range(X.shape[1])]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 42000)\n",
      "9.0\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = features / 255\n",
    "Y = labels_to_Y(labels)\n",
    "images = X_to_images(X)\n",
    "print(Y.shape)\n",
    "plt.imshow(images[0])\n",
    "print(labels[0, 0])\n",
    "print(Y[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 37800)\n",
      "(10, 37800)\n",
      "(784, 4200)\n",
      "(10, 4200)\n",
      "(1, 37800)\n"
     ]
    }
   ],
   "source": [
    "test_ratio = 1 - train_ratio\n",
    "train_m = int(m * train_ratio)\n",
    "test_m = m - train_m\n",
    "train_X = X[:, :train_m]\n",
    "test_X = X[:, train_m:]\n",
    "train_Y = Y[:, :train_m]\n",
    "test_Y = Y[:, train_m:]\n",
    "train_labels = Y_to_labels(train_Y)\n",
    "test_labels = Y_to_labels(test_Y)\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2, Design Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    a = 1/ (1 + np.exp(-z))\n",
    "    return a\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    p1 = np.exp(-z)\n",
    "    a = p1/((1+p1)**2)\n",
    "    return a\n",
    "\n",
    "def relu(z):\n",
    "    a = np.maximum(z, 0.01*z)\n",
    "    return a\n",
    "\n",
    "def relu_prime(z):\n",
    "    a = np.where(z > 0, 1, 0.01)\n",
    "    return a\n",
    "\n",
    "def tanh(z):\n",
    "    p1 = np.exp(z)\n",
    "    p2 = np.exp(-z)\n",
    "    a = (p1-p2)/(p1+p2)\n",
    "    return a\n",
    "    \n",
    "def tanh_prime(z):\n",
    "    p1 = tanh(z)\n",
    "    a = 1-p1**2\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_for__config_multi_layer_classifier_without_regularization(Y_hat, Y):\n",
    "    delta = 1e-10\n",
    "    l = -((Y+delta)*np.log(Y_hat+delta) + (1-Y+delta)*np.log(1-Y_hat+delta))\n",
    "    return l\n",
    "\n",
    "def forward_propagation_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    X = model['X']\n",
    "    Y = model['Y']\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f = model['f']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    costs = model['costs']\n",
    "    loss_function = model['loss_function']\n",
    "    A[0] = X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    costs.append(np.sum(loss_function(A[L], Y)))\n",
    "\n",
    "def back_propagation_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f_prime = model['f_prime']\n",
    "    m = model['m']\n",
    "    W = model['W']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    Y = model['Y']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    dA = model['dA']\n",
    "    dZ = model['dZ']\n",
    "    dZ[L] = A[L] - Y\n",
    "    dW[L] = np.matmul(dZ[L], A[L-1].T)/m\n",
    "    db[L] = np.sum(dZ[L], axis=1, keepdims=True)/m\n",
    "    for i in reversed(range(1, L)):\n",
    "        dZ[i] = np.matmul(W[i+1].T, dZ[i+1]) * f_prime[i](Z[i])\n",
    "        dW[i] = np.matmul(dZ[i], A[i-1].T)/m\n",
    "        db[i] = np.sum(dZ[i], axis=1, keepdims=True)/m\n",
    "        \n",
    "def update_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    L = model['L']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    learning_rate = model['learning_rate']\n",
    "    for i in range(1, L+1):\n",
    "        W[i] -= learning_rate*dW[i]\n",
    "        b[i] -= learning_rate*db[i]\n",
    "    \n",
    "    \n",
    "def predict_for_config_multi_layer_classifier_without_regularization(model, test_X):\n",
    "    L = model['L']\n",
    "    A = model['A']\n",
    "    Z = model['Z']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    f = model['f']\n",
    "    A[0] = test_X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    return A[L]\n",
    "    \n",
    "    \n",
    "def config_multi_layer_classifier_without_regularization(X, Y, neuron_of_hidden_layer, learning_rate):\n",
    "    n_input_layer = X.shape[0]\n",
    "    n_output_layer = Y.shape[0]\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = [n_input_layer] + neuron_of_hidden_layer + [n_output_layer]\n",
    "    L = len(n)-1\n",
    "    f = [tanh if i != 0 else None for i in range(L)]\n",
    "    f.append(sigmoid)\n",
    "    f_prime = [tanh_prime if i != 0 else None for i in range(L)]\n",
    "    f_prime.append(sigmoid_prime)\n",
    "    W = [np.random.normal(0, 1, (n[i], n[i-1])) if i != 0 else None for i in range(L+1)]\n",
    "    b = [np.random.normal(0, 1, (n[i], 1)) if i != 0 else None for i in range(L+1)]\n",
    "    Z = [None for i in range(L+1)]\n",
    "    A = [None for i in range(L+1)]\n",
    "    dW = [None for i in range(L+1)]\n",
    "    db = [None for i in range(L+1)]\n",
    "    dA = [None for i in range(L+1)]\n",
    "    dZ = [None for i in range(L+1)]\n",
    "    costs = []\n",
    "    \n",
    "    model = dict()\n",
    "    model['X'] = X\n",
    "    model['Y'] = Y\n",
    "    model['m'] = m\n",
    "    model['n'] = n\n",
    "    model['L'] = L\n",
    "    model['f'] = f\n",
    "    model['f_prime'] = f_prime\n",
    "    model['W'] = W\n",
    "    model['b'] = b\n",
    "    model['Z'] = Z\n",
    "    model['A'] = A\n",
    "    model['dW'] = dW\n",
    "    model['db'] = db\n",
    "    model['dA'] = dA\n",
    "    model['dZ'] = dZ\n",
    "    model['loss_function'] = loss_function_for__config_multi_layer_classifier_without_regularization\n",
    "    model['costs'] = costs\n",
    "    model['learning_rate'] = learning_rate\n",
    "    model['forwardprop'] = forward_propagation_for_config_multi_layer_classifier_without_regularization\n",
    "    model['backprop'] = back_propagation_for_config_multi_layer_classifier_without_regularization\n",
    "    model['update'] = update_for_config_multi_layer_classifier_without_regularization\n",
    "    model['predict'] = predict_for_config_multi_layer_classifier_without_regularization\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(iteration_times, model):\n",
    "    forwardprop = model['forwardprop']\n",
    "    backprop = model['backprop']\n",
    "    update = model['update']\n",
    "    costs = model['costs']\n",
    "    for i in range(iteration_times):\n",
    "        forwardprop(model)\n",
    "        backprop(model)\n",
    "        update(model)\n",
    "        print(\"iteration %d, current loss: %f\" % (i, costs[len(costs)-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = config_multi_layer_classifier_without_regularization(train_X, train_Y, [128,32], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, current loss: 19513.877682\n",
      "iteration 1, current loss: 19505.713593\n",
      "iteration 2, current loss: 19497.565354\n",
      "iteration 3, current loss: 19489.432892\n",
      "iteration 4, current loss: 19481.316048\n",
      "iteration 5, current loss: 19473.214612\n",
      "iteration 6, current loss: 19465.128354\n",
      "iteration 7, current loss: 19457.057038\n",
      "iteration 8, current loss: 19449.000428\n",
      "iteration 9, current loss: 19440.958300\n",
      "iteration 10, current loss: 19432.930431\n",
      "iteration 11, current loss: 19424.916605\n",
      "iteration 12, current loss: 19416.916608\n",
      "iteration 13, current loss: 19408.930221\n",
      "iteration 14, current loss: 19400.957225\n",
      "iteration 15, current loss: 19392.997392\n",
      "iteration 16, current loss: 19385.050487\n",
      "iteration 17, current loss: 19377.116264\n",
      "iteration 18, current loss: 19369.194466\n",
      "iteration 19, current loss: 19361.284828\n",
      "iteration 20, current loss: 19353.387070\n",
      "iteration 21, current loss: 19345.500900\n",
      "iteration 22, current loss: 19337.626017\n",
      "iteration 23, current loss: 19329.762107\n",
      "iteration 24, current loss: 19321.908850\n",
      "iteration 25, current loss: 19314.065914\n",
      "iteration 26, current loss: 19306.232967\n",
      "iteration 27, current loss: 19298.409673\n",
      "iteration 28, current loss: 19290.595697\n",
      "iteration 29, current loss: 19282.790714\n",
      "iteration 30, current loss: 19274.994405\n",
      "iteration 31, current loss: 19267.206470\n",
      "iteration 32, current loss: 19259.426625\n",
      "iteration 33, current loss: 19251.654615\n",
      "iteration 34, current loss: 19243.890206\n",
      "iteration 35, current loss: 19236.133198\n",
      "iteration 36, current loss: 19228.383419\n",
      "iteration 37, current loss: 19220.640731\n",
      "iteration 38, current loss: 19212.905024\n",
      "iteration 39, current loss: 19205.176213\n",
      "iteration 40, current loss: 19197.454239\n",
      "iteration 41, current loss: 19189.739061\n",
      "iteration 42, current loss: 19182.030653\n",
      "iteration 43, current loss: 19174.328999\n",
      "iteration 44, current loss: 19166.634091\n",
      "iteration 45, current loss: 19158.945923\n",
      "iteration 46, current loss: 19151.264491\n",
      "iteration 47, current loss: 19143.589790\n",
      "iteration 48, current loss: 19135.921809\n",
      "iteration 49, current loss: 19128.260527\n",
      "iteration 50, current loss: 19120.605906\n",
      "iteration 51, current loss: 19112.957882\n",
      "iteration 52, current loss: 19105.316346\n",
      "iteration 53, current loss: 19097.681138\n",
      "iteration 54, current loss: 19090.052022\n",
      "iteration 55, current loss: 19082.428680\n",
      "iteration 56, current loss: 19074.810689\n",
      "iteration 57, current loss: 19067.197524\n",
      "iteration 58, current loss: 19059.588543\n",
      "iteration 59, current loss: 19051.982997\n",
      "iteration 60, current loss: 19044.380034\n",
      "iteration 61, current loss: 19036.778722\n",
      "iteration 62, current loss: 19029.178083\n",
      "iteration 63, current loss: 19021.577154\n",
      "iteration 64, current loss: 19013.975075\n",
      "iteration 65, current loss: 19006.371229\n",
      "iteration 66, current loss: 18998.765424\n",
      "iteration 67, current loss: 18991.158104\n",
      "iteration 68, current loss: 18983.550571\n",
      "iteration 69, current loss: 18975.945127\n",
      "iteration 70, current loss: 18968.345068\n",
      "iteration 71, current loss: 18960.754466\n",
      "iteration 72, current loss: 18953.177729\n",
      "iteration 73, current loss: 18945.619053\n",
      "iteration 74, current loss: 18938.081920\n",
      "iteration 75, current loss: 18930.568788\n",
      "iteration 76, current loss: 18923.081031\n",
      "iteration 77, current loss: 18915.619068\n",
      "iteration 78, current loss: 18908.182605\n",
      "iteration 79, current loss: 18900.770880\n",
      "iteration 80, current loss: 18893.382869\n",
      "iteration 81, current loss: 18886.017435\n",
      "iteration 82, current loss: 18878.673417\n",
      "iteration 83, current loss: 18871.349683\n",
      "iteration 84, current loss: 18864.045154\n",
      "iteration 85, current loss: 18856.758812\n",
      "iteration 86, current loss: 18849.489701\n",
      "iteration 87, current loss: 18842.236924\n",
      "iteration 88, current loss: 18834.999634\n",
      "iteration 89, current loss: 18827.777035\n",
      "iteration 90, current loss: 18820.568370\n",
      "iteration 91, current loss: 18813.372924\n",
      "iteration 92, current loss: 18806.190016\n",
      "iteration 93, current loss: 18799.018995\n",
      "iteration 94, current loss: 18791.859243\n",
      "iteration 95, current loss: 18784.710169\n",
      "iteration 96, current loss: 18777.571210\n",
      "iteration 97, current loss: 18770.441827\n",
      "iteration 98, current loss: 18763.321511\n",
      "iteration 99, current loss: 18756.209779\n",
      "iteration 100, current loss: 18749.106176\n",
      "iteration 101, current loss: 18742.010278\n",
      "iteration 102, current loss: 18734.921692\n",
      "iteration 103, current loss: 18727.840062\n",
      "iteration 104, current loss: 18720.765066\n",
      "iteration 105, current loss: 18713.696419\n",
      "iteration 106, current loss: 18706.633876\n",
      "iteration 107, current loss: 18699.577225\n",
      "iteration 108, current loss: 18692.526289\n",
      "iteration 109, current loss: 18685.480918\n",
      "iteration 110, current loss: 18678.440982\n",
      "iteration 111, current loss: 18671.406356\n",
      "iteration 112, current loss: 18664.376910\n",
      "iteration 113, current loss: 18657.352495\n",
      "iteration 114, current loss: 18650.332923\n",
      "iteration 115, current loss: 18643.317954\n",
      "iteration 116, current loss: 18636.307286\n",
      "iteration 117, current loss: 18629.300542\n",
      "iteration 118, current loss: 18622.297280\n",
      "iteration 119, current loss: 18615.296996\n",
      "iteration 120, current loss: 18608.299156\n",
      "iteration 121, current loss: 18601.303234\n",
      "iteration 122, current loss: 18594.308774\n",
      "iteration 123, current loss: 18587.315465\n",
      "iteration 124, current loss: 18580.323229\n",
      "iteration 125, current loss: 18573.332300\n",
      "iteration 126, current loss: 18566.343288\n",
      "iteration 127, current loss: 18559.357200\n",
      "iteration 128, current loss: 18552.375393\n",
      "iteration 129, current loss: 18545.399481\n",
      "iteration 130, current loss: 18538.431180\n",
      "iteration 131, current loss: 18531.472161\n",
      "iteration 132, current loss: 18524.523927\n",
      "iteration 133, current loss: 18517.587757\n",
      "iteration 134, current loss: 18510.664711\n",
      "iteration 135, current loss: 18503.755694\n",
      "iteration 136, current loss: 18496.861524\n",
      "iteration 137, current loss: 18489.983002\n",
      "iteration 138, current loss: 18483.120937\n",
      "iteration 139, current loss: 18476.276140\n",
      "iteration 140, current loss: 18469.449374\n",
      "iteration 141, current loss: 18462.641303\n",
      "iteration 142, current loss: 18455.852426\n",
      "iteration 143, current loss: 18449.083038\n",
      "iteration 144, current loss: 18442.333211\n",
      "iteration 145, current loss: 18435.602803\n",
      "iteration 146, current loss: 18428.891474\n",
      "iteration 147, current loss: 18422.198727\n",
      "iteration 148, current loss: 18415.523942\n",
      "iteration 149, current loss: 18408.866412\n",
      "iteration 150, current loss: 18402.225375\n",
      "iteration 151, current loss: 18395.600039\n",
      "iteration 152, current loss: 18388.989605\n",
      "iteration 153, current loss: 18382.393281\n",
      "iteration 154, current loss: 18375.810293\n",
      "iteration 155, current loss: 18369.239898\n",
      "iteration 156, current loss: 18362.681387\n",
      "iteration 157, current loss: 18356.134088\n",
      "iteration 158, current loss: 18349.597374\n",
      "iteration 159, current loss: 18343.070664\n",
      "iteration 160, current loss: 18336.553421\n",
      "iteration 161, current loss: 18330.045163\n",
      "iteration 162, current loss: 18323.545457\n",
      "iteration 163, current loss: 18317.053926\n",
      "iteration 164, current loss: 18310.570252\n",
      "iteration 165, current loss: 18304.094177\n",
      "iteration 166, current loss: 18297.625506\n",
      "iteration 167, current loss: 18291.164106\n",
      "iteration 168, current loss: 18284.709906\n",
      "iteration 169, current loss: 18278.262891\n",
      "iteration 170, current loss: 18271.823092\n",
      "iteration 171, current loss: 18265.390579\n",
      "iteration 172, current loss: 18258.965441\n",
      "iteration 173, current loss: 18252.547775\n",
      "iteration 174, current loss: 18246.137667\n",
      "iteration 175, current loss: 18239.735178\n",
      "iteration 176, current loss: 18233.340333\n",
      "iteration 177, current loss: 18226.953111\n",
      "iteration 178, current loss: 18220.573443\n",
      "iteration 179, current loss: 18214.201212\n",
      "iteration 180, current loss: 18207.836256\n",
      "iteration 181, current loss: 18201.478377\n",
      "iteration 182, current loss: 18195.127349\n",
      "iteration 183, current loss: 18188.782929\n",
      "iteration 184, current loss: 18182.444873\n",
      "iteration 185, current loss: 18176.112944\n",
      "iteration 186, current loss: 18169.786933\n",
      "iteration 187, current loss: 18163.466664\n",
      "iteration 188, current loss: 18157.152018\n",
      "iteration 189, current loss: 18150.842941\n",
      "iteration 190, current loss: 18144.539460\n",
      "iteration 191, current loss: 18138.241690\n",
      "iteration 192, current loss: 18131.949850\n",
      "iteration 193, current loss: 18125.664257\n",
      "iteration 194, current loss: 18119.385332\n",
      "iteration 195, current loss: 18113.113590\n",
      "iteration 196, current loss: 18106.849623\n",
      "iteration 197, current loss: 18100.594080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 198, current loss: 18094.347644\n",
      "iteration 199, current loss: 18088.110999\n",
      "iteration 200, current loss: 18081.884803\n",
      "iteration 201, current loss: 18075.669664\n",
      "iteration 202, current loss: 18069.466113\n",
      "iteration 203, current loss: 18063.274596\n",
      "iteration 204, current loss: 18057.095458\n",
      "iteration 205, current loss: 18050.928948\n",
      "iteration 206, current loss: 18044.775214\n",
      "iteration 207, current loss: 18038.634317\n",
      "iteration 208, current loss: 18032.506240\n",
      "iteration 209, current loss: 18026.390895\n",
      "iteration 210, current loss: 18020.288141\n",
      "iteration 211, current loss: 18014.197792\n",
      "iteration 212, current loss: 18008.119631\n",
      "iteration 213, current loss: 18002.053418\n",
      "iteration 214, current loss: 17995.998901\n",
      "iteration 215, current loss: 17989.955821\n",
      "iteration 216, current loss: 17983.923924\n",
      "iteration 217, current loss: 17977.902959\n",
      "iteration 218, current loss: 17971.892687\n",
      "iteration 219, current loss: 17965.892881\n",
      "iteration 220, current loss: 17959.903327\n",
      "iteration 221, current loss: 17953.923823\n",
      "iteration 222, current loss: 17947.954180\n",
      "iteration 223, current loss: 17941.994216\n",
      "iteration 224, current loss: 17936.043756\n",
      "iteration 225, current loss: 17930.102629\n",
      "iteration 226, current loss: 17924.170661\n",
      "iteration 227, current loss: 17918.247674\n",
      "iteration 228, current loss: 17912.333485\n",
      "iteration 229, current loss: 17906.427899\n",
      "iteration 230, current loss: 17900.530710\n",
      "iteration 231, current loss: 17894.641702\n",
      "iteration 232, current loss: 17888.760646\n",
      "iteration 233, current loss: 17882.887300\n",
      "iteration 234, current loss: 17877.021417\n",
      "iteration 235, current loss: 17871.162737\n",
      "iteration 236, current loss: 17865.311000\n",
      "iteration 237, current loss: 17859.465941\n",
      "iteration 238, current loss: 17853.627297\n",
      "iteration 239, current loss: 17847.794806\n",
      "iteration 240, current loss: 17841.968214\n",
      "iteration 241, current loss: 17836.147273\n",
      "iteration 242, current loss: 17830.331746\n",
      "iteration 243, current loss: 17824.521406\n",
      "iteration 244, current loss: 17818.716036\n",
      "iteration 245, current loss: 17812.915433\n",
      "iteration 246, current loss: 17807.119407\n",
      "iteration 247, current loss: 17801.327779\n",
      "iteration 248, current loss: 17795.540382\n",
      "iteration 249, current loss: 17789.757064\n",
      "iteration 250, current loss: 17783.977686\n",
      "iteration 251, current loss: 17778.202119\n",
      "iteration 252, current loss: 17772.430254\n",
      "iteration 253, current loss: 17766.661993\n",
      "iteration 254, current loss: 17760.897256\n",
      "iteration 255, current loss: 17755.135978\n",
      "iteration 256, current loss: 17749.378115\n",
      "iteration 257, current loss: 17743.623636\n",
      "iteration 258, current loss: 17737.872528\n",
      "iteration 259, current loss: 17732.124794\n",
      "iteration 260, current loss: 17726.380444\n",
      "iteration 261, current loss: 17720.639498\n",
      "iteration 262, current loss: 17714.901975\n",
      "iteration 263, current loss: 17709.167890\n",
      "iteration 264, current loss: 17703.437245\n",
      "iteration 265, current loss: 17697.710024\n",
      "iteration 266, current loss: 17691.986183\n",
      "iteration 267, current loss: 17686.265649\n",
      "iteration 268, current loss: 17680.548313\n",
      "iteration 269, current loss: 17674.834024\n",
      "iteration 270, current loss: 17669.122594\n",
      "iteration 271, current loss: 17663.413792\n",
      "iteration 272, current loss: 17657.707352\n",
      "iteration 273, current loss: 17652.002971\n",
      "iteration 274, current loss: 17646.300316\n",
      "iteration 275, current loss: 17640.599033\n",
      "iteration 276, current loss: 17634.898748\n",
      "iteration 277, current loss: 17629.199078\n",
      "iteration 278, current loss: 17623.499637\n",
      "iteration 279, current loss: 17617.800047\n",
      "iteration 280, current loss: 17612.099945\n",
      "iteration 281, current loss: 17606.398987\n",
      "iteration 282, current loss: 17600.696866\n",
      "iteration 283, current loss: 17594.993312\n",
      "iteration 284, current loss: 17589.288103\n",
      "iteration 285, current loss: 17583.581075\n",
      "iteration 286, current loss: 17577.872125\n",
      "iteration 287, current loss: 17572.161220\n",
      "iteration 288, current loss: 17566.448398\n",
      "iteration 289, current loss: 17560.733773\n",
      "iteration 290, current loss: 17555.017535\n",
      "iteration 291, current loss: 17549.299945\n",
      "iteration 292, current loss: 17543.581331\n",
      "iteration 293, current loss: 17537.862082\n",
      "iteration 294, current loss: 17532.142637\n",
      "iteration 295, current loss: 17526.423470\n",
      "iteration 296, current loss: 17520.705081\n",
      "iteration 297, current loss: 17514.987978\n",
      "iteration 298, current loss: 17509.272669\n",
      "iteration 299, current loss: 17503.559644\n",
      "iteration 300, current loss: 17497.849367\n",
      "iteration 301, current loss: 17492.142268\n",
      "iteration 302, current loss: 17486.438737\n",
      "iteration 303, current loss: 17480.739115\n",
      "iteration 304, current loss: 17475.043694\n",
      "iteration 305, current loss: 17469.352712\n",
      "iteration 306, current loss: 17463.666349\n",
      "iteration 307, current loss: 17457.984728\n",
      "iteration 308, current loss: 17452.307909\n",
      "iteration 309, current loss: 17446.635889\n",
      "iteration 310, current loss: 17440.968597\n",
      "iteration 311, current loss: 17435.305901\n",
      "iteration 312, current loss: 17429.647607\n",
      "iteration 313, current loss: 17423.993462\n",
      "iteration 314, current loss: 17418.343162\n",
      "iteration 315, current loss: 17412.696362\n",
      "iteration 316, current loss: 17407.052680\n",
      "iteration 317, current loss: 17401.411709\n",
      "iteration 318, current loss: 17395.773026\n",
      "iteration 319, current loss: 17390.136198\n",
      "iteration 320, current loss: 17384.500791\n",
      "iteration 321, current loss: 17378.866371\n",
      "iteration 322, current loss: 17373.232514\n",
      "iteration 323, current loss: 17367.598806\n",
      "iteration 324, current loss: 17361.964843\n",
      "iteration 325, current loss: 17356.330240\n",
      "iteration 326, current loss: 17350.694625\n",
      "iteration 327, current loss: 17345.057651\n",
      "iteration 328, current loss: 17339.418994\n",
      "iteration 329, current loss: 17333.778364\n",
      "iteration 330, current loss: 17328.135510\n",
      "iteration 331, current loss: 17322.490234\n",
      "iteration 332, current loss: 17316.842400\n",
      "iteration 333, current loss: 17311.191949\n",
      "iteration 334, current loss: 17305.538913\n",
      "iteration 335, current loss: 17299.883426\n",
      "iteration 336, current loss: 17294.225741\n",
      "iteration 337, current loss: 17288.566228\n",
      "iteration 338, current loss: 17282.905383\n",
      "iteration 339, current loss: 17277.243814\n",
      "iteration 340, current loss: 17271.582229\n",
      "iteration 341, current loss: 17265.921407\n",
      "iteration 342, current loss: 17260.262169\n",
      "iteration 343, current loss: 17254.605335\n",
      "iteration 344, current loss: 17248.951687\n",
      "iteration 345, current loss: 17243.301931\n",
      "iteration 346, current loss: 17237.656670\n",
      "iteration 347, current loss: 17232.016375\n",
      "iteration 348, current loss: 17226.381377\n",
      "iteration 349, current loss: 17220.751862\n",
      "iteration 350, current loss: 17215.127874\n",
      "iteration 351, current loss: 17209.509331\n",
      "iteration 352, current loss: 17203.896039\n",
      "iteration 353, current loss: 17198.287713\n",
      "iteration 354, current loss: 17192.684005\n",
      "iteration 355, current loss: 17187.084525\n",
      "iteration 356, current loss: 17181.488876\n",
      "iteration 357, current loss: 17175.896679\n",
      "iteration 358, current loss: 17170.307608\n",
      "iteration 359, current loss: 17164.721416\n",
      "iteration 360, current loss: 17159.137969\n",
      "iteration 361, current loss: 17153.557274\n",
      "iteration 362, current loss: 17147.979498\n",
      "iteration 363, current loss: 17142.404993\n",
      "iteration 364, current loss: 17136.834299\n",
      "iteration 365, current loss: 17131.268149\n",
      "iteration 366, current loss: 17125.707456\n",
      "iteration 367, current loss: 17120.153294\n",
      "iteration 368, current loss: 17114.606857\n",
      "iteration 369, current loss: 17109.069427\n",
      "iteration 370, current loss: 17103.542319\n",
      "iteration 371, current loss: 17098.026836\n",
      "iteration 372, current loss: 17092.524222\n",
      "iteration 373, current loss: 17087.035623\n",
      "iteration 374, current loss: 17081.562056\n",
      "iteration 375, current loss: 17076.104390\n",
      "iteration 376, current loss: 17070.663330\n",
      "iteration 377, current loss: 17065.239421\n",
      "iteration 378, current loss: 17059.833045\n",
      "iteration 379, current loss: 17054.444434\n",
      "iteration 380, current loss: 17049.073677\n",
      "iteration 381, current loss: 17043.720730\n",
      "iteration 382, current loss: 17038.385437\n",
      "iteration 383, current loss: 17033.067532\n",
      "iteration 384, current loss: 17027.766659\n",
      "iteration 385, current loss: 17022.482384\n",
      "iteration 386, current loss: 17017.214202\n",
      "iteration 387, current loss: 17011.961556\n",
      "iteration 388, current loss: 17006.723842\n",
      "iteration 389, current loss: 17001.500422\n",
      "iteration 390, current loss: 16996.290635\n",
      "iteration 391, current loss: 16991.093797\n",
      "iteration 392, current loss: 16985.909217\n",
      "iteration 393, current loss: 16980.736194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 394, current loss: 16975.574023\n",
      "iteration 395, current loss: 16970.421997\n",
      "iteration 396, current loss: 16965.279410\n",
      "iteration 397, current loss: 16960.145554\n",
      "iteration 398, current loss: 16955.019719\n",
      "iteration 399, current loss: 16949.901199\n",
      "iteration 400, current loss: 16944.789287\n",
      "iteration 401, current loss: 16939.683283\n",
      "iteration 402, current loss: 16934.582497\n",
      "iteration 403, current loss: 16929.486262\n",
      "iteration 404, current loss: 16924.393945\n",
      "iteration 405, current loss: 16919.304970\n",
      "iteration 406, current loss: 16914.218844\n",
      "iteration 407, current loss: 16909.135185\n",
      "iteration 408, current loss: 16904.053760\n",
      "iteration 409, current loss: 16898.974512\n",
      "iteration 410, current loss: 16893.897591\n",
      "iteration 411, current loss: 16888.823356\n",
      "iteration 412, current loss: 16883.752366\n",
      "iteration 413, current loss: 16878.685338\n",
      "iteration 414, current loss: 16873.623087\n",
      "iteration 415, current loss: 16868.566447\n",
      "iteration 416, current loss: 16863.516196\n",
      "iteration 417, current loss: 16858.472991\n",
      "iteration 418, current loss: 16853.437328\n",
      "iteration 419, current loss: 16848.409526\n",
      "iteration 420, current loss: 16843.389735\n",
      "iteration 421, current loss: 16838.377957\n",
      "iteration 422, current loss: 16833.374074\n",
      "iteration 423, current loss: 16828.377881\n",
      "iteration 424, current loss: 16823.389115\n",
      "iteration 425, current loss: 16818.407474\n",
      "iteration 426, current loss: 16813.432645\n",
      "iteration 427, current loss: 16808.464318\n",
      "iteration 428, current loss: 16803.502200\n",
      "iteration 429, current loss: 16798.546031\n",
      "iteration 430, current loss: 16793.595599\n",
      "iteration 431, current loss: 16788.650747\n",
      "iteration 432, current loss: 16783.711394\n",
      "iteration 433, current loss: 16778.777540\n",
      "iteration 434, current loss: 16773.849279\n",
      "iteration 435, current loss: 16768.926804\n",
      "iteration 436, current loss: 16764.010413\n",
      "iteration 437, current loss: 16759.100499\n",
      "iteration 438, current loss: 16754.197539\n",
      "iteration 439, current loss: 16749.302073\n",
      "iteration 440, current loss: 16744.414676\n",
      "iteration 441, current loss: 16739.535923\n",
      "iteration 442, current loss: 16734.666356\n",
      "iteration 443, current loss: 16729.806452\n",
      "iteration 444, current loss: 16724.956596\n",
      "iteration 445, current loss: 16720.117064\n",
      "iteration 446, current loss: 16715.288017\n",
      "iteration 447, current loss: 16710.469501\n",
      "iteration 448, current loss: 16705.661458\n",
      "iteration 449, current loss: 16700.863738\n",
      "iteration 450, current loss: 16696.076115\n",
      "iteration 451, current loss: 16691.298306\n",
      "iteration 452, current loss: 16686.529985\n",
      "iteration 453, current loss: 16681.770795\n",
      "iteration 454, current loss: 16677.020363\n",
      "iteration 455, current loss: 16672.278305\n",
      "iteration 456, current loss: 16667.544232\n",
      "iteration 457, current loss: 16662.817759\n",
      "iteration 458, current loss: 16658.098500\n",
      "iteration 459, current loss: 16653.386075\n",
      "iteration 460, current loss: 16648.680108\n",
      "iteration 461, current loss: 16643.980227\n",
      "iteration 462, current loss: 16639.286064\n",
      "iteration 463, current loss: 16634.597255\n",
      "iteration 464, current loss: 16629.913441\n",
      "iteration 465, current loss: 16625.234268\n",
      "iteration 466, current loss: 16620.559389\n",
      "iteration 467, current loss: 16615.888468\n",
      "iteration 468, current loss: 16611.221184\n",
      "iteration 469, current loss: 16606.557237\n",
      "iteration 470, current loss: 16601.896355\n",
      "iteration 471, current loss: 16597.238301\n",
      "iteration 472, current loss: 16592.582886\n",
      "iteration 473, current loss: 16587.929975\n",
      "iteration 474, current loss: 16583.279499\n",
      "iteration 475, current loss: 16578.631461\n",
      "iteration 476, current loss: 16573.985942\n",
      "iteration 477, current loss: 16569.343102\n",
      "iteration 478, current loss: 16564.703173\n",
      "iteration 479, current loss: 16560.066452\n",
      "iteration 480, current loss: 16555.433281\n",
      "iteration 481, current loss: 16550.804027\n",
      "iteration 482, current loss: 16546.179057\n",
      "iteration 483, current loss: 16541.558715\n",
      "iteration 484, current loss: 16536.943299\n",
      "iteration 485, current loss: 16532.333038\n",
      "iteration 486, current loss: 16527.728086\n",
      "iteration 487, current loss: 16523.128508\n",
      "iteration 488, current loss: 16518.534284\n",
      "iteration 489, current loss: 16513.945308\n",
      "iteration 490, current loss: 16509.361401\n",
      "iteration 491, current loss: 16504.782314\n",
      "iteration 492, current loss: 16500.207745\n",
      "iteration 493, current loss: 16495.637346\n",
      "iteration 494, current loss: 16491.070735\n",
      "iteration 495, current loss: 16486.507505\n",
      "iteration 496, current loss: 16481.947233\n",
      "iteration 497, current loss: 16477.389484\n",
      "iteration 498, current loss: 16472.833820\n",
      "iteration 499, current loss: 16468.279801\n",
      "iteration 500, current loss: 16463.726988\n",
      "iteration 501, current loss: 16459.174943\n",
      "iteration 502, current loss: 16454.623232\n",
      "iteration 503, current loss: 16450.071421\n",
      "iteration 504, current loss: 16445.519079\n",
      "iteration 505, current loss: 16440.965774\n",
      "iteration 506, current loss: 16436.411073\n",
      "iteration 507, current loss: 16431.854546\n",
      "iteration 508, current loss: 16427.295762\n",
      "iteration 509, current loss: 16422.734293\n",
      "iteration 510, current loss: 16418.169724\n",
      "iteration 511, current loss: 16413.601652\n",
      "iteration 512, current loss: 16409.029703\n",
      "iteration 513, current loss: 16404.453538\n",
      "iteration 514, current loss: 16399.872871\n",
      "iteration 515, current loss: 16395.287483\n",
      "iteration 516, current loss: 16390.697244\n",
      "iteration 517, current loss: 16386.102129\n",
      "iteration 518, current loss: 16381.502239\n",
      "iteration 519, current loss: 16376.897810\n",
      "iteration 520, current loss: 16372.289230\n",
      "iteration 521, current loss: 16367.677031\n",
      "iteration 522, current loss: 16363.061878\n",
      "iteration 523, current loss: 16358.444547\n",
      "iteration 524, current loss: 16353.825878\n",
      "iteration 525, current loss: 16349.206735\n",
      "iteration 526, current loss: 16344.587946\n",
      "iteration 527, current loss: 16339.970260\n",
      "iteration 528, current loss: 16335.354305\n",
      "iteration 529, current loss: 16330.740579\n",
      "iteration 530, current loss: 16326.129448\n",
      "iteration 531, current loss: 16321.521190\n",
      "iteration 532, current loss: 16316.916044\n",
      "iteration 533, current loss: 16312.314295\n",
      "iteration 534, current loss: 16307.716350\n",
      "iteration 535, current loss: 16303.122818\n",
      "iteration 536, current loss: 16298.534539\n",
      "iteration 537, current loss: 16293.952570\n",
      "iteration 538, current loss: 16289.378088\n",
      "iteration 539, current loss: 16284.812246\n",
      "iteration 540, current loss: 16280.255996\n",
      "iteration 541, current loss: 16275.709949\n",
      "iteration 542, current loss: 16271.174301\n",
      "iteration 543, current loss: 16266.648835\n",
      "iteration 544, current loss: 16262.132982\n",
      "iteration 545, current loss: 16257.625917\n",
      "iteration 546, current loss: 16253.126642\n",
      "iteration 547, current loss: 16248.634065\n",
      "iteration 548, current loss: 16244.147054\n",
      "iteration 549, current loss: 16239.664467\n",
      "iteration 550, current loss: 16235.185181\n",
      "iteration 551, current loss: 16230.708106\n",
      "iteration 552, current loss: 16226.232199\n",
      "iteration 553, current loss: 16221.756487\n",
      "iteration 554, current loss: 16217.280090\n",
      "iteration 555, current loss: 16212.802260\n",
      "iteration 556, current loss: 16208.322422\n",
      "iteration 557, current loss: 16203.840229\n",
      "iteration 558, current loss: 16199.355612\n",
      "iteration 559, current loss: 16194.868826\n",
      "iteration 560, current loss: 16190.380466\n",
      "iteration 561, current loss: 16185.891448\n",
      "iteration 562, current loss: 16181.402946\n",
      "iteration 563, current loss: 16176.916275\n",
      "iteration 564, current loss: 16172.432750\n",
      "iteration 565, current loss: 16167.953541\n",
      "iteration 566, current loss: 16163.479553\n",
      "iteration 567, current loss: 16159.011356\n",
      "iteration 568, current loss: 16154.549169\n",
      "iteration 569, current loss: 16150.092886\n",
      "iteration 570, current loss: 16145.642132\n",
      "iteration 571, current loss: 16141.196335\n",
      "iteration 572, current loss: 16136.754787\n",
      "iteration 573, current loss: 16132.316704\n",
      "iteration 574, current loss: 16127.881274\n",
      "iteration 575, current loss: 16123.447687\n",
      "iteration 576, current loss: 16119.015163\n",
      "iteration 577, current loss: 16114.582967\n",
      "iteration 578, current loss: 16110.150419\n",
      "iteration 579, current loss: 16105.716900\n",
      "iteration 580, current loss: 16101.281852\n",
      "iteration 581, current loss: 16096.844776\n",
      "iteration 582, current loss: 16092.405231\n",
      "iteration 583, current loss: 16087.962827\n",
      "iteration 584, current loss: 16083.517220\n",
      "iteration 585, current loss: 16079.068117\n",
      "iteration 586, current loss: 16074.615271\n",
      "iteration 587, current loss: 16070.158490\n",
      "iteration 588, current loss: 16065.697653\n",
      "iteration 589, current loss: 16061.232721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 590, current loss: 16056.763764\n",
      "iteration 591, current loss: 16052.290983\n",
      "iteration 592, current loss: 16047.814724\n",
      "iteration 593, current loss: 16043.335494\n",
      "iteration 594, current loss: 16038.853960\n",
      "iteration 595, current loss: 16034.370927\n",
      "iteration 596, current loss: 16029.887307\n",
      "iteration 597, current loss: 16025.404076\n",
      "iteration 598, current loss: 16020.922216\n",
      "iteration 599, current loss: 16016.442669\n",
      "iteration 600, current loss: 16011.966291\n",
      "iteration 601, current loss: 16007.493821\n",
      "iteration 602, current loss: 16003.025861\n",
      "iteration 603, current loss: 15998.562871\n",
      "iteration 604, current loss: 15994.105169\n",
      "iteration 605, current loss: 15989.652942\n",
      "iteration 606, current loss: 15985.206255\n",
      "iteration 607, current loss: 15980.765071\n",
      "iteration 608, current loss: 15976.329263\n",
      "iteration 609, current loss: 15971.898635\n",
      "iteration 610, current loss: 15967.472934\n",
      "iteration 611, current loss: 15963.051867\n",
      "iteration 612, current loss: 15958.635117\n",
      "iteration 613, current loss: 15954.222354\n",
      "iteration 614, current loss: 15949.813246\n",
      "iteration 615, current loss: 15945.407470\n",
      "iteration 616, current loss: 15941.004714\n",
      "iteration 617, current loss: 15936.604688\n",
      "iteration 618, current loss: 15932.207124\n",
      "iteration 619, current loss: 15927.811778\n",
      "iteration 620, current loss: 15923.418432\n",
      "iteration 621, current loss: 15919.026892\n",
      "iteration 622, current loss: 15914.636991\n",
      "iteration 623, current loss: 15910.248580\n",
      "iteration 624, current loss: 15905.861532\n",
      "iteration 625, current loss: 15901.475736\n",
      "iteration 626, current loss: 15897.091096\n",
      "iteration 627, current loss: 15892.707527\n",
      "iteration 628, current loss: 15888.324949\n",
      "iteration 629, current loss: 15883.943292\n",
      "iteration 630, current loss: 15879.562486\n",
      "iteration 631, current loss: 15875.182464\n",
      "iteration 632, current loss: 15870.803157\n",
      "iteration 633, current loss: 15866.424496\n",
      "iteration 634, current loss: 15862.046407\n",
      "iteration 635, current loss: 15857.668815\n",
      "iteration 636, current loss: 15853.291642\n",
      "iteration 637, current loss: 15848.914806\n",
      "iteration 638, current loss: 15844.538222\n",
      "iteration 639, current loss: 15840.161802\n",
      "iteration 640, current loss: 15835.785456\n",
      "iteration 641, current loss: 15831.409091\n",
      "iteration 642, current loss: 15827.032613\n",
      "iteration 643, current loss: 15822.655922\n",
      "iteration 644, current loss: 15818.278921\n",
      "iteration 645, current loss: 15813.901507\n",
      "iteration 646, current loss: 15809.523577\n",
      "iteration 647, current loss: 15805.145023\n",
      "iteration 648, current loss: 15800.765739\n",
      "iteration 649, current loss: 15796.385615\n",
      "iteration 650, current loss: 15792.004544\n",
      "iteration 651, current loss: 15787.622417\n",
      "iteration 652, current loss: 15783.239134\n",
      "iteration 653, current loss: 15778.854597\n",
      "iteration 654, current loss: 15774.468720\n",
      "iteration 655, current loss: 15770.081435\n",
      "iteration 656, current loss: 15765.692689\n",
      "iteration 657, current loss: 15761.302457\n",
      "iteration 658, current loss: 15756.910745\n",
      "iteration 659, current loss: 15752.517596\n",
      "iteration 660, current loss: 15748.123097\n",
      "iteration 661, current loss: 15743.727382\n",
      "iteration 662, current loss: 15739.330640\n",
      "iteration 663, current loss: 15734.933111\n",
      "iteration 664, current loss: 15730.535095\n",
      "iteration 665, current loss: 15726.136944\n",
      "iteration 666, current loss: 15721.739059\n",
      "iteration 667, current loss: 15717.341886\n",
      "iteration 668, current loss: 15712.945901\n",
      "iteration 669, current loss: 15708.551603\n",
      "iteration 670, current loss: 15704.159497\n",
      "iteration 671, current loss: 15699.770082\n",
      "iteration 672, current loss: 15695.383835\n",
      "iteration 673, current loss: 15691.001198\n",
      "iteration 674, current loss: 15686.622562\n",
      "iteration 675, current loss: 15682.248263\n",
      "iteration 676, current loss: 15677.878571\n",
      "iteration 677, current loss: 15673.513683\n",
      "iteration 678, current loss: 15669.153729\n",
      "iteration 679, current loss: 15664.798768\n",
      "iteration 680, current loss: 15660.448793\n",
      "iteration 681, current loss: 15656.103739\n",
      "iteration 682, current loss: 15651.763489\n",
      "iteration 683, current loss: 15647.427884\n",
      "iteration 684, current loss: 15643.096731\n",
      "iteration 685, current loss: 15638.769812\n",
      "iteration 686, current loss: 15634.446892\n",
      "iteration 687, current loss: 15630.127729\n",
      "iteration 688, current loss: 15625.812081\n",
      "iteration 689, current loss: 15621.499710\n",
      "iteration 690, current loss: 15617.190392\n",
      "iteration 691, current loss: 15612.883917\n",
      "iteration 692, current loss: 15608.580098\n",
      "iteration 693, current loss: 15604.278768\n",
      "iteration 694, current loss: 15599.979790\n",
      "iteration 695, current loss: 15595.683051\n",
      "iteration 696, current loss: 15591.388464\n",
      "iteration 697, current loss: 15587.095973\n",
      "iteration 698, current loss: 15582.805544\n",
      "iteration 699, current loss: 15578.517171\n",
      "iteration 700, current loss: 15574.230868\n",
      "iteration 701, current loss: 15569.946673\n",
      "iteration 702, current loss: 15565.664641\n",
      "iteration 703, current loss: 15561.384842\n",
      "iteration 704, current loss: 15557.107364\n",
      "iteration 705, current loss: 15552.832306\n",
      "iteration 706, current loss: 15548.559783\n",
      "iteration 707, current loss: 15544.289919\n",
      "iteration 708, current loss: 15540.022852\n",
      "iteration 709, current loss: 15535.758732\n",
      "iteration 710, current loss: 15531.497724\n",
      "iteration 711, current loss: 15527.240005\n",
      "iteration 712, current loss: 15522.985769\n",
      "iteration 713, current loss: 15518.735225\n",
      "iteration 714, current loss: 15514.488597\n",
      "iteration 715, current loss: 15510.246125\n",
      "iteration 716, current loss: 15506.008063\n",
      "iteration 717, current loss: 15501.774674\n",
      "iteration 718, current loss: 15497.546231\n",
      "iteration 719, current loss: 15493.323012\n",
      "iteration 720, current loss: 15489.105295\n",
      "iteration 721, current loss: 15484.893354\n",
      "iteration 722, current loss: 15480.687455\n",
      "iteration 723, current loss: 15476.487850\n",
      "iteration 724, current loss: 15472.294774\n",
      "iteration 725, current loss: 15468.108444\n",
      "iteration 726, current loss: 15463.929050\n",
      "iteration 727, current loss: 15459.756761\n",
      "iteration 728, current loss: 15455.591720\n",
      "iteration 729, current loss: 15451.434043\n",
      "iteration 730, current loss: 15447.283824\n",
      "iteration 731, current loss: 15443.141131\n",
      "iteration 732, current loss: 15439.006013\n",
      "iteration 733, current loss: 15434.878498\n",
      "iteration 734, current loss: 15430.758597\n",
      "iteration 735, current loss: 15426.646306\n",
      "iteration 736, current loss: 15422.541607\n",
      "iteration 737, current loss: 15418.444471\n",
      "iteration 738, current loss: 15414.354856\n",
      "iteration 739, current loss: 15410.272711\n",
      "iteration 740, current loss: 15406.197978\n",
      "iteration 741, current loss: 15402.130586\n",
      "iteration 742, current loss: 15398.070458\n",
      "iteration 743, current loss: 15394.017504\n",
      "iteration 744, current loss: 15389.971629\n",
      "iteration 745, current loss: 15385.932723\n",
      "iteration 746, current loss: 15381.900670\n",
      "iteration 747, current loss: 15377.875342\n",
      "iteration 748, current loss: 15373.856601\n",
      "iteration 749, current loss: 15369.844299\n",
      "iteration 750, current loss: 15365.838280\n",
      "iteration 751, current loss: 15361.838379\n",
      "iteration 752, current loss: 15357.844426\n",
      "iteration 753, current loss: 15353.856243\n",
      "iteration 754, current loss: 15349.873651\n",
      "iteration 755, current loss: 15345.896470\n",
      "iteration 756, current loss: 15341.924519\n",
      "iteration 757, current loss: 15337.957621\n",
      "iteration 758, current loss: 15333.995605\n",
      "iteration 759, current loss: 15330.038309\n",
      "iteration 760, current loss: 15326.085581\n",
      "iteration 761, current loss: 15322.137282\n",
      "iteration 762, current loss: 15318.193289\n",
      "iteration 763, current loss: 15314.253498\n",
      "iteration 764, current loss: 15310.317823\n",
      "iteration 765, current loss: 15306.386196\n",
      "iteration 766, current loss: 15302.458573\n",
      "iteration 767, current loss: 15298.534927\n",
      "iteration 768, current loss: 15294.615251\n",
      "iteration 769, current loss: 15290.699554\n",
      "iteration 770, current loss: 15286.787855\n",
      "iteration 771, current loss: 15282.880186\n",
      "iteration 772, current loss: 15278.976580\n",
      "iteration 773, current loss: 15275.077068\n",
      "iteration 774, current loss: 15271.181677\n",
      "iteration 775, current loss: 15267.290420\n",
      "iteration 776, current loss: 15263.403292\n",
      "iteration 777, current loss: 15259.520269\n",
      "iteration 778, current loss: 15255.641298\n",
      "iteration 779, current loss: 15251.766304\n",
      "iteration 780, current loss: 15247.895181\n",
      "iteration 781, current loss: 15244.027798\n",
      "iteration 782, current loss: 15240.163999\n",
      "iteration 783, current loss: 15236.303609\n",
      "iteration 784, current loss: 15232.446436\n",
      "iteration 785, current loss: 15228.592282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 786, current loss: 15224.740949\n",
      "iteration 787, current loss: 15220.892249\n",
      "iteration 788, current loss: 15217.046013\n",
      "iteration 789, current loss: 15213.202100\n",
      "iteration 790, current loss: 15209.360408\n",
      "iteration 791, current loss: 15205.520876\n",
      "iteration 792, current loss: 15201.683491\n",
      "iteration 793, current loss: 15197.848286\n",
      "iteration 794, current loss: 15194.015334\n",
      "iteration 795, current loss: 15190.184737\n",
      "iteration 796, current loss: 15186.356615\n",
      "iteration 797, current loss: 15182.531088\n",
      "iteration 798, current loss: 15178.708255\n",
      "iteration 799, current loss: 15174.888183\n",
      "iteration 800, current loss: 15171.070891\n",
      "iteration 801, current loss: 15167.256342\n",
      "iteration 802, current loss: 15163.444435\n",
      "iteration 803, current loss: 15159.635010\n",
      "iteration 804, current loss: 15155.827847\n",
      "iteration 805, current loss: 15152.022674\n",
      "iteration 806, current loss: 15148.219174\n",
      "iteration 807, current loss: 15144.416992\n",
      "iteration 808, current loss: 15140.615743\n",
      "iteration 809, current loss: 15136.815019\n",
      "iteration 810, current loss: 15133.014394\n",
      "iteration 811, current loss: 15129.213433\n",
      "iteration 812, current loss: 15125.411694\n",
      "iteration 813, current loss: 15121.608729\n",
      "iteration 814, current loss: 15117.804097\n",
      "iteration 815, current loss: 15113.997356\n",
      "iteration 816, current loss: 15110.188077\n",
      "iteration 817, current loss: 15106.375841\n",
      "iteration 818, current loss: 15102.560245\n",
      "iteration 819, current loss: 15098.740909\n",
      "iteration 820, current loss: 15094.917478\n",
      "iteration 821, current loss: 15091.089631\n",
      "iteration 822, current loss: 15087.257092\n",
      "iteration 823, current loss: 15083.419635\n",
      "iteration 824, current loss: 15079.577097\n",
      "iteration 825, current loss: 15075.729391\n",
      "iteration 826, current loss: 15071.876518\n",
      "iteration 827, current loss: 15068.018580\n",
      "iteration 828, current loss: 15064.155793\n",
      "iteration 829, current loss: 15060.288495\n",
      "iteration 830, current loss: 15056.417149\n",
      "iteration 831, current loss: 15052.542346\n",
      "iteration 832, current loss: 15048.664794\n",
      "iteration 833, current loss: 15044.785302\n",
      "iteration 834, current loss: 15040.904760\n",
      "iteration 835, current loss: 15037.024104\n",
      "iteration 836, current loss: 15033.144288\n",
      "iteration 837, current loss: 15029.266252\n",
      "iteration 838, current loss: 15025.390886\n",
      "iteration 839, current loss: 15021.519013\n",
      "iteration 840, current loss: 15017.651366\n",
      "iteration 841, current loss: 15013.788582\n",
      "iteration 842, current loss: 15009.931201\n",
      "iteration 843, current loss: 15006.079667\n",
      "iteration 844, current loss: 15002.234341\n",
      "iteration 845, current loss: 14998.395508\n",
      "iteration 846, current loss: 14994.563389\n",
      "iteration 847, current loss: 14990.738157\n",
      "iteration 848, current loss: 14986.919937\n",
      "iteration 849, current loss: 14983.108824\n",
      "iteration 850, current loss: 14979.304879\n",
      "iteration 851, current loss: 14975.508141\n",
      "iteration 852, current loss: 14971.718624\n",
      "iteration 853, current loss: 14967.936318\n",
      "iteration 854, current loss: 14964.161193\n",
      "iteration 855, current loss: 14960.393194\n",
      "iteration 856, current loss: 14956.632244\n",
      "iteration 857, current loss: 14952.878240\n",
      "iteration 858, current loss: 14949.131053\n",
      "iteration 859, current loss: 14945.390525\n",
      "iteration 860, current loss: 14941.656471\n",
      "iteration 861, current loss: 14937.928677\n",
      "iteration 862, current loss: 14934.206898\n",
      "iteration 863, current loss: 14930.490860\n",
      "iteration 864, current loss: 14926.780260\n",
      "iteration 865, current loss: 14923.074764\n",
      "iteration 866, current loss: 14919.374015\n",
      "iteration 867, current loss: 14915.677633\n",
      "iteration 868, current loss: 14911.985223\n",
      "iteration 869, current loss: 14908.296390\n",
      "iteration 870, current loss: 14904.610745\n",
      "iteration 871, current loss: 14900.927935\n",
      "iteration 872, current loss: 14897.247666\n",
      "iteration 873, current loss: 14893.569733\n",
      "iteration 874, current loss: 14889.894063\n",
      "iteration 875, current loss: 14886.220741\n",
      "iteration 876, current loss: 14882.550047\n",
      "iteration 877, current loss: 14878.882464\n",
      "iteration 878, current loss: 14875.218676\n",
      "iteration 879, current loss: 14871.559524\n",
      "iteration 880, current loss: 14867.905951\n",
      "iteration 881, current loss: 14864.258920\n",
      "iteration 882, current loss: 14860.619331\n",
      "iteration 883, current loss: 14856.987948\n",
      "iteration 884, current loss: 14853.365352\n",
      "iteration 885, current loss: 14849.751924\n",
      "iteration 886, current loss: 14846.147846\n",
      "iteration 887, current loss: 14842.553134\n",
      "iteration 888, current loss: 14838.967671\n",
      "iteration 889, current loss: 14835.391240\n",
      "iteration 890, current loss: 14831.823565\n",
      "iteration 891, current loss: 14828.264334\n",
      "iteration 892, current loss: 14824.713215\n",
      "iteration 893, current loss: 14821.169877\n",
      "iteration 894, current loss: 14817.633991\n",
      "iteration 895, current loss: 14814.105238\n",
      "iteration 896, current loss: 14810.583310\n",
      "iteration 897, current loss: 14807.067910\n",
      "iteration 898, current loss: 14803.558752\n",
      "iteration 899, current loss: 14800.055559\n",
      "iteration 900, current loss: 14796.558061\n",
      "iteration 901, current loss: 14793.065995\n",
      "iteration 902, current loss: 14789.579102\n",
      "iteration 903, current loss: 14786.097127\n",
      "iteration 904, current loss: 14782.619818\n",
      "iteration 905, current loss: 14779.146922\n",
      "iteration 906, current loss: 14775.678191\n",
      "iteration 907, current loss: 14772.213372\n",
      "iteration 908, current loss: 14768.752216\n",
      "iteration 909, current loss: 14765.294471\n",
      "iteration 910, current loss: 14761.839886\n",
      "iteration 911, current loss: 14758.388207\n",
      "iteration 912, current loss: 14754.939180\n",
      "iteration 913, current loss: 14751.492550\n",
      "iteration 914, current loss: 14748.048062\n",
      "iteration 915, current loss: 14744.605459\n",
      "iteration 916, current loss: 14741.164487\n",
      "iteration 917, current loss: 14737.724892\n",
      "iteration 918, current loss: 14734.286422\n",
      "iteration 919, current loss: 14730.848829\n",
      "iteration 920, current loss: 14727.411873\n",
      "iteration 921, current loss: 14723.975319\n",
      "iteration 922, current loss: 14720.538945\n",
      "iteration 923, current loss: 14717.102539\n",
      "iteration 924, current loss: 14713.665909\n",
      "iteration 925, current loss: 14710.228879\n",
      "iteration 926, current loss: 14706.791297\n",
      "iteration 927, current loss: 14703.353041\n",
      "iteration 928, current loss: 14699.914018\n",
      "iteration 929, current loss: 14696.474177\n",
      "iteration 930, current loss: 14693.033515\n",
      "iteration 931, current loss: 14689.592086\n",
      "iteration 932, current loss: 14686.150018\n",
      "iteration 933, current loss: 14682.707524\n",
      "iteration 934, current loss: 14679.264918\n",
      "iteration 935, current loss: 14675.822629\n",
      "iteration 936, current loss: 14672.381208\n",
      "iteration 937, current loss: 14668.941326\n",
      "iteration 938, current loss: 14665.503761\n",
      "iteration 939, current loss: 14662.069374\n",
      "iteration 940, current loss: 14658.639068\n",
      "iteration 941, current loss: 14655.213738\n",
      "iteration 942, current loss: 14651.794219\n",
      "iteration 943, current loss: 14648.381243\n",
      "iteration 944, current loss: 14644.975398\n",
      "iteration 945, current loss: 14641.577110\n",
      "iteration 946, current loss: 14638.186638\n",
      "iteration 947, current loss: 14634.804084\n",
      "iteration 948, current loss: 14631.429415\n",
      "iteration 949, current loss: 14628.062488\n",
      "iteration 950, current loss: 14624.703081\n",
      "iteration 951, current loss: 14621.350918\n",
      "iteration 952, current loss: 14618.005694\n",
      "iteration 953, current loss: 14614.667091\n",
      "iteration 954, current loss: 14611.334796\n",
      "iteration 955, current loss: 14608.008513\n",
      "iteration 956, current loss: 14604.687963\n",
      "iteration 957, current loss: 14601.372899\n",
      "iteration 958, current loss: 14598.063099\n",
      "iteration 959, current loss: 14594.758375\n",
      "iteration 960, current loss: 14591.458566\n",
      "iteration 961, current loss: 14588.163544\n",
      "iteration 962, current loss: 14584.873205\n",
      "iteration 963, current loss: 14581.587470\n",
      "iteration 964, current loss: 14578.306285\n",
      "iteration 965, current loss: 14575.029610\n",
      "iteration 966, current loss: 14571.757420\n",
      "iteration 967, current loss: 14568.489701\n",
      "iteration 968, current loss: 14565.226443\n",
      "iteration 969, current loss: 14561.967636\n",
      "iteration 970, current loss: 14558.713269\n",
      "iteration 971, current loss: 14555.463320\n",
      "iteration 972, current loss: 14552.217762\n",
      "iteration 973, current loss: 14548.976550\n",
      "iteration 974, current loss: 14545.739626\n",
      "iteration 975, current loss: 14542.506918\n",
      "iteration 976, current loss: 14539.278336\n",
      "iteration 977, current loss: 14536.053775\n",
      "iteration 978, current loss: 14532.833114\n",
      "iteration 979, current loss: 14529.616221\n",
      "iteration 980, current loss: 14526.402952\n",
      "iteration 981, current loss: 14523.193157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 982, current loss: 14519.986678\n",
      "iteration 983, current loss: 14516.783359\n",
      "iteration 984, current loss: 14513.583041\n",
      "iteration 985, current loss: 14510.385574\n",
      "iteration 986, current loss: 14507.190811\n",
      "iteration 987, current loss: 14503.998623\n",
      "iteration 988, current loss: 14500.808889\n",
      "iteration 989, current loss: 14497.621513\n",
      "iteration 990, current loss: 14494.436415\n",
      "iteration 991, current loss: 14491.253544\n",
      "iteration 992, current loss: 14488.072874\n",
      "iteration 993, current loss: 14484.894413\n",
      "iteration 994, current loss: 14481.718200\n",
      "iteration 995, current loss: 14478.544312\n",
      "iteration 996, current loss: 14475.372865\n",
      "iteration 997, current loss: 14472.204010\n",
      "iteration 998, current loss: 14469.037940\n",
      "iteration 999, current loss: 14465.874882\n",
      "iteration 1000, current loss: 14462.715098\n",
      "iteration 1001, current loss: 14459.558875\n",
      "iteration 1002, current loss: 14456.406524\n",
      "iteration 1003, current loss: 14453.258366\n",
      "iteration 1004, current loss: 14450.114725\n",
      "iteration 1005, current loss: 14446.975915\n",
      "iteration 1006, current loss: 14443.842235\n",
      "iteration 1007, current loss: 14440.713954\n",
      "iteration 1008, current loss: 14437.591303\n",
      "iteration 1009, current loss: 14434.474472\n",
      "iteration 1010, current loss: 14431.363602\n",
      "iteration 1011, current loss: 14428.258786\n",
      "iteration 1012, current loss: 14425.160067\n",
      "iteration 1013, current loss: 14422.067441\n",
      "iteration 1014, current loss: 14418.980860\n",
      "iteration 1015, current loss: 14415.900240\n",
      "iteration 1016, current loss: 14412.825462\n",
      "iteration 1017, current loss: 14409.756384\n",
      "iteration 1018, current loss: 14406.692843\n",
      "iteration 1019, current loss: 14403.634664\n",
      "iteration 1020, current loss: 14400.581662\n",
      "iteration 1021, current loss: 14397.533655\n",
      "iteration 1022, current loss: 14394.490462\n",
      "iteration 1023, current loss: 14391.451908\n",
      "iteration 1024, current loss: 14388.417833\n",
      "iteration 1025, current loss: 14385.388090\n",
      "iteration 1026, current loss: 14382.362547\n",
      "iteration 1027, current loss: 14379.341092\n",
      "iteration 1028, current loss: 14376.323633\n",
      "iteration 1029, current loss: 14373.310097\n",
      "iteration 1030, current loss: 14370.300429\n",
      "iteration 1031, current loss: 14367.294597\n",
      "iteration 1032, current loss: 14364.292580\n",
      "iteration 1033, current loss: 14361.294379\n",
      "iteration 1034, current loss: 14358.300002\n",
      "iteration 1035, current loss: 14355.309472\n",
      "iteration 1036, current loss: 14352.322816\n",
      "iteration 1037, current loss: 14349.340066\n",
      "iteration 1038, current loss: 14346.361258\n",
      "iteration 1039, current loss: 14343.386421\n",
      "iteration 1040, current loss: 14340.415583\n",
      "iteration 1041, current loss: 14337.448764\n",
      "iteration 1042, current loss: 14334.485972\n",
      "iteration 1043, current loss: 14331.527206\n",
      "iteration 1044, current loss: 14328.572451\n",
      "iteration 1045, current loss: 14325.621676\n",
      "iteration 1046, current loss: 14322.674837\n",
      "iteration 1047, current loss: 14319.731875\n",
      "iteration 1048, current loss: 14316.792715\n",
      "iteration 1049, current loss: 14313.857269\n",
      "iteration 1050, current loss: 14310.925435\n",
      "iteration 1051, current loss: 14307.997102\n",
      "iteration 1052, current loss: 14305.072147\n",
      "iteration 1053, current loss: 14302.150440\n",
      "iteration 1054, current loss: 14299.231846\n",
      "iteration 1055, current loss: 14296.316228\n",
      "iteration 1056, current loss: 14293.403446\n",
      "iteration 1057, current loss: 14290.493363\n",
      "iteration 1058, current loss: 14287.585849\n",
      "iteration 1059, current loss: 14284.680777\n",
      "iteration 1060, current loss: 14281.778032\n",
      "iteration 1061, current loss: 14278.877508\n",
      "iteration 1062, current loss: 14275.979115\n",
      "iteration 1063, current loss: 14273.082774\n",
      "iteration 1064, current loss: 14270.188422\n",
      "iteration 1065, current loss: 14267.296014\n",
      "iteration 1066, current loss: 14264.405516\n",
      "iteration 1067, current loss: 14261.516915\n",
      "iteration 1068, current loss: 14258.630208\n",
      "iteration 1069, current loss: 14255.745409\n",
      "iteration 1070, current loss: 14252.862542\n",
      "iteration 1071, current loss: 14249.981642\n",
      "iteration 1072, current loss: 14247.102756\n",
      "iteration 1073, current loss: 14244.225935\n",
      "iteration 1074, current loss: 14241.351238\n",
      "iteration 1075, current loss: 14238.478727\n",
      "iteration 1076, current loss: 14235.608468\n",
      "iteration 1077, current loss: 14232.740528\n",
      "iteration 1078, current loss: 14229.874975\n",
      "iteration 1079, current loss: 14227.011874\n",
      "iteration 1080, current loss: 14224.151288\n",
      "iteration 1081, current loss: 14221.293275\n",
      "iteration 1082, current loss: 14218.437889\n",
      "iteration 1083, current loss: 14215.585175\n",
      "iteration 1084, current loss: 14212.735170\n",
      "iteration 1085, current loss: 14209.887900\n",
      "iteration 1086, current loss: 14207.043380\n",
      "iteration 1087, current loss: 14204.201610\n",
      "iteration 1088, current loss: 14201.362575\n",
      "iteration 1089, current loss: 14198.526247\n",
      "iteration 1090, current loss: 14195.692576\n",
      "iteration 1091, current loss: 14192.861498\n",
      "iteration 1092, current loss: 14190.032931\n",
      "iteration 1093, current loss: 14187.206773\n",
      "iteration 1094, current loss: 14184.382905\n",
      "iteration 1095, current loss: 14181.561193\n",
      "iteration 1096, current loss: 14178.741485\n",
      "iteration 1097, current loss: 14175.923615\n",
      "iteration 1098, current loss: 14173.107405\n",
      "iteration 1099, current loss: 14170.292666\n",
      "iteration 1100, current loss: 14167.479200\n",
      "iteration 1101, current loss: 14164.666805\n",
      "iteration 1102, current loss: 14161.855276\n",
      "iteration 1103, current loss: 14159.044410\n",
      "iteration 1104, current loss: 14156.234010\n",
      "iteration 1105, current loss: 14153.423888\n",
      "iteration 1106, current loss: 14150.613870\n",
      "iteration 1107, current loss: 14147.803804\n",
      "iteration 1108, current loss: 14144.993561\n",
      "iteration 1109, current loss: 14142.183041\n",
      "iteration 1110, current loss: 14139.372178\n",
      "iteration 1111, current loss: 14136.560948\n",
      "iteration 1112, current loss: 14133.749364\n",
      "iteration 1113, current loss: 14130.937486\n",
      "iteration 1114, current loss: 14128.125416\n",
      "iteration 1115, current loss: 14125.313300\n",
      "iteration 1116, current loss: 14122.501321\n",
      "iteration 1117, current loss: 14119.689699\n",
      "iteration 1118, current loss: 14116.878677\n",
      "iteration 1119, current loss: 14114.068519\n",
      "iteration 1120, current loss: 14111.259493\n",
      "iteration 1121, current loss: 14108.451865\n",
      "iteration 1122, current loss: 14105.645888\n",
      "iteration 1123, current loss: 14102.841791\n",
      "iteration 1124, current loss: 14100.039772\n",
      "iteration 1125, current loss: 14097.239991\n",
      "iteration 1126, current loss: 14094.442568\n",
      "iteration 1127, current loss: 14091.647576\n",
      "iteration 1128, current loss: 14088.855046\n",
      "iteration 1129, current loss: 14086.064964\n",
      "iteration 1130, current loss: 14083.277278\n",
      "iteration 1131, current loss: 14080.491900\n",
      "iteration 1132, current loss: 14077.708711\n",
      "iteration 1133, current loss: 14074.927565\n",
      "iteration 1134, current loss: 14072.148298\n",
      "iteration 1135, current loss: 14069.370733\n",
      "iteration 1136, current loss: 14066.594681\n",
      "iteration 1137, current loss: 14063.819955\n",
      "iteration 1138, current loss: 14061.046364\n",
      "iteration 1139, current loss: 14058.273725\n",
      "iteration 1140, current loss: 14055.501866\n",
      "iteration 1141, current loss: 14052.730622\n",
      "iteration 1142, current loss: 14049.959846\n",
      "iteration 1143, current loss: 14047.189404\n",
      "iteration 1144, current loss: 14044.419178\n",
      "iteration 1145, current loss: 14041.649064\n",
      "iteration 1146, current loss: 14038.878972\n",
      "iteration 1147, current loss: 14036.108824\n",
      "iteration 1148, current loss: 14033.338551\n",
      "iteration 1149, current loss: 14030.568092\n",
      "iteration 1150, current loss: 14027.797389\n",
      "iteration 1151, current loss: 14025.026384\n",
      "iteration 1152, current loss: 14022.255017\n",
      "iteration 1153, current loss: 14019.483225\n",
      "iteration 1154, current loss: 14016.710935\n",
      "iteration 1155, current loss: 14013.938066\n",
      "iteration 1156, current loss: 14011.164527\n",
      "iteration 1157, current loss: 14008.390212\n",
      "iteration 1158, current loss: 14005.615007\n",
      "iteration 1159, current loss: 14002.838783\n",
      "iteration 1160, current loss: 14000.061402\n",
      "iteration 1161, current loss: 13997.282712\n",
      "iteration 1162, current loss: 13994.502556\n",
      "iteration 1163, current loss: 13991.720767\n",
      "iteration 1164, current loss: 13988.937172\n",
      "iteration 1165, current loss: 13986.151594\n",
      "iteration 1166, current loss: 13983.363858\n",
      "iteration 1167, current loss: 13980.573785\n",
      "iteration 1168, current loss: 13977.781204\n",
      "iteration 1169, current loss: 13974.985947\n",
      "iteration 1170, current loss: 13972.187857\n",
      "iteration 1171, current loss: 13969.386790\n",
      "iteration 1172, current loss: 13966.582615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1173, current loss: 13963.775223\n",
      "iteration 1174, current loss: 13960.964527\n",
      "iteration 1175, current loss: 13958.150467\n",
      "iteration 1176, current loss: 13955.333014\n",
      "iteration 1177, current loss: 13952.512173\n",
      "iteration 1178, current loss: 13949.687984\n",
      "iteration 1179, current loss: 13946.860530\n",
      "iteration 1180, current loss: 13944.029931\n",
      "iteration 1181, current loss: 13941.196349\n",
      "iteration 1182, current loss: 13938.359984\n",
      "iteration 1183, current loss: 13935.521069\n",
      "iteration 1184, current loss: 13932.679864\n",
      "iteration 1185, current loss: 13929.836651\n",
      "iteration 1186, current loss: 13926.991720\n",
      "iteration 1187, current loss: 13924.145364\n",
      "iteration 1188, current loss: 13921.297865\n",
      "iteration 1189, current loss: 13918.449488\n",
      "iteration 1190, current loss: 13915.600473\n",
      "iteration 1191, current loss: 13912.751028\n",
      "iteration 1192, current loss: 13909.901332\n",
      "iteration 1193, current loss: 13907.051530\n",
      "iteration 1194, current loss: 13904.201744\n",
      "iteration 1195, current loss: 13901.352076\n",
      "iteration 1196, current loss: 13898.502622\n",
      "iteration 1197, current loss: 13895.653485\n",
      "iteration 1198, current loss: 13892.804791\n",
      "iteration 1199, current loss: 13889.956704\n",
      "iteration 1200, current loss: 13887.109447\n",
      "iteration 1201, current loss: 13884.263315\n",
      "iteration 1202, current loss: 13881.418688\n",
      "iteration 1203, current loss: 13878.576036\n",
      "iteration 1204, current loss: 13875.735912\n",
      "iteration 1205, current loss: 13872.898945\n",
      "iteration 1206, current loss: 13870.065811\n",
      "iteration 1207, current loss: 13867.237202\n",
      "iteration 1208, current loss: 13864.413786\n",
      "iteration 1209, current loss: 13861.596177\n",
      "iteration 1210, current loss: 13858.784896\n",
      "iteration 1211, current loss: 13855.980355\n",
      "iteration 1212, current loss: 13853.182844\n",
      "iteration 1213, current loss: 13850.392535\n",
      "iteration 1214, current loss: 13847.609490\n",
      "iteration 1215, current loss: 13844.833677\n",
      "iteration 1216, current loss: 13842.064992\n",
      "iteration 1217, current loss: 13839.303271\n",
      "iteration 1218, current loss: 13836.548312\n",
      "iteration 1219, current loss: 13833.799886\n",
      "iteration 1220, current loss: 13831.057744\n",
      "iteration 1221, current loss: 13828.321631\n",
      "iteration 1222, current loss: 13825.591289\n",
      "iteration 1223, current loss: 13822.866461\n",
      "iteration 1224, current loss: 13820.146892\n",
      "iteration 1225, current loss: 13817.432334\n",
      "iteration 1226, current loss: 13814.722545\n",
      "iteration 1227, current loss: 13812.017292\n",
      "iteration 1228, current loss: 13809.316350\n",
      "iteration 1229, current loss: 13806.619502\n",
      "iteration 1230, current loss: 13803.926543\n",
      "iteration 1231, current loss: 13801.237275\n",
      "iteration 1232, current loss: 13798.551514\n",
      "iteration 1233, current loss: 13795.869084\n",
      "iteration 1234, current loss: 13793.189823\n",
      "iteration 1235, current loss: 13790.513579\n",
      "iteration 1236, current loss: 13787.840213\n",
      "iteration 1237, current loss: 13785.169598\n",
      "iteration 1238, current loss: 13782.501619\n",
      "iteration 1239, current loss: 13779.836173\n",
      "iteration 1240, current loss: 13777.173168\n",
      "iteration 1241, current loss: 13774.512527\n",
      "iteration 1242, current loss: 13771.854177\n",
      "iteration 1243, current loss: 13769.198059\n",
      "iteration 1244, current loss: 13766.544118\n",
      "iteration 1245, current loss: 13763.892306\n",
      "iteration 1246, current loss: 13761.242576\n",
      "iteration 1247, current loss: 13758.594881\n",
      "iteration 1248, current loss: 13755.949174\n",
      "iteration 1249, current loss: 13753.305399\n",
      "iteration 1250, current loss: 13750.663494\n",
      "iteration 1251, current loss: 13748.023388\n",
      "iteration 1252, current loss: 13745.385001\n",
      "iteration 1253, current loss: 13742.748243\n",
      "iteration 1254, current loss: 13740.113019\n",
      "iteration 1255, current loss: 13737.479236\n",
      "iteration 1256, current loss: 13734.846806\n",
      "iteration 1257, current loss: 13732.215658\n",
      "iteration 1258, current loss: 13729.585754\n",
      "iteration 1259, current loss: 13726.957096\n",
      "iteration 1260, current loss: 13724.329745\n",
      "iteration 1261, current loss: 13721.703828\n",
      "iteration 1262, current loss: 13719.079547\n",
      "iteration 1263, current loss: 13716.457184\n",
      "iteration 1264, current loss: 13713.837086\n",
      "iteration 1265, current loss: 13711.219659\n",
      "iteration 1266, current loss: 13708.605341\n",
      "iteration 1267, current loss: 13705.994576\n",
      "iteration 1268, current loss: 13703.387788\n",
      "iteration 1269, current loss: 13700.785354\n",
      "iteration 1270, current loss: 13698.187582\n",
      "iteration 1271, current loss: 13695.594704\n",
      "iteration 1272, current loss: 13693.006870\n",
      "iteration 1273, current loss: 13690.424152\n",
      "iteration 1274, current loss: 13687.846553\n",
      "iteration 1275, current loss: 13685.274021\n",
      "iteration 1276, current loss: 13682.706463\n",
      "iteration 1277, current loss: 13680.143756\n",
      "iteration 1278, current loss: 13677.585761\n",
      "iteration 1279, current loss: 13675.032335\n",
      "iteration 1280, current loss: 13672.483338\n",
      "iteration 1281, current loss: 13669.938636\n",
      "iteration 1282, current loss: 13667.398112\n",
      "iteration 1283, current loss: 13664.861664\n",
      "iteration 1284, current loss: 13662.329209\n",
      "iteration 1285, current loss: 13659.800681\n",
      "iteration 1286, current loss: 13657.276033\n",
      "iteration 1287, current loss: 13654.755235\n",
      "iteration 1288, current loss: 13652.238272\n",
      "iteration 1289, current loss: 13649.725142\n",
      "iteration 1290, current loss: 13647.215857\n",
      "iteration 1291, current loss: 13644.710434\n",
      "iteration 1292, current loss: 13642.208901\n",
      "iteration 1293, current loss: 13639.711289\n",
      "iteration 1294, current loss: 13637.217633\n",
      "iteration 1295, current loss: 13634.727970\n",
      "iteration 1296, current loss: 13632.242336\n",
      "iteration 1297, current loss: 13629.760767\n",
      "iteration 1298, current loss: 13627.283298\n",
      "iteration 1299, current loss: 13624.809960\n",
      "iteration 1300, current loss: 13622.340781\n",
      "iteration 1301, current loss: 13619.875787\n",
      "iteration 1302, current loss: 13617.414999\n",
      "iteration 1303, current loss: 13614.958434\n",
      "iteration 1304, current loss: 13612.506104\n",
      "iteration 1305, current loss: 13610.058018\n",
      "iteration 1306, current loss: 13607.614180\n",
      "iteration 1307, current loss: 13605.174591\n",
      "iteration 1308, current loss: 13602.739244\n",
      "iteration 1309, current loss: 13600.308132\n",
      "iteration 1310, current loss: 13597.881241\n",
      "iteration 1311, current loss: 13595.458551\n",
      "iteration 1312, current loss: 13593.040041\n",
      "iteration 1313, current loss: 13590.625682\n",
      "iteration 1314, current loss: 13588.215442\n",
      "iteration 1315, current loss: 13585.809285\n",
      "iteration 1316, current loss: 13583.407169\n",
      "iteration 1317, current loss: 13581.009048\n",
      "iteration 1318, current loss: 13578.614871\n",
      "iteration 1319, current loss: 13576.224584\n",
      "iteration 1320, current loss: 13573.838127\n",
      "iteration 1321, current loss: 13571.455435\n",
      "iteration 1322, current loss: 13569.076443\n",
      "iteration 1323, current loss: 13566.701077\n",
      "iteration 1324, current loss: 13564.329263\n",
      "iteration 1325, current loss: 13561.960922\n",
      "iteration 1326, current loss: 13559.595971\n",
      "iteration 1327, current loss: 13557.234326\n",
      "iteration 1328, current loss: 13554.875899\n",
      "iteration 1329, current loss: 13552.520600\n",
      "iteration 1330, current loss: 13550.168334\n",
      "iteration 1331, current loss: 13547.819009\n",
      "iteration 1332, current loss: 13545.472527\n",
      "iteration 1333, current loss: 13543.128790\n",
      "iteration 1334, current loss: 13540.787697\n",
      "iteration 1335, current loss: 13538.449149\n",
      "iteration 1336, current loss: 13536.113043\n",
      "iteration 1337, current loss: 13533.779275\n",
      "iteration 1338, current loss: 13531.447741\n",
      "iteration 1339, current loss: 13529.118338\n",
      "iteration 1340, current loss: 13526.790958\n",
      "iteration 1341, current loss: 13524.465497\n",
      "iteration 1342, current loss: 13522.141848\n",
      "iteration 1343, current loss: 13519.819904\n",
      "iteration 1344, current loss: 13517.499559\n",
      "iteration 1345, current loss: 13515.180704\n",
      "iteration 1346, current loss: 13512.863234\n",
      "iteration 1347, current loss: 13510.547041\n",
      "iteration 1348, current loss: 13508.232020\n",
      "iteration 1349, current loss: 13505.918064\n",
      "iteration 1350, current loss: 13503.605070\n",
      "iteration 1351, current loss: 13501.292936\n",
      "iteration 1352, current loss: 13498.981562\n",
      "iteration 1353, current loss: 13496.670850\n",
      "iteration 1354, current loss: 13494.360708\n",
      "iteration 1355, current loss: 13492.051046\n",
      "iteration 1356, current loss: 13489.741784\n",
      "iteration 1357, current loss: 13487.432845\n",
      "iteration 1358, current loss: 13485.124163\n",
      "iteration 1359, current loss: 13482.815682\n",
      "iteration 1360, current loss: 13480.507359\n",
      "iteration 1361, current loss: 13478.199162\n",
      "iteration 1362, current loss: 13475.891077\n",
      "iteration 1363, current loss: 13473.583107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1364, current loss: 13471.275272\n",
      "iteration 1365, current loss: 13468.967615\n",
      "iteration 1366, current loss: 13466.660199\n",
      "iteration 1367, current loss: 13464.353108\n",
      "iteration 1368, current loss: 13462.046450\n",
      "iteration 1369, current loss: 13459.740351\n",
      "iteration 1370, current loss: 13457.434960\n",
      "iteration 1371, current loss: 13455.130441\n",
      "iteration 1372, current loss: 13452.826973\n",
      "iteration 1373, current loss: 13450.524747\n",
      "iteration 1374, current loss: 13448.223961\n",
      "iteration 1375, current loss: 13445.924815\n",
      "iteration 1376, current loss: 13443.627510\n",
      "iteration 1377, current loss: 13441.332241\n",
      "iteration 1378, current loss: 13439.039196\n",
      "iteration 1379, current loss: 13436.748550\n",
      "iteration 1380, current loss: 13434.460466\n",
      "iteration 1381, current loss: 13432.175091\n",
      "iteration 1382, current loss: 13429.892555\n",
      "iteration 1383, current loss: 13427.612970\n",
      "iteration 1384, current loss: 13425.336432\n",
      "iteration 1385, current loss: 13423.063020\n",
      "iteration 1386, current loss: 13420.792795\n",
      "iteration 1387, current loss: 13418.525803\n",
      "iteration 1388, current loss: 13416.262076\n",
      "iteration 1389, current loss: 13414.001632\n",
      "iteration 1390, current loss: 13411.744475\n",
      "iteration 1391, current loss: 13409.490598\n",
      "iteration 1392, current loss: 13407.239986\n",
      "iteration 1393, current loss: 13404.992611\n",
      "iteration 1394, current loss: 13402.748439\n",
      "iteration 1395, current loss: 13400.507427\n",
      "iteration 1396, current loss: 13398.269526\n",
      "iteration 1397, current loss: 13396.034682\n",
      "iteration 1398, current loss: 13393.802834\n",
      "iteration 1399, current loss: 13391.573918\n",
      "iteration 1400, current loss: 13389.347865\n",
      "iteration 1401, current loss: 13387.124602\n",
      "iteration 1402, current loss: 13384.904055\n",
      "iteration 1403, current loss: 13382.686145\n",
      "iteration 1404, current loss: 13380.470791\n",
      "iteration 1405, current loss: 13378.257910\n",
      "iteration 1406, current loss: 13376.047416\n",
      "iteration 1407, current loss: 13373.839223\n",
      "iteration 1408, current loss: 13371.633238\n",
      "iteration 1409, current loss: 13369.429371\n",
      "iteration 1410, current loss: 13367.227526\n",
      "iteration 1411, current loss: 13365.027606\n",
      "iteration 1412, current loss: 13362.829509\n",
      "iteration 1413, current loss: 13360.633132\n",
      "iteration 1414, current loss: 13358.438365\n",
      "iteration 1415, current loss: 13356.245098\n",
      "iteration 1416, current loss: 13354.053212\n",
      "iteration 1417, current loss: 13351.862585\n",
      "iteration 1418, current loss: 13349.673087\n",
      "iteration 1419, current loss: 13347.484584\n",
      "iteration 1420, current loss: 13345.296932\n",
      "iteration 1421, current loss: 13343.109979\n",
      "iteration 1422, current loss: 13340.923565\n",
      "iteration 1423, current loss: 13338.737520\n",
      "iteration 1424, current loss: 13336.551662\n",
      "iteration 1425, current loss: 13334.365799\n",
      "iteration 1426, current loss: 13332.179725\n",
      "iteration 1427, current loss: 13329.993223\n",
      "iteration 1428, current loss: 13327.806061\n",
      "iteration 1429, current loss: 13325.617994\n",
      "iteration 1430, current loss: 13323.428761\n",
      "iteration 1431, current loss: 13321.238087\n",
      "iteration 1432, current loss: 13319.045684\n",
      "iteration 1433, current loss: 13316.851247\n",
      "iteration 1434, current loss: 13314.654461\n",
      "iteration 1435, current loss: 13312.454999\n",
      "iteration 1436, current loss: 13310.252523\n",
      "iteration 1437, current loss: 13308.046688\n",
      "iteration 1438, current loss: 13305.837145\n",
      "iteration 1439, current loss: 13303.623543\n",
      "iteration 1440, current loss: 13301.405531\n",
      "iteration 1441, current loss: 13299.182761\n",
      "iteration 1442, current loss: 13296.954889\n",
      "iteration 1443, current loss: 13294.721573\n",
      "iteration 1444, current loss: 13292.482474\n",
      "iteration 1445, current loss: 13290.237254\n",
      "iteration 1446, current loss: 13287.985577\n",
      "iteration 1447, current loss: 13285.727114\n",
      "iteration 1448, current loss: 13283.461563\n",
      "iteration 1449, current loss: 13281.188680\n",
      "iteration 1450, current loss: 13278.908330\n",
      "iteration 1451, current loss: 13276.620572\n",
      "iteration 1452, current loss: 13274.325755\n",
      "iteration 1453, current loss: 13272.024629\n",
      "iteration 1454, current loss: 13269.718443\n",
      "iteration 1455, current loss: 13267.408993\n",
      "iteration 1456, current loss: 13265.098589\n",
      "iteration 1457, current loss: 13262.789923\n",
      "iteration 1458, current loss: 13260.485834\n",
      "iteration 1459, current loss: 13258.189032\n",
      "iteration 1460, current loss: 13255.901844\n",
      "iteration 1461, current loss: 13253.626036\n",
      "iteration 1462, current loss: 13251.362756\n",
      "iteration 1463, current loss: 13249.112570\n",
      "iteration 1464, current loss: 13246.875565\n",
      "iteration 1465, current loss: 13244.651475\n",
      "iteration 1466, current loss: 13242.439794\n",
      "iteration 1467, current loss: 13240.239878\n",
      "iteration 1468, current loss: 13238.051007\n",
      "iteration 1469, current loss: 13235.872439\n",
      "iteration 1470, current loss: 13233.703440\n",
      "iteration 1471, current loss: 13231.543299\n",
      "iteration 1472, current loss: 13229.391342\n",
      "iteration 1473, current loss: 13227.246935\n",
      "iteration 1474, current loss: 13225.109488\n",
      "iteration 1475, current loss: 13222.978452\n",
      "iteration 1476, current loss: 13220.853319\n",
      "iteration 1477, current loss: 13218.733621\n",
      "iteration 1478, current loss: 13216.618925\n",
      "iteration 1479, current loss: 13214.508831\n",
      "iteration 1480, current loss: 13212.402969\n",
      "iteration 1481, current loss: 13210.301000\n",
      "iteration 1482, current loss: 13208.202607\n",
      "iteration 1483, current loss: 13206.107499\n",
      "iteration 1484, current loss: 13204.015404\n",
      "iteration 1485, current loss: 13201.926070\n",
      "iteration 1486, current loss: 13199.839263\n",
      "iteration 1487, current loss: 13197.754765\n",
      "iteration 1488, current loss: 13195.672371\n",
      "iteration 1489, current loss: 13193.591889\n",
      "iteration 1490, current loss: 13191.513139\n",
      "iteration 1491, current loss: 13189.435952\n",
      "iteration 1492, current loss: 13187.360168\n",
      "iteration 1493, current loss: 13185.285636\n",
      "iteration 1494, current loss: 13183.212210\n",
      "iteration 1495, current loss: 13181.139753\n",
      "iteration 1496, current loss: 13179.068132\n",
      "iteration 1497, current loss: 13176.997220\n",
      "iteration 1498, current loss: 13174.926892\n",
      "iteration 1499, current loss: 13172.857027\n",
      "iteration 1500, current loss: 13170.787508\n",
      "iteration 1501, current loss: 13168.718215\n",
      "iteration 1502, current loss: 13166.649032\n",
      "iteration 1503, current loss: 13164.579842\n",
      "iteration 1504, current loss: 13162.510527\n",
      "iteration 1505, current loss: 13160.440966\n",
      "iteration 1506, current loss: 13158.371037\n",
      "iteration 1507, current loss: 13156.300614\n",
      "iteration 1508, current loss: 13154.229565\n",
      "iteration 1509, current loss: 13152.157755\n",
      "iteration 1510, current loss: 13150.085042\n",
      "iteration 1511, current loss: 13148.011276\n",
      "iteration 1512, current loss: 13145.936299\n",
      "iteration 1513, current loss: 13143.859945\n",
      "iteration 1514, current loss: 13141.782036\n",
      "iteration 1515, current loss: 13139.702381\n",
      "iteration 1516, current loss: 13137.620779\n",
      "iteration 1517, current loss: 13135.537009\n",
      "iteration 1518, current loss: 13133.450838\n",
      "iteration 1519, current loss: 13131.362011\n",
      "iteration 1520, current loss: 13129.270256\n",
      "iteration 1521, current loss: 13127.175275\n",
      "iteration 1522, current loss: 13125.076751\n",
      "iteration 1523, current loss: 13122.974337\n",
      "iteration 1524, current loss: 13120.867664\n",
      "iteration 1525, current loss: 13118.756335\n",
      "iteration 1526, current loss: 13116.639926\n",
      "iteration 1527, current loss: 13114.517991\n",
      "iteration 1528, current loss: 13112.390068\n",
      "iteration 1529, current loss: 13110.255682\n",
      "iteration 1530, current loss: 13108.114365\n",
      "iteration 1531, current loss: 13105.965671\n",
      "iteration 1532, current loss: 13103.809205\n",
      "iteration 1533, current loss: 13101.644666\n",
      "iteration 1534, current loss: 13099.471888\n",
      "iteration 1535, current loss: 13097.290908\n",
      "iteration 1536, current loss: 13095.102032\n",
      "iteration 1537, current loss: 13092.905899\n",
      "iteration 1538, current loss: 13090.703536\n",
      "iteration 1539, current loss: 13088.496379\n",
      "iteration 1540, current loss: 13086.286242\n",
      "iteration 1541, current loss: 13084.075224\n",
      "iteration 1542, current loss: 13081.865566\n",
      "iteration 1543, current loss: 13079.659474\n",
      "iteration 1544, current loss: 13077.458947\n",
      "iteration 1545, current loss: 13075.265645\n",
      "iteration 1546, current loss: 13073.080823\n",
      "iteration 1547, current loss: 13070.905322\n",
      "iteration 1548, current loss: 13068.739605\n",
      "iteration 1549, current loss: 13066.583823\n",
      "iteration 1550, current loss: 13064.437886\n",
      "iteration 1551, current loss: 13062.301531\n",
      "iteration 1552, current loss: 13060.174375\n",
      "iteration 1553, current loss: 13058.055963\n",
      "iteration 1554, current loss: 13055.945801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1555, current loss: 13053.843376\n",
      "iteration 1556, current loss: 13051.748177\n",
      "iteration 1557, current loss: 13049.659705\n",
      "iteration 1558, current loss: 13047.577476\n",
      "iteration 1559, current loss: 13045.501033\n",
      "iteration 1560, current loss: 13043.429941\n",
      "iteration 1561, current loss: 13041.363794\n",
      "iteration 1562, current loss: 13039.302209\n",
      "iteration 1563, current loss: 13037.244834\n",
      "iteration 1564, current loss: 13035.191338\n",
      "iteration 1565, current loss: 13033.141417\n",
      "iteration 1566, current loss: 13031.094792\n",
      "iteration 1567, current loss: 13029.051208\n",
      "iteration 1568, current loss: 13027.010430\n",
      "iteration 1569, current loss: 13024.972250\n",
      "iteration 1570, current loss: 13022.936481\n",
      "iteration 1571, current loss: 13020.902960\n",
      "iteration 1572, current loss: 13018.871547\n",
      "iteration 1573, current loss: 13016.842130\n",
      "iteration 1574, current loss: 13014.814618\n",
      "iteration 1575, current loss: 13012.788950\n",
      "iteration 1576, current loss: 13010.765088\n",
      "iteration 1577, current loss: 13008.743022\n",
      "iteration 1578, current loss: 13006.722769\n",
      "iteration 1579, current loss: 13004.704368\n",
      "iteration 1580, current loss: 13002.687882\n",
      "iteration 1581, current loss: 13000.673391\n",
      "iteration 1582, current loss: 12998.660990\n",
      "iteration 1583, current loss: 12996.650785\n",
      "iteration 1584, current loss: 12994.642883\n",
      "iteration 1585, current loss: 12992.637390\n",
      "iteration 1586, current loss: 12990.634406\n",
      "iteration 1587, current loss: 12988.634013\n",
      "iteration 1588, current loss: 12986.636278\n",
      "iteration 1589, current loss: 12984.641247\n",
      "iteration 1590, current loss: 12982.648939\n",
      "iteration 1591, current loss: 12980.659348\n",
      "iteration 1592, current loss: 12978.672444\n",
      "iteration 1593, current loss: 12976.688170\n",
      "iteration 1594, current loss: 12974.706447\n",
      "iteration 1595, current loss: 12972.727172\n",
      "iteration 1596, current loss: 12970.750226\n",
      "iteration 1597, current loss: 12968.775472\n",
      "iteration 1598, current loss: 12966.802759\n",
      "iteration 1599, current loss: 12964.831926\n",
      "iteration 1600, current loss: 12962.862802\n",
      "iteration 1601, current loss: 12960.895211\n",
      "iteration 1602, current loss: 12958.928969\n",
      "iteration 1603, current loss: 12956.963892\n",
      "iteration 1604, current loss: 12954.999791\n",
      "iteration 1605, current loss: 12953.036479\n",
      "iteration 1606, current loss: 12951.073768\n",
      "iteration 1607, current loss: 12949.111468\n",
      "iteration 1608, current loss: 12947.149395\n",
      "iteration 1609, current loss: 12945.187361\n",
      "iteration 1610, current loss: 12943.225186\n",
      "iteration 1611, current loss: 12941.262687\n",
      "iteration 1612, current loss: 12939.299687\n",
      "iteration 1613, current loss: 12937.336010\n",
      "iteration 1614, current loss: 12935.371481\n",
      "iteration 1615, current loss: 12933.405932\n",
      "iteration 1616, current loss: 12931.439196\n",
      "iteration 1617, current loss: 12929.471107\n",
      "iteration 1618, current loss: 12927.501505\n",
      "iteration 1619, current loss: 12925.530234\n",
      "iteration 1620, current loss: 12923.557141\n",
      "iteration 1621, current loss: 12921.582078\n",
      "iteration 1622, current loss: 12919.604903\n",
      "iteration 1623, current loss: 12917.625477\n",
      "iteration 1624, current loss: 12915.643673\n",
      "iteration 1625, current loss: 12913.659369\n",
      "iteration 1626, current loss: 12911.672454\n",
      "iteration 1627, current loss: 12909.682828\n",
      "iteration 1628, current loss: 12907.690406\n",
      "iteration 1629, current loss: 12905.695121\n",
      "iteration 1630, current loss: 12903.696921\n",
      "iteration 1631, current loss: 12901.695780\n",
      "iteration 1632, current loss: 12899.691698\n",
      "iteration 1633, current loss: 12897.684702\n",
      "iteration 1634, current loss: 12895.674853\n",
      "iteration 1635, current loss: 12893.662247\n",
      "iteration 1636, current loss: 12891.647019\n",
      "iteration 1637, current loss: 12889.629343\n",
      "iteration 1638, current loss: 12887.609433\n",
      "iteration 1639, current loss: 12885.587546\n",
      "iteration 1640, current loss: 12883.563974\n",
      "iteration 1641, current loss: 12881.539046\n",
      "iteration 1642, current loss: 12879.513120\n",
      "iteration 1643, current loss: 12877.486579\n",
      "iteration 1644, current loss: 12875.459818\n",
      "iteration 1645, current loss: 12873.433245\n",
      "iteration 1646, current loss: 12871.407262\n",
      "iteration 1647, current loss: 12869.382264\n",
      "iteration 1648, current loss: 12867.358629\n",
      "iteration 1649, current loss: 12865.336709\n",
      "iteration 1650, current loss: 12863.316827\n",
      "iteration 1651, current loss: 12861.299269\n",
      "iteration 1652, current loss: 12859.284285\n",
      "iteration 1653, current loss: 12857.272084\n",
      "iteration 1654, current loss: 12855.262835\n",
      "iteration 1655, current loss: 12853.256668\n",
      "iteration 1656, current loss: 12851.253673\n",
      "iteration 1657, current loss: 12849.253906\n",
      "iteration 1658, current loss: 12847.257387\n",
      "iteration 1659, current loss: 12845.264108\n",
      "iteration 1660, current loss: 12843.274032\n",
      "iteration 1661, current loss: 12841.287098\n",
      "iteration 1662, current loss: 12839.303224\n",
      "iteration 1663, current loss: 12837.322310\n",
      "iteration 1664, current loss: 12835.344237\n",
      "iteration 1665, current loss: 12833.368875\n",
      "iteration 1666, current loss: 12831.396080\n",
      "iteration 1667, current loss: 12829.425696\n",
      "iteration 1668, current loss: 12827.457557\n",
      "iteration 1669, current loss: 12825.491489\n",
      "iteration 1670, current loss: 12823.527305\n",
      "iteration 1671, current loss: 12821.564809\n",
      "iteration 1672, current loss: 12819.603795\n",
      "iteration 1673, current loss: 12817.644046\n",
      "iteration 1674, current loss: 12815.685329\n",
      "iteration 1675, current loss: 12813.727401\n",
      "iteration 1676, current loss: 12811.769999\n",
      "iteration 1677, current loss: 12809.812844\n",
      "iteration 1678, current loss: 12807.855637\n",
      "iteration 1679, current loss: 12805.898051\n",
      "iteration 1680, current loss: 12803.939736\n",
      "iteration 1681, current loss: 12801.980307\n",
      "iteration 1682, current loss: 12800.019344\n",
      "iteration 1683, current loss: 12798.056381\n",
      "iteration 1684, current loss: 12796.090905\n",
      "iteration 1685, current loss: 12794.122342\n",
      "iteration 1686, current loss: 12792.150049\n",
      "iteration 1687, current loss: 12790.173306\n",
      "iteration 1688, current loss: 12788.191300\n",
      "iteration 1689, current loss: 12786.203115\n",
      "iteration 1690, current loss: 12784.207725\n",
      "iteration 1691, current loss: 12782.203987\n",
      "iteration 1692, current loss: 12780.190654\n",
      "iteration 1693, current loss: 12778.166410\n",
      "iteration 1694, current loss: 12776.129935\n",
      "iteration 1695, current loss: 12774.080050\n",
      "iteration 1696, current loss: 12772.015922\n",
      "iteration 1697, current loss: 12769.937386\n",
      "iteration 1698, current loss: 12767.845313\n",
      "iteration 1699, current loss: 12765.741959\n",
      "iteration 1700, current loss: 12763.631092\n",
      "iteration 1701, current loss: 12761.517693\n",
      "iteration 1702, current loss: 12759.407169\n",
      "iteration 1703, current loss: 12757.304324\n",
      "iteration 1704, current loss: 12755.212531\n",
      "iteration 1705, current loss: 12753.133484\n",
      "iteration 1706, current loss: 12751.067475\n",
      "iteration 1707, current loss: 12749.013929\n",
      "iteration 1708, current loss: 12746.971874\n",
      "iteration 1709, current loss: 12744.940258\n",
      "iteration 1710, current loss: 12742.918093\n",
      "iteration 1711, current loss: 12740.904512\n",
      "iteration 1712, current loss: 12738.898766\n",
      "iteration 1713, current loss: 12736.900206\n",
      "iteration 1714, current loss: 12734.908265\n",
      "iteration 1715, current loss: 12732.922430\n",
      "iteration 1716, current loss: 12730.942235\n",
      "iteration 1717, current loss: 12728.967241\n",
      "iteration 1718, current loss: 12726.997032\n",
      "iteration 1719, current loss: 12725.031203\n",
      "iteration 1720, current loss: 12723.069357\n",
      "iteration 1721, current loss: 12721.111103\n",
      "iteration 1722, current loss: 12719.156050\n",
      "iteration 1723, current loss: 12717.203805\n",
      "iteration 1724, current loss: 12715.253971\n",
      "iteration 1725, current loss: 12713.306149\n",
      "iteration 1726, current loss: 12711.359931\n",
      "iteration 1727, current loss: 12709.414904\n",
      "iteration 1728, current loss: 12707.470644\n",
      "iteration 1729, current loss: 12705.526718\n",
      "iteration 1730, current loss: 12703.582681\n",
      "iteration 1731, current loss: 12701.638079\n",
      "iteration 1732, current loss: 12699.692441\n",
      "iteration 1733, current loss: 12697.745286\n",
      "iteration 1734, current loss: 12695.796120\n",
      "iteration 1735, current loss: 12693.844438\n",
      "iteration 1736, current loss: 12691.889729\n",
      "iteration 1737, current loss: 12689.931475\n",
      "iteration 1738, current loss: 12687.969167\n",
      "iteration 1739, current loss: 12686.002310\n",
      "iteration 1740, current loss: 12684.030437\n",
      "iteration 1741, current loss: 12682.053133\n",
      "iteration 1742, current loss: 12680.070051\n",
      "iteration 1743, current loss: 12678.080943\n",
      "iteration 1744, current loss: 12676.085690\n",
      "iteration 1745, current loss: 12674.084328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1746, current loss: 12672.077078\n",
      "iteration 1747, current loss: 12670.064362\n",
      "iteration 1748, current loss: 12668.046813\n",
      "iteration 1749, current loss: 12666.025260\n",
      "iteration 1750, current loss: 12664.000702\n",
      "iteration 1751, current loss: 12661.974255\n",
      "iteration 1752, current loss: 12659.947090\n",
      "iteration 1753, current loss: 12657.920359\n",
      "iteration 1754, current loss: 12655.895127\n",
      "iteration 1755, current loss: 12653.872309\n",
      "iteration 1756, current loss: 12651.852630\n",
      "iteration 1757, current loss: 12649.836601\n",
      "iteration 1758, current loss: 12647.824520\n",
      "iteration 1759, current loss: 12645.816484\n",
      "iteration 1760, current loss: 12643.812412\n",
      "iteration 1761, current loss: 12641.812082\n",
      "iteration 1762, current loss: 12639.815160\n",
      "iteration 1763, current loss: 12637.821231\n",
      "iteration 1764, current loss: 12635.829830\n",
      "iteration 1765, current loss: 12633.840466\n",
      "iteration 1766, current loss: 12631.852640\n",
      "iteration 1767, current loss: 12629.865865\n",
      "iteration 1768, current loss: 12627.879675\n",
      "iteration 1769, current loss: 12625.893644\n",
      "iteration 1770, current loss: 12623.907388\n",
      "iteration 1771, current loss: 12621.920577\n",
      "iteration 1772, current loss: 12619.932938\n",
      "iteration 1773, current loss: 12617.944262\n",
      "iteration 1774, current loss: 12615.954402\n",
      "iteration 1775, current loss: 12613.963279\n",
      "iteration 1776, current loss: 12611.970873\n",
      "iteration 1777, current loss: 12609.977222\n",
      "iteration 1778, current loss: 12607.982412\n",
      "iteration 1779, current loss: 12605.986567\n",
      "iteration 1780, current loss: 12603.989839\n",
      "iteration 1781, current loss: 12601.992388\n",
      "iteration 1782, current loss: 12599.994377\n",
      "iteration 1783, current loss: 12597.995949\n",
      "iteration 1784, current loss: 12595.997225\n",
      "iteration 1785, current loss: 12593.998285\n",
      "iteration 1786, current loss: 12591.999172\n",
      "iteration 1787, current loss: 12589.999879\n",
      "iteration 1788, current loss: 12588.000362\n",
      "iteration 1789, current loss: 12586.000536\n",
      "iteration 1790, current loss: 12584.000296\n",
      "iteration 1791, current loss: 12581.999527\n",
      "iteration 1792, current loss: 12579.998128\n",
      "iteration 1793, current loss: 12577.996041\n",
      "iteration 1794, current loss: 12575.993281\n",
      "iteration 1795, current loss: 12573.989962\n",
      "iteration 1796, current loss: 12571.986335\n",
      "iteration 1797, current loss: 12569.982794\n",
      "iteration 1798, current loss: 12567.979891\n",
      "iteration 1799, current loss: 12565.978312\n",
      "iteration 1800, current loss: 12563.978838\n",
      "iteration 1801, current loss: 12561.982291\n",
      "iteration 1802, current loss: 12559.989467\n",
      "iteration 1803, current loss: 12558.001075\n",
      "iteration 1804, current loss: 12556.017685\n",
      "iteration 1805, current loss: 12554.039699\n",
      "iteration 1806, current loss: 12552.067349\n",
      "iteration 1807, current loss: 12550.100705\n",
      "iteration 1808, current loss: 12548.139703\n",
      "iteration 1809, current loss: 12546.184167\n",
      "iteration 1810, current loss: 12544.233850\n",
      "iteration 1811, current loss: 12542.288449\n",
      "iteration 1812, current loss: 12540.347638\n",
      "iteration 1813, current loss: 12538.411077\n",
      "iteration 1814, current loss: 12536.478433\n",
      "iteration 1815, current loss: 12534.549381\n",
      "iteration 1816, current loss: 12532.623619\n",
      "iteration 1817, current loss: 12530.700866\n",
      "iteration 1818, current loss: 12528.780869\n",
      "iteration 1819, current loss: 12526.863401\n",
      "iteration 1820, current loss: 12524.948265\n",
      "iteration 1821, current loss: 12523.035292\n",
      "iteration 1822, current loss: 12521.124340\n",
      "iteration 1823, current loss: 12519.215296\n",
      "iteration 1824, current loss: 12517.308071\n",
      "iteration 1825, current loss: 12515.402604\n",
      "iteration 1826, current loss: 12513.498853\n",
      "iteration 1827, current loss: 12511.596801\n",
      "iteration 1828, current loss: 12509.696448\n",
      "iteration 1829, current loss: 12507.797814\n",
      "iteration 1830, current loss: 12505.900931\n",
      "iteration 1831, current loss: 12504.005846\n",
      "iteration 1832, current loss: 12502.112616\n",
      "iteration 1833, current loss: 12500.221303\n",
      "iteration 1834, current loss: 12498.331978\n",
      "iteration 1835, current loss: 12496.444715\n",
      "iteration 1836, current loss: 12494.559586\n",
      "iteration 1837, current loss: 12492.676665\n",
      "iteration 1838, current loss: 12490.796022\n",
      "iteration 1839, current loss: 12488.917723\n",
      "iteration 1840, current loss: 12487.041825\n",
      "iteration 1841, current loss: 12485.168381\n",
      "iteration 1842, current loss: 12483.297434\n",
      "iteration 1843, current loss: 12481.429017\n",
      "iteration 1844, current loss: 12479.563152\n",
      "iteration 1845, current loss: 12477.699851\n",
      "iteration 1846, current loss: 12475.839113\n",
      "iteration 1847, current loss: 12473.980925\n",
      "iteration 1848, current loss: 12472.125262\n",
      "iteration 1849, current loss: 12470.272084\n",
      "iteration 1850, current loss: 12468.421338\n",
      "iteration 1851, current loss: 12466.572960\n",
      "iteration 1852, current loss: 12464.726867\n",
      "iteration 1853, current loss: 12462.882967\n",
      "iteration 1854, current loss: 12461.041149\n",
      "iteration 1855, current loss: 12459.201290\n",
      "iteration 1856, current loss: 12457.363250\n",
      "iteration 1857, current loss: 12455.526874\n",
      "iteration 1858, current loss: 12453.691991\n",
      "iteration 1859, current loss: 12451.858413\n",
      "iteration 1860, current loss: 12450.025936\n",
      "iteration 1861, current loss: 12448.194336\n",
      "iteration 1862, current loss: 12446.363375\n",
      "iteration 1863, current loss: 12444.532793\n",
      "iteration 1864, current loss: 12442.702314\n",
      "iteration 1865, current loss: 12440.871643\n",
      "iteration 1866, current loss: 12439.040468\n",
      "iteration 1867, current loss: 12437.208464\n",
      "iteration 1868, current loss: 12435.375293\n",
      "iteration 1869, current loss: 12433.540609\n",
      "iteration 1870, current loss: 12431.704067\n",
      "iteration 1871, current loss: 12429.865333\n",
      "iteration 1872, current loss: 12428.024093\n",
      "iteration 1873, current loss: 12426.180071\n",
      "iteration 1874, current loss: 12424.333049\n",
      "iteration 1875, current loss: 12422.482883\n",
      "iteration 1876, current loss: 12420.629532\n",
      "iteration 1877, current loss: 12418.773069\n",
      "iteration 1878, current loss: 12416.913700\n",
      "iteration 1879, current loss: 12415.051768\n",
      "iteration 1880, current loss: 12413.187745\n",
      "iteration 1881, current loss: 12411.322210\n",
      "iteration 1882, current loss: 12409.455816\n",
      "iteration 1883, current loss: 12407.589247\n",
      "iteration 1884, current loss: 12405.723167\n",
      "iteration 1885, current loss: 12403.858186\n",
      "iteration 1886, current loss: 12401.994816\n",
      "iteration 1887, current loss: 12400.133461\n",
      "iteration 1888, current loss: 12398.274400\n",
      "iteration 1889, current loss: 12396.417801\n",
      "iteration 1890, current loss: 12394.563732\n",
      "iteration 1891, current loss: 12392.712178\n",
      "iteration 1892, current loss: 12390.863062\n",
      "iteration 1893, current loss: 12389.016268\n",
      "iteration 1894, current loss: 12387.171652\n",
      "iteration 1895, current loss: 12385.329059\n",
      "iteration 1896, current loss: 12383.488336\n",
      "iteration 1897, current loss: 12381.649333\n",
      "iteration 1898, current loss: 12379.811916\n",
      "iteration 1899, current loss: 12377.975963\n",
      "iteration 1900, current loss: 12376.141372\n",
      "iteration 1901, current loss: 12374.308057\n",
      "iteration 1902, current loss: 12372.475949\n",
      "iteration 1903, current loss: 12370.644992\n",
      "iteration 1904, current loss: 12368.815148\n",
      "iteration 1905, current loss: 12366.986390\n",
      "iteration 1906, current loss: 12365.158700\n",
      "iteration 1907, current loss: 12363.332069\n",
      "iteration 1908, current loss: 12361.506496\n",
      "iteration 1909, current loss: 12359.681985\n",
      "iteration 1910, current loss: 12357.858542\n",
      "iteration 1911, current loss: 12356.036178\n",
      "iteration 1912, current loss: 12354.214904\n",
      "iteration 1913, current loss: 12352.394733\n",
      "iteration 1914, current loss: 12350.575677\n",
      "iteration 1915, current loss: 12348.757748\n",
      "iteration 1916, current loss: 12346.940959\n",
      "iteration 1917, current loss: 12345.125319\n",
      "iteration 1918, current loss: 12343.310839\n",
      "iteration 1919, current loss: 12341.497527\n",
      "iteration 1920, current loss: 12339.685389\n",
      "iteration 1921, current loss: 12337.874432\n",
      "iteration 1922, current loss: 12336.064660\n",
      "iteration 1923, current loss: 12334.256075\n",
      "iteration 1924, current loss: 12332.448679\n",
      "iteration 1925, current loss: 12330.642469\n",
      "iteration 1926, current loss: 12328.837445\n",
      "iteration 1927, current loss: 12327.033601\n",
      "iteration 1928, current loss: 12325.230931\n",
      "iteration 1929, current loss: 12323.429427\n",
      "iteration 1930, current loss: 12321.629080\n",
      "iteration 1931, current loss: 12319.829880\n",
      "iteration 1932, current loss: 12318.031815\n",
      "iteration 1933, current loss: 12316.234875\n",
      "iteration 1934, current loss: 12314.439050\n",
      "iteration 1935, current loss: 12312.644332\n",
      "iteration 1936, current loss: 12310.850716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1937, current loss: 12309.058203\n",
      "iteration 1938, current loss: 12307.266799\n",
      "iteration 1939, current loss: 12305.476516\n",
      "iteration 1940, current loss: 12303.687379\n",
      "iteration 1941, current loss: 12301.899421\n",
      "iteration 1942, current loss: 12300.112687\n",
      "iteration 1943, current loss: 12298.327236\n",
      "iteration 1944, current loss: 12296.543141\n",
      "iteration 1945, current loss: 12294.760484\n",
      "iteration 1946, current loss: 12292.979363\n",
      "iteration 1947, current loss: 12291.199883\n",
      "iteration 1948, current loss: 12289.422161\n",
      "iteration 1949, current loss: 12287.646313\n",
      "iteration 1950, current loss: 12285.872461\n",
      "iteration 1951, current loss: 12284.100722\n",
      "iteration 1952, current loss: 12282.331207\n",
      "iteration 1953, current loss: 12280.564018\n",
      "iteration 1954, current loss: 12278.799243\n",
      "iteration 1955, current loss: 12277.036956\n",
      "iteration 1956, current loss: 12275.277213\n",
      "iteration 1957, current loss: 12273.520053\n",
      "iteration 1958, current loss: 12271.765499\n",
      "iteration 1959, current loss: 12270.013553\n",
      "iteration 1960, current loss: 12268.264203\n",
      "iteration 1961, current loss: 12266.517422\n",
      "iteration 1962, current loss: 12264.773168\n",
      "iteration 1963, current loss: 12263.031388\n",
      "iteration 1964, current loss: 12261.292019\n",
      "iteration 1965, current loss: 12259.554986\n",
      "iteration 1966, current loss: 12257.820210\n",
      "iteration 1967, current loss: 12256.087602\n",
      "iteration 1968, current loss: 12254.357068\n",
      "iteration 1969, current loss: 12252.628510\n",
      "iteration 1970, current loss: 12250.901824\n",
      "iteration 1971, current loss: 12249.176901\n",
      "iteration 1972, current loss: 12247.453630\n",
      "iteration 1973, current loss: 12245.731894\n",
      "iteration 1974, current loss: 12244.011572\n",
      "iteration 1975, current loss: 12242.292541\n",
      "iteration 1976, current loss: 12240.574673\n",
      "iteration 1977, current loss: 12238.857836\n",
      "iteration 1978, current loss: 12237.141894\n",
      "iteration 1979, current loss: 12235.426707\n",
      "iteration 1980, current loss: 12233.712132\n",
      "iteration 1981, current loss: 12231.998021\n",
      "iteration 1982, current loss: 12230.284225\n",
      "iteration 1983, current loss: 12228.570590\n",
      "iteration 1984, current loss: 12226.856959\n",
      "iteration 1985, current loss: 12225.143172\n",
      "iteration 1986, current loss: 12223.429068\n",
      "iteration 1987, current loss: 12221.714483\n",
      "iteration 1988, current loss: 12219.999249\n",
      "iteration 1989, current loss: 12218.283200\n",
      "iteration 1990, current loss: 12216.566166\n",
      "iteration 1991, current loss: 12214.847978\n",
      "iteration 1992, current loss: 12213.128464\n",
      "iteration 1993, current loss: 12211.407454\n",
      "iteration 1994, current loss: 12209.684777\n",
      "iteration 1995, current loss: 12207.960265\n",
      "iteration 1996, current loss: 12206.233749\n",
      "iteration 1997, current loss: 12204.505064\n",
      "iteration 1998, current loss: 12202.774051\n",
      "iteration 1999, current loss: 12201.040554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1da3f858fd0>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(2000, model)\n",
    "costs = model['costs']\n",
    "plt.plot(costs[-2000:])\n",
    "#print(W[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1da3fa6ef98>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3Xl8XNV99/HPb7Rv1mItlizZsvGCTbCNEQbMkgCJMRBC8pQQyIITSEmakNLS52mgaZ+my5OmS0JDQ5MXCaSQ0AIhENyUzWFPjI1lYwzeJa+yLVnWLsvaz/PHHBvZkrXYku+M5vt+veY1d849M/M7Hllf3XvuvWPOOURERPoKBV2AiIhEHoWDiIj0o3AQEZF+FA4iItKPwkFERPpROIiISD8KBxER6UfhICIi/SgcRESkn/igCzhVubm5rrS0NOgyRESixtq1aw855/KG0zdqw6G0tJTy8vKgyxARiRpmtnu4fbVbSURE+lE4iIhIPwoHERHpR+EgIiL9KBxERKQfhYOIiPSjcBARkX5iKhx6ex0/fGU7b2yrDboUEZGIFlPhEAoZD76xg5c31wRdiohIRIupcAAozExhf1N70GWIiES02AuHrGQONB0JugwRkYgWe+GQmcKBRm05iIgMJubCoSgzmbrDnbR39QRdiohIxIq5cJiUmQxATbO2HkRETibmwqEoKwWA/dq1JCJyUjEXDoV+y0GT0iIiJxeD4RDecjigw1lFRE4q5sIhJTGOrNQEbTmIiAwi5sIBdDiriMhQYjIcijKTdZa0iMggYjIcdJa0iMjgYjIcirNTaWzroqW9K+hSREQiUkyGw5ScVAD21mvrQURkIMMKBzPbZWbvmdl6Myv3bTlmtsLMtvv7bN9uZna/mVWY2QYzW9jndZb5/tvNbFmf9vP961f459poD7Svo+Gwp/7wWL6NiEjUGsmWwxXOuQXOuTL/+B7gZefcTOBl/xjgGmCmv90B/AjCYQL8NXAhsAj466OB4vvc0ed5S095RMNQciwc2sbybUREotbp7Fa6AXjELz8CfLJP+6MubBWQZWaFwNXACudcvXOuAVgBLPXrJjjn3nLOOeDRPq81JjJTEshKTVA4iIicxHDDwQEvmdlaM7vDtxU45w4A+Pt83z4Z2NvnuVW+bbD2qgHax9SUnFT2aM5BRGRA8cPsd4lzbr+Z5QMrzGzLIH0Hmi9wp9De/4XDwXQHwJQpUwaveAglOals3Nd0Wq8hIjJeDWvLwTm3398fBJ4hPGdQ43cJ4e8P+u5VQEmfpxcD+4doLx6gfaA6HnTOlTnnyvLy8oZT+klNyUmlquEIPb0D5pCISEwbMhzMLM3MMo4uA0uA94HlwNEjjpYBz/rl5cCt/qili4Amv9vpRWCJmWX7ieglwIt+XYuZXeSPUrq1z2uNmak5qXT3Op0MJyIygOHsVioAnvFHl8YD/+mce8HM1gBPmtntwB7g077/c8C1QAXQBnwJwDlXb2Z/B6zx/f7WOVfvl/8I+A8gBXje38bUlD5HLBVnp47124mIRJUhw8E5twOYP0B7HXDVAO0O+PpJXuth4OEB2suBDw2j3lFz9HDW3XVtLD7rTL6ziEjki8kzpAEmZ6WQFB9iR21r0KWIiEScmA2HUMiYnpdOxUGFg4jIiWI2HADOykujslaX0BAROVGMh0M6exvaaO/qCboUEZGIEtvhkJ+Oc7CrTlsPIiJ9xXQ4zMhLB9C8g4jICWI6HKblpmEGlQe15SAi0ldMh0NKYhyTs1Ko1OGsIiLHielwgPCktMJBROR4CgcfDr26AJ+IyDExHw6zJ6XT3tXL3gZ98Y+IyFExHw5nT5oAwOYDLQFXIiISOWI+HGYVZGAGW6qbgy5FRCRixHw4pCTGMW1iGlu05SAickzMhwPA2YUZ2nIQEelD4UB43mF3fRuHO7qDLkVEJCIoHICzJ2XgHGyr0a4lERFQOAAwpzB8xNKWaoWDiAgoHIDwt8KlJ8Wz5YDmHUREQOEAhL8VbvakDJ3rICLiKRy8c4omsHF/Ez26jIaIiMLhqHnFWRzu7GGHLsInIqJwOGp+cSYA71Y1BVyJiEjwFA7e9Lx00hLj2FDVGHQpIiKBUzh4cSHj3OJM3t2rcBARUTj0Mb84i80HWujs7g26FBGRQCkc+phXnEVnT6+usyQiMU/h0Me8o5PS2rUkIjFO4dBHcXYK+RlJlO9uCLoUEZFAKRz6MDMuKM2hfJfCQURim8LhBBeUZrOv8Qj7Go8EXYqISGAUDicoK80BoHxXfcCViIgER+FwgjmFE0hPimeNwkFEYpjC4QRxIWPh1GzW7NS8g4jELoXDABaVZrO1poWmtq6gSxERCYTCYQDH5h12a9eSiMQmhcMAFpRkkRQfYmVlXdCliIgEQuEwgOSEOC4ozeH3FYeCLkVEJBAKh5O4ZEYuW6pbONjSHnQpIiJn3LDDwczizOwdM/uNfzzNzFab2XYze8LMEn17kn9c4deX9nmNe337VjO7uk/7Ut9WYWb3jN7wTt2lM3IBWFmhXUsiEntGsuVwF7C5z+N/BO5zzs0EGoDbffvtQINzbgZwn++Hmc0FbgbOAZYC/+4DJw54ALgGmAvc4vsG6pyiCWSlJvA77VoSkRg0rHAws2LgOuCn/rEBVwJP+S6PAJ/0yzf4x/j1V/n+NwCPO+c6nHM7gQpgkb9VOOd2OOc6gcd930CFQsYlZ+Xy+4pDOOeCLkdE5Iwa7pbDvwJ/Dhz9FpyJQKNzrts/rgIm++XJwF4Av77J9z/WfsJzTtbej5ndYWblZlZeW1s7zNJP3SUzcjnQ1E5lbeuYv5eISCQZMhzM7OPAQefc2r7NA3R1Q6wbaXv/RucedM6VOefK8vLyBql6dHxkdvg9Xt58cMzfS0Qkkgxny+ES4BNmtovwLp8rCW9JZJlZvO9TDOz3y1VACYBfnwnU920/4Tknaw9cUVYK5xRN4Leba4IuRUTkjBoyHJxz9zrnip1zpYQnlF9xzn0OeBW40XdbBjzrl5f7x/j1r7jwTvvlwM3+aKZpwEzgbWANMNMf/ZTo32P5qIxuFHx0TgFrdzdQ19oRdCkiImfM6Zzn8E3gbjOrIDyn8JBvfwiY6NvvBu4BcM5tBJ4ENgEvAF93zvX4eYk7gRcJHw31pO8bET42t4BeB69s0a4lEYkdFq1H4pSVlbny8vIxfx/nHIu/+wrnTs7kwVvLxvz9RETGipmtdc4N6xeZzpAegpnx0TkFvLn9EO1dPUGXIyJyRigchuHqcyZxpKuH17aO/eGzIiKRQOEwDBdNzyE3PZH/fjciDqISERlzCodhiI8Lce25hby8pYbWju6hnyAiEuUUDsN0/fwi2rt6eVnnPIhIDFA4DNP5U7IpzExm+XrtWhKR8U/hMEyhkHH9/CLe2F5LY1tn0OWIiIwphcMIXD+viK4ex3PvVQddiojImFI4jMCHJk9gZn46T63dO3RnEZEopnAYATPj02XFrNvTSMXBlqDLEREZMwqHEfrUecXEhYxfrq0KuhQRkTGjcBihvIwkrpidz9Pr9tHd0zv0E0REopDC4RTcVFZMbUsHr2/T5TREZHxSOJyCK87OJzc9kcfXaGJaRMYnhcMpSIgLcVNZCS9vrqGqoS3ockRERp3C4RR9/qKpmBk/X7U76FJEREadwuEUFWWlsGRuAU+s2avveRCRcUfhcBqWLS6lsa2LZ9fvC7oUEZFRpXA4DRdOy+HsSRn87Pe7iNavWxURGYjC4TSYGV++bDpbqlt4devBoMsRERk1CofTdMOCIiZnpfDAq5XaehCRcUPhcJoS4kLccfl01u5u4O2d9UGXIyIyKhQOo+AzF5SQm57IA69VBl2KiMioUDiMguSEOG67dBpvbKtlQ1Vj0OWIiJw2hcMo+cJFU8lKTeB7L20LuhQRkdOmcBglGckJfO0jZ/H6tlpW76gLuhwRkdOicBhFt15cSsGEJP7lpa06cklEoprCYRQlJ8TxjStnsmZXA6/pct4iEsUUDqPsprISpuSk8s8vbKW3V1sPIhKdFA6jLDE+xJ8tmcWmA808846uuSQi0UnhMAaun1fE/JIs/vnFrbR1dgddjojIiCkcxkAoZPzVdXOobm7nJ2/sDLocEZERUziMkbLSHK49dxI/fr2Smub2oMsRERkRhcMY+ubSs+npdXzvpa1BlyIiMiIKhzE0dWIayxZP5Zdrq9i4vynockREhk3hMMbuvHImWSkJfHv5Rh3aKiJRQ+EwxjJTErj3mjms2dXAL9fuDbocEZFhUTicATeeX8yi0hz+4fkt1LV2BF2OiMiQhgwHM0s2s7fN7F0z22hmf+Pbp5nZajPbbmZPmFmib0/yjyv8+tI+r3Wvb99qZlf3aV/q2yrM7J7RH2awQiHj7z/1IVrbu/nOc1uCLkdEZEjD2XLoAK50zs0HFgBLzewi4B+B+5xzM4EG4Hbf/3agwTk3A7jP98PM5gI3A+cAS4F/N7M4M4sDHgCuAeYCt/i+48qsggzuuHw6v1pXxcrKQ0GXIyIyqCHDwYW1+ocJ/uaAK4GnfPsjwCf98g3+MX79VWZmvv1x51yHc24nUAEs8rcK59wO51wn8LjvO+5848qZlE5M5Zu/2sDhDp05LSKRa1hzDv4v/PXAQWAFUAk0OueO/oarAib75cnAXgC/vgmY2Lf9hOecrH3cSUmM418+PZ+qhiN857nNQZcjInJSwwoH51yPc24BUEz4L/05A3Xz93aSdSNt78fM7jCzcjMrr62Nzktil5Xm8OVLp/HY6j28uT06xyAi49+IjlZyzjUCrwEXAVlmFu9XFQP7/XIVUALg12cC9X3bT3jOydoHev8HnXNlzrmyvLy8kZQeUf5syWzOykvjz5/aQHN7V9DliIj0M5yjlfLMLMsvpwAfBTYDrwI3+m7LgGf98nL/GL/+FRf+WrTlwM3+aKZpwEzgbWANMNMf/ZRIeNJ6+WgMLlIlJ4R3Lx1s6eDep9/Tt8aJSMQZzpZDIfCqmW0g/It8hXPuN8A3gbvNrILwnMJDvv9DwETffjdwD4BzbiPwJLAJeAH4ut9d1Q3cCbxIOHSe9H3HtfOmZPNnS2bxPxsO8IvVe4IuR0TkOBatf7WWlZW58vLyoMs4Lb29jtseWcPKijqe/tpiPjQ5M+iSRGQcM7O1zrmy4fTVGdIBCoWM79+0gJy0RL7+n+toOqL5BxGJDAqHgOWkJfLDz57HvoYj/PF/vUOPLs4nIhFA4RABykpz+LtPfojXt9XyDzr/QUQiQPzQXeRMuGXRFLZWt/DT3+1kVkEGN11QMvSTRETGiLYcIshfXjeHS2fk8q1fv6frL4lIoBQOESQ+LsQDn11I6cQ0vvLoWn17nIgERuEQYTJTE3jktkWkJ8fzxZ+tYW99W9AliUgMUjhEoKKsFB69bRGd3b184aHVHNIXBInIGaZwiFAzCzJ4+IsXUN3czq0PvU1jW2fQJYlIDFE4RLDzp2bz48+fT8XBVm59+G2dJCciZ4zCIcJ9ZHY+P/r8QjYfaGbZw2/Toqu4isgZoHCIAlfNKeCBzy7k/X1NfPFna3SZbxEZcwqHKLHknEn82y3n8e7eRm55cJUmqUVkTCkcosg15xbyk2VlVNa2ctOP32Jf45GgSxKRcUrhEGWumJ3PL26/kNrWDm780UoqDrYEXZKIjEMKhyhUVprDk1+5mK4exx/86C1WVuhSGyIyuhQOUWpO4QSe+dpiCiYkcevDb/PY6t1BlyQi44jCIYqV5KTyqz9azGUzc/nWM+/z7eUb6erpDbosERkHFA5RLiM5gZ8uu4AvXzqN/1i5i1seXEV1U3vQZYlIlFM4jANxIeMvPz6XH9y8gE0Hmrnu/jf53XbNQ4jIqVM4jCM3LJjM8jsvISctkS88vJof/HY7vfraURE5BQqHcWZGfgbP3nkJN8wv4r7fbuNzP13Nfp0PISIjpHAYh1IT47nvMwv47v86l/V7G1n6r2/wmw37gy5LRKKIwmGcMjNuXjSF5+66jGl56dz5n+9w9xPrdeE+ERkWhcM4Ny03jae+ejF/fNVMfr1+H9f84E1W76gLuiwRiXAKhxiQEBfi7o/N4pdfXUzIjM88uIq/fvZ9Dnd0B12aiEQohUMMOX9qNi/8yWV8cXEpj7y1m6U/eIOVlTrkVUT6UzjEmNTEeL79iXN48isXE2fGZ3+ymr/89Xu0aitCRPpQOMSoRdNyeP6uy7n90mk8tnoPV9/3hk6cE5FjFA4xLCUxjr/6+Fye+urFJMWH+PxDq/nj/3qHmmZdfkMk1ikchPOn5vDcXZdx11UzeWFjNVf+y2v89M0duoifSAxTOAgAyQlx/OnHZrHiTy9n0bQc/v5/NnPd/W+ySoe9isQkhYMcZ+rENB7+4gX85NYyDnf0cPODq/jDR8upONgadGkicgYpHKQfM+Njcwv47d0f5v9cPZu3Kuu4+l/f4N6n3+Og5iNEYoI5F51X7SwrK3Pl5eVBlxET6lo7+LdXKvjFqt0kxIW4dfFU/vCy6eSmJwVdmoiMgJmtdc6VDauvwkGGa9ehw3x/xTb+e8N+kuJDfO7CqXzl8unkT0gOujQRGQaFg4ypytpWHni1gmfX7ycuZHymrITbLp3GtNy0oEsTkUEoHOSM2FPXxr+/VsHT6/bR1dvLVWcX8OXLpnHhtBzMLOjyROQEIwmHISekzazEzF41s81mttHM7vLtOWa2wsy2+/ts325mdr+ZVZjZBjNb2Oe1lvn+281sWZ/2883sPf+c+02/WaLClImpfPcP5vG7e67gG1fMYO3uem5+cBXX//B3PPNOFZ3dOk9CJFoNueVgZoVAoXNunZllAGuBTwJfBOqdc981s3uAbOfcN83sWuAbwLXAhcAPnHMXmlkOUA6UAc6/zvnOuQYzexu4C1gFPAfc75x7frC6tOUQedq7enjmnX389M0dVNYeJj8jiVsWTeGzF06hQPMSIoEb1S0H59wB59w6v9wCbAYmAzcAj/hujxAODHz7oy5sFZDlA+ZqYIVzrt451wCsAJb6dROcc2+5cFI92ue1JIokJ8Rxy6IprPjTD/OzL13AnMIJ/ODl7Sz+7it87bG1vFVZR7TuxhSJNfEj6WxmpcB5wGqgwDl3AMIBYmb5vttkYG+fp1X5tsHaqwZolygVChlXzM7nitn57K47zC9W7ebJ8iqee6+amfnpfOHiqXzqvMlkJCcEXaqInMSwT4Izs3TgV8CfOOeaB+s6QJs7hfaBarjDzMrNrLy2tnaokiUCTJ2Yxreum8vqv7iKf7pxHskJcfzfZzdy0Xde5q9+/T5bqgf7URKRoAxry8HMEggHw2POuad9c42ZFfqthkLgoG+vAkr6PL0Y2O/bP3JC+2u+vXiA/v045x4EHoTwnMNwapfIkJwQx01lJdxUVsL6vY38/K3dPFG+l5+v2s05RRP4g4XF3LCgiIk6sU4kIgznaCUDHgI2O+e+32fVcuDoEUfLgGf7tN/qj1q6CGjyu59eBJaYWbY/smkJ8KJf12JmF/n3urXPa8k4tKAki+/dNJ9V917Ft6+fS8iMv/3NJi78zsv84aPlvPB+tY50EgnYcI5WuhR4E3gPOPo/9i8Izzs8CUwB9gCfds7V+1/wPwSWAm3Al5xz5f61bvPPBfh/zrmf+fYy4D+AFOB54BtuiMJ0tNL4srW6hV+tq+KZd/ZR29JBdmoCn5hfxCcWFHFeSTahkI5uFjldOglOolZ3Ty9vbj/EU+uqWLGphs7uXooyk7luXiHXzy/i3MmZOsFO5BQpHGRcaGnvYsWmGn6z4QBvbq+lq8cxJSeVj88r5OPziphTmKGgEBkBhYOMO01tXby4sZr/3rCflZV19PQ6pk5M5WNzCvjY3ALKSnOI064nkUEpHGRcq2vt4MWNNby0qZqVFXV09vSSnZrAlWeHg+LyWbmkJo7oFB6RmKBwkJjR2tHN61trWbGpmle2HKS5vZvE+BAXTsvh8pl5fHh2HjPz07X7SQSFg8Sorp5e1uys5+UtB3ljWy3b/VebFmYmc9nMXD48K59LZ+SSmaozsyU2jSQctO0t40ZCXIjFM3JZPCMXgP2NR3hjWy2vb6vl+ferebK8ipDBucVZLD5rIpeclcv5U7NJSYwLuHKRyKMtB4kJ3T29vFvVyOtba1lZWcf6vY109zoS40IsnJrF4rNyuWTGROYVZ5EQp69Wl/FJu5VEhtDa0c2aXfWsrDjEyso6Nh1oxjlIS4xj0bQcFp+Vy8VnTWRu4QSdgCfjhnYriQwhPSn+2JVjARoOd7JqRx2/rwyHxatbNwOQlZrAxdMnsnhGLhdPn8hZeWma3JaYoHAQAbLTErnm3EKuObcQgANNR3irso7fV9SxsvIQz79fDYTDYuGUbBZOyWLh1GzmF2eRlqT/RjL+aLeSyBCcc+yqa2P1jjrW7Wlg3Z5GKvyRUHEh4+xJGZw/NduHRjYlOSnaupCIpDkHkTHW2NbJO3sbWbe7gXV7Gli/p5HDnT0AZKcmcG5xFguKM5lXnMW8kkzyM/Q1qRI8zTmIjLGs1MTj5ix6eh1bq1t4Z28DG/Y28W5VIw+8doie3vAfX4WZyczzYTG/OItzizPJTNH5FhK5FA4ioyAuZMwtmsDcogl87sJw25HOHjbub2L93kY2VDWxoaqRFzfWHHvO9Ny0DwKjJJNzijJJTtA5FxIZFA4iYyQlMY6y0hzKSnOOtTW1dbFhXzgs1u9t5K0ddfx6ffiLD+NCxqyCDOYXZzK/JIt5xZnMKsjQeRcSCM05iASsprmdd/3WxbtV4fumI10AJMWHmFM4gXP8Vsk5RZnMLsjQWd1ySjQhLRLFnHPsrms7FhTv72ti04FmWtq7AQgZTM9LDwdGYTg05hZO0Pdvy5A0IS0SxcyM0tw0SnPTuGHBZCAcGFUNR9i4v5lNB5rZtL+JNTvredbvkgLIz0hi9qQMZhVkMKsgnVkFGcwsyCBd52HIKdBPjUgUMDNKclIpyUll6YcmHWtvONzJ5gPNbNzfzJbqFrbVtPDY6t20d/Ue6zM5K+VYaMyelM7M/Axm5Kdr8lsGpXAQiWLZaYnHXYkWwofVVjW0sdWHxdaaVrbXtBz7qlUI75oqnZh2bCvjrPx0puWmMS03jYxkHWIrCgeRcScuZEydmMbUiWksOeeDrYyunl52HTrM1poWtlW3sK2mlW01Lby0qZrePlOPuelJTPdBMT3vg/uSnFSS4rW1ESsUDiIxIiEuxEw/D8G8D9rbu3rYU9/GjtrD7Dx0mJ2HWtl56DAvb6nhifLOY/1CBoWZKZTkpDAlJ5UpfjfX0eWctERdNmQcUTiIxLjkhDi/eymj37qmI13sOhQOjR2HDrO3vo099W28trWWgy0dx/VNTYzrFxjhxykUZ6dqjiPKKBxE5KQyUxKYX5LF/JKsfuuOdPZQ1RAOi6O3vfVt7Klr43fbD3Gkq+e4/gUTkpiSk0pxdiqTs1KYnJ1y3L3CI7IoHETklKQkxn2wm+oEzjkOtXZ+EBh9bm/vrKe6uf3YdaeOyk1PZHJ2KsV9AmNSZjKTJiQzKTOZ3PQk4vTFS2eMwkFERp2ZkZeRRF5GEudPze63vrunl+rmdvY1HGFf45EP7huPsPlAM7/dXENHd+9xz4kLGXnpSRRkJjNpQhKTJiT75eTjlvX9GqND/4oicsbFx4Uozg7vYhrI0S2P6qZ2qpvDtxq/XNPczo7aw6ysrDt21nhfGUnxx4KiYEIy+ROSyEtPOnZ/NLTSk+I1gT4IhYOIRJy+Wx7nknnSfm2d3ccCpKa5neqmDn8fbqusPERtSwfdvf0vE5ScEAq/hw+M/IzkY+/ZN0Ry05NIjI+9ix8qHEQkaqUmxjM9L53peekn7dPb62g60kVtawe1LR0cbGmntqXjg1trBzsPHWb1znoa27oGfI3s1IQBgyP8+IOtk6zUhHGzNaJwEJFxLRQystMSyU5LHPBw3b46unuoa+08LjhODJS1exo42NzRb04EICHOyE0fOERy0hKZmJZEbnoiOWmJZKUmRvQEu8JBRMRLio+jKCuFoqyUQfs552jt6PbB0dEvTGpbOtjf1M67VU3UHe5goItfhwxy0hKPhcbE9EQmpiUyMf2EZb9+QsqZnSNROIiIjJCZkZGcQEZywqC7tCB8ZFZ9Wyf1hzupb+3k0OFO6lo7qD/cyaHWD5Y37m/mUGvHgJPsEN4qyUlLZEpOKr/86uKxGNZxFA4iImMoPi5EfkYy+RnJw+rf0d1Dw+Eu6g53UNfa2ec+HCShM7T1oHAQEYkgSfFxTMqMY1Lm8MJkrMTe8VkiIjIkhYOIiPSjcBARkX4UDiIi0o/CQURE+lE4iIhIPwoHERHpR+EgIiL9mBvooh9RwMxqgd2n+PRc4NAolhOk8TKW8TIO0Fgi1XgZy+mMY6pzLm84HaM2HE6HmZU758qCrmM0jJexjJdxgMYSqcbLWM7UOLRbSURE+lE4iIhIP7EaDg8GXcAoGi9jGS/jAI0lUo2XsZyRccTknIOIiAwuVrccRERkEDEVDma21My2mlmFmd0TdD3DYWa7zOw9M1tvZuW+LcfMVpjZdn+f7dvNzO7349tgZgsDrv1hMztoZu/3aRtx7Wa2zPffbmbLImgs3zazff6zWW9m1/ZZd68fy1Yzu7pPe6A/g2ZWYmavmtlmM9toZnf59qj7XAYZS1R9LmaWbGZvm9m7fhx/49unmdlq/+/7hJkl+vYk/7jCry8danynxDkXEzcgDqgEpgOJwLvA3KDrGkbdu4DcE9r+CbjHL98D/KNfvhZ4HjDgImB1wLVfDiwE3j/V2oEcYIe/z/bL2REylm8D/3uAvnP9z1cSMM3/3MVFws8gUAgs9MsZwDZfb9R9LoOMJao+F/9vm+6XE4DV/t/6SeBm3/5j4I/88teAH/vlm4EnBhvfqdYVS1sOi4AK59wO51wn8DhwQ8A1naobgEf88iPAJ/u0P+rCVgFZZlYYRIEAzrk3gPoTmkda+9XACudcvXOuAVgBLB376o93krGczA3A4865DufcTqCC8M9f4D+DzrkDzrl1frkF2AxMJgo/l0HGcjIR+bn4f9s5AB62AAACoklEQVRW/zDB3xxwJfCUbz/xMzn6WT0FXGVmxsnHd0piKRwmA3v7PK5i8B+kSOGAl8xsrZnd4dsKnHMHIPwfBMj37dEwxpHWHuljutPvbnn46K4YomQsfnfEeYT/Uo3qz+WEsUCUfS5mFmdm64GDhIO2Emh0znUPUNOxev36JmAiozyOWAqHgb6VOxoO1brEObcQuAb4upldPkjfaB0jnLz2SB7Tj4CzgAXAAeB7vj3ix2Jm6cCvgD9xzjUP1nWAtkgfS9R9Ls65HufcAqCY8F/7cwap6YyMI5bCoQoo6fO4GNgfUC3D5pzb7+8PAs8Q/sGpObq7yN8f9N2jYYwjrT1ix+Scq/H/qXuBn/DBJnxEj8XMEgj/Mn3MOfe0b47Kz2WgsUTr5wLgnGsEXiM855BlZvED1HSsXr8+k/Auz1EdRyyFwxpgpj8CIJHwRM7ygGsalJmlmVnG0WVgCfA+4bqPHh2yDHjWLy8HbvVHmFwENB3dVRBBRlr7i8ASM8v2uweW+LbAnTCf8ynCnw2Ex3KzP6pkGjATeJsI+Bn0+6YfAjY7577fZ1XUfS4nG0u0fS5mlmdmWX45Bfgo4fmTV4EbfbcTP5Ojn9WNwCsuPCN9svGdmjM1Ix8JN8JHXmwjvD/vW0HXM4x6pxM++uBdYOPRmgnvX3wZ2O7vc9wHRz084Mf3HlAWcP3/RXizvovwXzW3n0rtwG2EJ9cqgC9F0Fh+7mvd4P9jFvbp/y0/lq3ANZHyMwhcSnhXwwZgvb9dG42fyyBjiarPBZgHvOPrfR/4v759OuFf7hXAL4Ek357sH1f49dOHGt+p3HSGtIiI9BNLu5VERGSYFA4iItKPwkFERPpROIiISD8KBxER6UfhICIi/SgcRESkH4WDiIj08/8BNBuI+qYlxrMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs[-3000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3, Inspect Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pred, real):\n",
    "    right = np.sum(pred==real)\n",
    "    acc = right/real.shape[1]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy: 0.954735\n",
      "test set accuracy: 0.888333\n"
     ]
    }
   ],
   "source": [
    "predict = model['predict']\n",
    "pred = predict(model, train_X)\n",
    "pred = Y_to_labels(pred)\n",
    "acc = get_accuracy(pred, train_labels)\n",
    "print(\"training set accuracy: %f\" % acc)\n",
    "pred = predict(model, test_X)\n",
    "pred = Y_to_labels(pred)\n",
    "acc = get_accuracy(pred, test_labels)\n",
    "print(\"test set accuracy: %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"W-four-layer\", *W)\n",
    "np.savez(\"b-four-layer\", *b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 28000)\n"
     ]
    }
   ],
   "source": [
    "test_X = np.genfromtxt(\"test.csv\", delimiter=',', skip_header=1)\n",
    "test_X = test_X.T\n",
    "test_X = test_X/255\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28000)\n"
     ]
    }
   ],
   "source": [
    "test_Y = predict(test_X)\n",
    "test_labels = Y_to_labels(test_Y)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.zeros((test_labels.shape[1], 2), dtype=int)\n",
    "for i in range(output.shape[0]):\n",
    "    output[i, 0] = i+1\n",
    "    output[i, 1] = test_labels[0, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"4layer-submission.csv\", output, fmt=\"%d\", delimiter=',', header='ImageId,Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17346840320>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAADmBJREFUeJzt3X+s1fV9x/HX28vlUilmID8kwEZXqcVBi8sdltlNVrWzixs0bWnJtNiY3i6RuCYmm2PNarIsNcsq1cU0QaHFzB91bVXaGqojm6xpa7kQBSutOMb0FgYF6sBlwuXy3h/3S3ML93zO4Zzvr+v7+UjIPef7/n7P982B1/2ecz7n+/2YuwtAPBdU3QCAahB+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBjStzZ+OtxydoYpm7BEJ5U/+rk37CWlm3o/Cb2fWS7pHUJekBd78rtf4ETdSVdk0nuwSQ8JxvaXndtl/2m1mXpPskfUjS5ZJWmtnl7T4egHJ18p5/saRX3H2vu5+U9KikZfm0BaBonYR/lqTXRtwfyJb9CjPrM7N+M+sf1IkOdgcgT52Ef7QPFc45P9jd17l7r7v3dqung90ByFMn4R+QNGfE/dmS9nfWDoCydBL+bZLmmdk7zGy8pE9I2pRPWwCK1vZQn7ufMrPVkr6r4aG+De7+49w6A1Cojsb53f0pSU/l1AuAEvH1XiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4LqaJZeM9sn6bikIUmn3L03j6YAFK+j8Gf+wN0P5/A4AErEy34gqE7D75KeNrPtZtaXR0MAytHpy/6r3H2/mU2X9IyZ/cTdt45cIful0CdJE3Rhh7sDkJeOjvzuvj/7eUjS45IWj7LOOnfvdffebvV0sjsAOWo7/GY20cwmnbkt6YOSXsyrMQDF6uRl/wxJj5vZmcd52N0359IVgMK1HX533yvpvTn2gjZ1XXZpw9rBq6eV2Em99BzzhrVJj/6wxE7qiaE+ICjCDwRF+IGgCD8QFOEHgiL8QFB5nNWHDr3217+brJ+YejpZnzLvaMPas4vWttVTq7qtK1kf9KFC95+y4+SEhrVPLf6z5Laztqaf87c98aO2eqoTjvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBTj/Dk43LckWb/gT44k648uuDtZv6y7vmPpdXZlz2DD2gsrvpTc9r7rFibrm99cmqyP37wtWa8DjvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBTj/Dl4/d2NLxEtSS8serCkTpCXWyfvStb/efa1yfrFeTZTEI78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBU03F+M9sg6QZJh9x9QbZsiqSvSZoraZ+kFe7+i+LaLN4FEycm6//5F41nI3/p4/c0efT0+fjNHD99Mln/yv+8p6PH78Tm//6tZH3cta+W1Mm5fEnjf7Nvff2BEjupp1aO/F+VdP1Zy+6QtMXd50nakt0HMIY0Db+7b5V09pQwyyRtzG5vlLQ8574AFKzd9/wz3P2AJGU/p+fXEoAyFP7dfjPrk9QnSRN0YdG7A9Cido/8B81spiRlPw81WtHd17l7r7v3dqunzd0ByFu74d8kaVV2e5WkJ/NpB0BZmobfzB6R9ANJl5nZgJndIukuSdeZ2R5J12X3AYwhTd/zu/vKBqVrcu6lUkPvuTRZ77+l8bX1B9On83es2Tj+vyyYVGwDCeNU3Th+M+OOvNGwtnpgaXLbtbO2JOtHetNzJUx/In1G/9Dh9FwOZeAbfkBQhB8IivADQRF+ICjCDwRF+IGguHQ33rKGXv6PhrUd69PTqutv0kN9u264N1n/yIZPpx+foT4AVSH8QFCEHwiK8ANBEX4gKMIPBEX4gaAY5y/Bwm/flqxf3J++tPf44+lzhifph+fdU3Qz/q3hxackSVf/8SeT9WevGPvTrnPkB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOfP3PiV77S97cKnVyfr8z/X+LxyqR6XcY4mda6/JL2+533pB7giXV6x8Zlk/bH5l6QfoAQc+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqKbj/Ga2QdINkg65+4Js2Z2SPi3p59lqa9z9qaKaLMOfTkqf3/2TxDzcF+4Zn9yWcfz66Zo2LVn3qSeT9W5LX4PhxoteS9Yf09gY5/+qpOtHWb7W3Rdlf8Z08IGImobf3bdKOlpCLwBK1Ml7/tVmttPMNpjZ5Nw6AlCKdsP/ZUnvlLRI0gFJX2y0opn1mVm/mfUP6kSbuwOQt7bC7+4H3X3I3U9Lul/S4sS669y91917u9XTbp8ActZW+M1s5oi7H5b0Yj7tAChLK0N9j0haKmmqmQ1I+rykpWa2SJJL2ifpMwX2CKAATcPv7itHWby+gF4qdUpDyfpNO29uWJv9he/n3A3ycLhvScPa0d5TyW13feAfk/XE1z4kSR/56UfTK2igSb14fMMPCIrwA0ERfiAowg8ERfiBoAg/EBSX7m7RR+c+37C2efnVyW3f9sSP8m4nhNRQnSS9/u70eNtLH7+3YW3Q00O7HfurZqe7MNQHoCKEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4/wtum3KCw1rXX97Ornt5jeXJuvjN29rp6VSvHz/7yTrM2b9IlkfOt3+8WXNux5K1v/wwvTl1qX05bU7sfDbtyXr8/c2mZY9z2baxJEfCIrwA0ERfiAowg8ERfiBoAg/EBThB4JinD8Ht07elaxfeu/BZH3vyfR00d2WHhUe9OLGs9dftDZZn9aVnoWp8PPmC7Lw6dXJ+vzPNRnHHwPTsnPkB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgzD197XMzmyPpQUmXSDotaZ2732NmUyR9TdJcSfskrXD35MndF9kUv9KuyaHt/PmS9ybr3/r6AyV1cq5xTc5Lbza9eJF6rDtZP+GDhe37n47NSdYf/tnihrVx176adzu18Jxv0TE/aq2s28qR/5Sk2919vqT3SbrVzC6XdIekLe4+T9KW7D6AMaJp+N39gLvvyG4fl7Rb0ixJyyRtzFbbKGl5UU0CyN95vec3s7mSrpD0nKQZ7n5AGv4FIWl63s0BKE7L4Tezt0v6hqTPuvux89iuz8z6zax/UCfa6RFAAVoKv5l1azj4D7n7N7PFB81sZlafKWnUqym6+zp373X33m6lTwIBUJ6m4Tczk7Re0m53v3tEaZOkVdntVZKezL89AEVp5ZTeqyTdJGmXmZ2Zp3qNpLskPWZmt0h6VdLHimmxHOOOvJGsL9l+Y8NaavpuKX3Z75Y0Gbip8rTZVwZPJes37by5sH1fcnt6GHHcnr2F7futoGn43f17avzfr56D9gCa4ht+QFCEHwiK8ANBEX4gKMIPBEX4gaCantKbpzqf0tuJ/1ve+NRRSdr//s5+x56emh7P3nntfW0/9u9tvzlZP77n15L1niPpv9vsL3z/fFtCB/I+pRfAWxDhB4Ii/EBQhB8IivADQRF+ICjCDwTFOP8Y0DX14mT90PJ3tf3Y059NTx8+xDnxYwrj/ACaIvxAUIQfCIrwA0ERfiAowg8ERfiBoFq5bj8qNnT4SLJ+8QM/aP+x294SYx1HfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8Iqmn4zWyOmf2rme02sx+b2Z9ny+80s5+Z2fPZnz8qvl0AeWnlSz6nJN3u7jvMbJKk7Wb2TFZb6+7/UFx7AIrSNPzufkDSgez2cTPbLWlW0Y0BKNZ5vec3s7mSrpD0XLZotZntNLMNZja5wTZ9ZtZvZv2DOtFRswDy03L4zeztkr4h6bPufkzSlyW9U9IiDb8y+OJo27n7OnfvdffebvXk0DKAPLQUfjPr1nDwH3L3b0qSux909yF3Py3pfknp2SoB1Eorn/abpPWSdrv73SOWzxyx2oclvZh/ewCK0sqn/VdJuknSLjN7Plu2RtJKM1skySXtk/SZQjoEUIhWPu3/nqTRrgP+VP7tACgL3/ADgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EZe5e3s7Mfi7pv0YsmirpcGkNnJ+69lbXviR6a1eevf2Gu09rZcVSw3/Ozs363b23sgYS6tpbXfuS6K1dVfXGy34gKMIPBFV1+NdVvP+UuvZW174kemtXJb1V+p4fQHWqPvIDqEgl4Tez683sp2b2ipndUUUPjZjZPjPblc083F9xLxvM7JCZvThi2RQze8bM9mQ/R50mraLeajFzc2Jm6Uqfu7rNeF36y34z65L0sqTrJA1I2iZppbu/VGojDZjZPkm97l75mLCZ/b6kNyQ96O4LsmV/L+mou9+V/eKc7O5/WZPe7pT0RtUzN2cTyswcObO0pOWSblaFz12irxWq4Hmr4si/WNIr7r7X3U9KelTSsgr6qD133yrp6FmLl0namN3eqOH/PKVr0FstuPsBd9+R3T4u6czM0pU+d4m+KlFF+GdJem3E/QHVa8pvl/S0mW03s76qmxnFjGza9DPTp0+vuJ+zNZ25uUxnzSxdm+eunRmv81ZF+Eeb/adOQw5XuftvS/qQpFuzl7doTUszN5dllJmla6HdGa/zVkX4ByTNGXF/tqT9FfQxKnffn/08JOlx1W/24YNnJknNfh6quJ9fqtPMzaPNLK0aPHd1mvG6ivBvkzTPzN5hZuMlfULSpgr6OIeZTcw+iJGZTZT0QdVv9uFNklZlt1dJerLCXn5FXWZubjSztCp+7uo243UlX/LJhjK+JKlL0gZ3/7vSmxiFmf2mho/20vAkpg9X2ZuZPSJpqYbP+joo6fOSnpD0mKRfl/SqpI+5e+kfvDXobamGX7r+cubmM++xS+7t/ZL+XdIuSaezxWs0/P66sucu0ddKVfC88Q0/ICi+4QcERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+IKj/B9AKFnIxYPbGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_images = X_to_images(test_X)\n",
    "plt.imshow(test_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
