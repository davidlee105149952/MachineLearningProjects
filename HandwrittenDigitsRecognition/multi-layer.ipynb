{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digits Recognition Using Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data comes from the \"MNIST\" data set, you can download it from [Kaggle](https://www.kaggle.com/c/digit-recognizer/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1, Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"train.csv\", delimiter=',', skip_header=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "[2. 1. 5. 4. 5. 2. 2. 7. 2. 5.]\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "print(data.shape)\n",
    "print(data[:10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 42000)\n",
      "(1, 42000)\n"
     ]
    }
   ],
   "source": [
    "features = data[:, 1:].T\n",
    "labels = data[:, 0]\n",
    "labels = np.reshape(labels, (1, -1))\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = features.shape[1]\n",
    "nx = features.shape[0]\n",
    "ny = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_Y(labels):\n",
    "    Y = np.zeros((ny, m))\n",
    "    for i in range(m):\n",
    "        Y[int(labels[0, i]), i] = 1\n",
    "    return Y\n",
    "\n",
    "def Y_to_labels(Y):\n",
    "    labels = np.argmax(Y, axis=0).astype(float)\n",
    "    labels = np.reshape(labels, (1, -1))\n",
    "    return labels\n",
    "\n",
    "def X_to_images(X):\n",
    "    images = [np.reshape(X[:, i], (28, 28)) for i in range(X.shape[1])]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 42000)\n",
      "2.0\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = features / 255\n",
    "Y = labels_to_Y(labels)\n",
    "images = X_to_images(X)\n",
    "print(Y.shape)\n",
    "plt.imshow(images[0])\n",
    "print(labels[0, 0])\n",
    "print(Y[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 37800)\n",
      "(10, 37800)\n",
      "(784, 4200)\n",
      "(10, 4200)\n",
      "(1, 37800)\n"
     ]
    }
   ],
   "source": [
    "test_ratio = 1 - train_ratio\n",
    "train_m = int(m * train_ratio)\n",
    "test_m = m - train_m\n",
    "train_X = X[:, :train_m]\n",
    "test_X = X[:, train_m:]\n",
    "train_Y = Y[:, :train_m]\n",
    "test_Y = Y[:, train_m:]\n",
    "train_labels = Y_to_labels(train_Y)\n",
    "test_labels = Y_to_labels(test_Y)\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2, Design Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    a = 1/ (1 + np.exp(-z))\n",
    "    return a\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    p1 = np.exp(-z)\n",
    "    a = p1/((1+p1)**2)\n",
    "    return a\n",
    "\n",
    "def relu(z):\n",
    "    a = np.maximum(z, 0.01*z)\n",
    "    return a\n",
    "\n",
    "def relu_prime(z):\n",
    "    a = np.where(z > 0, 1, 0.01)\n",
    "    return a\n",
    "\n",
    "def tanh(z):\n",
    "    p1 = np.exp(z)\n",
    "    p2 = np.exp(-z)\n",
    "    a = (p1-p2)/(p1+p2)\n",
    "    return a\n",
    "    \n",
    "def tanh_prime(z):\n",
    "    p1 = tanh(z)\n",
    "    a = 1-p1**2\n",
    "    return a\n",
    "\n",
    "def softmax(z):\n",
    "    t = np.exp(z)\n",
    "    n = np.sum(t, axis = 0)\n",
    "    a = t/n\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_for__config_multi_layer_classifier_without_regularization(Y_hat, Y):\n",
    "    delta = 1e-10\n",
    "    l = np.sum(-Y*np.log(Y_hat+delta), axis=0)\n",
    "    return l\n",
    "\n",
    "def forward_propagation_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    X = model['X']\n",
    "    Y = model['Y']\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f = model['f']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    costs = model['costs']\n",
    "    loss_function = model['loss_function']\n",
    "    A[0] = X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    costs.append(np.sum(loss_function(A[L], Y)))\n",
    "\n",
    "def back_propagation_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    n = model['n']\n",
    "    L = model['L']\n",
    "    f_prime = model['f_prime']\n",
    "    m = model['m']\n",
    "    W = model['W']\n",
    "    Z = model['Z']\n",
    "    A = model['A']\n",
    "    Y = model['Y']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    dA = model['dA']\n",
    "    dZ = model['dZ']\n",
    "    dZ[L] = A[L] - Y\n",
    "    dW[L] = np.matmul(dZ[L], A[L-1].T)/m\n",
    "    db[L] = np.sum(dZ[L], axis=1, keepdims=True)/m\n",
    "    for i in reversed(range(1, L)):\n",
    "        dZ[i] = np.matmul(W[i+1].T, dZ[i+1]) * f_prime[i](Z[i])\n",
    "        dW[i] = np.matmul(dZ[i], A[i-1].T)/m\n",
    "        db[i] = np.sum(dZ[i], axis=1, keepdims=True)/m\n",
    "        \n",
    "def update_for_config_multi_layer_classifier_without_regularization(model):\n",
    "    L = model['L']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    dW = model['dW']\n",
    "    db = model['db']\n",
    "    learning_rate = model['learning_rate']\n",
    "    for i in range(1, L+1):\n",
    "        W[i] -= learning_rate*dW[i]\n",
    "        b[i] -= learning_rate*db[i]\n",
    "    \n",
    "    \n",
    "def predict_for_config_multi_layer_classifier_without_regularization(model, test_X):\n",
    "    L = model['L']\n",
    "    A = model['A']\n",
    "    Z = model['Z']\n",
    "    W = model['W']\n",
    "    b = model['b']\n",
    "    f = model['f']\n",
    "    A[0] = test_X\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.matmul(W[i], A[i-1]) + b[i]\n",
    "        A[i] = f[i](Z[i])\n",
    "    return A[L]\n",
    "    \n",
    "    \n",
    "def config_multi_layer_classifier_without_regularization(X, Y, neuron_of_hidden_layer, learning_rate):\n",
    "    n_input_layer = X.shape[0]\n",
    "    n_output_layer = Y.shape[0]\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = [n_input_layer] + neuron_of_hidden_layer + [n_output_layer]\n",
    "    L = len(n)-1\n",
    "    f = [tanh if i != 0 else None for i in range(L)]\n",
    "    f.append(softmax)\n",
    "    f_prime = [tanh_prime if i != 0 else None for i in range(L)]\n",
    "    f_prime.append(None)\n",
    "    W = [np.random.normal(0, 1, (n[i], n[i-1])) if i != 0 else None for i in range(L+1)]\n",
    "    b = [np.random.normal(0, 1, (n[i], 1)) if i != 0 else None for i in range(L+1)]\n",
    "    Z = [None for i in range(L+1)]\n",
    "    A = [None for i in range(L+1)]\n",
    "    dW = [None for i in range(L+1)]\n",
    "    db = [None for i in range(L+1)]\n",
    "    dA = [None for i in range(L+1)]\n",
    "    dZ = [None for i in range(L+1)]\n",
    "    costs = []\n",
    "    \n",
    "    model = dict()\n",
    "    model['X'] = X\n",
    "    model['Y'] = Y\n",
    "    model['m'] = m\n",
    "    model['n'] = n\n",
    "    model['L'] = L\n",
    "    model['f'] = f\n",
    "    model['f_prime'] = f_prime\n",
    "    model['W'] = W\n",
    "    model['b'] = b\n",
    "    model['Z'] = Z\n",
    "    model['A'] = A\n",
    "    model['dW'] = dW\n",
    "    model['db'] = db\n",
    "    model['dA'] = dA\n",
    "    model['dZ'] = dZ\n",
    "    model['loss_function'] = loss_function_for__config_multi_layer_classifier_without_regularization\n",
    "    model['costs'] = costs\n",
    "    model['learning_rate'] = learning_rate\n",
    "    model['forwardprop'] = forward_propagation_for_config_multi_layer_classifier_without_regularization\n",
    "    model['backprop'] = back_propagation_for_config_multi_layer_classifier_without_regularization\n",
    "    model['update'] = update_for_config_multi_layer_classifier_without_regularization\n",
    "    model['predict'] = predict_for_config_multi_layer_classifier_without_regularization\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(iteration_times, model):\n",
    "    forwardprop = model['forwardprop']\n",
    "    backprop = model['backprop']\n",
    "    update = model['update']\n",
    "    costs = model['costs']\n",
    "    for i in range(iteration_times):\n",
    "        forwardprop(model)\n",
    "        backprop(model)\n",
    "        update(model)\n",
    "        print(\"iteration %d, current loss: %f\" % (i, costs[len(costs)-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = config_multi_layer_classifier_without_regularization(train_X, train_Y, [28], 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, current loss: 5411.901852\n",
      "iteration 1, current loss: 5411.742682\n",
      "iteration 2, current loss: 5411.583529\n",
      "iteration 3, current loss: 5411.424393\n",
      "iteration 4, current loss: 5411.265275\n",
      "iteration 5, current loss: 5411.106175\n",
      "iteration 6, current loss: 5410.947092\n",
      "iteration 7, current loss: 5410.788026\n",
      "iteration 8, current loss: 5410.628978\n",
      "iteration 9, current loss: 5410.469947\n",
      "iteration 10, current loss: 5410.310934\n",
      "iteration 11, current loss: 5410.151938\n",
      "iteration 12, current loss: 5409.992960\n",
      "iteration 13, current loss: 5409.833999\n",
      "iteration 14, current loss: 5409.675056\n",
      "iteration 15, current loss: 5409.516130\n",
      "iteration 16, current loss: 5409.357222\n",
      "iteration 17, current loss: 5409.198332\n",
      "iteration 18, current loss: 5409.039459\n",
      "iteration 19, current loss: 5408.880603\n",
      "iteration 20, current loss: 5408.721766\n",
      "iteration 21, current loss: 5408.562946\n",
      "iteration 22, current loss: 5408.404143\n",
      "iteration 23, current loss: 5408.245358\n",
      "iteration 24, current loss: 5408.086591\n",
      "iteration 25, current loss: 5407.927842\n",
      "iteration 26, current loss: 5407.769110\n",
      "iteration 27, current loss: 5407.610396\n",
      "iteration 28, current loss: 5407.451700\n",
      "iteration 29, current loss: 5407.293021\n",
      "iteration 30, current loss: 5407.134361\n",
      "iteration 31, current loss: 5406.975718\n",
      "iteration 32, current loss: 5406.817093\n",
      "iteration 33, current loss: 5406.658485\n",
      "iteration 34, current loss: 5406.499896\n",
      "iteration 35, current loss: 5406.341324\n",
      "iteration 36, current loss: 5406.182770\n",
      "iteration 37, current loss: 5406.024234\n",
      "iteration 38, current loss: 5405.865716\n",
      "iteration 39, current loss: 5405.707215\n",
      "iteration 40, current loss: 5405.548733\n",
      "iteration 41, current loss: 5405.390268\n",
      "iteration 42, current loss: 5405.231822\n",
      "iteration 43, current loss: 5405.073393\n",
      "iteration 44, current loss: 5404.914983\n",
      "iteration 45, current loss: 5404.756590\n",
      "iteration 46, current loss: 5404.598215\n",
      "iteration 47, current loss: 5404.439859\n",
      "iteration 48, current loss: 5404.281520\n",
      "iteration 49, current loss: 5404.123200\n",
      "iteration 50, current loss: 5403.964897\n",
      "iteration 51, current loss: 5403.806613\n",
      "iteration 52, current loss: 5403.648346\n",
      "iteration 53, current loss: 5403.490098\n",
      "iteration 54, current loss: 5403.331868\n",
      "iteration 55, current loss: 5403.173656\n",
      "iteration 56, current loss: 5403.015463\n",
      "iteration 57, current loss: 5402.857287\n",
      "iteration 58, current loss: 5402.699130\n",
      "iteration 59, current loss: 5402.540991\n",
      "iteration 60, current loss: 5402.382870\n",
      "iteration 61, current loss: 5402.224767\n",
      "iteration 62, current loss: 5402.066683\n",
      "iteration 63, current loss: 5401.908617\n",
      "iteration 64, current loss: 5401.750569\n",
      "iteration 65, current loss: 5401.592540\n",
      "iteration 66, current loss: 5401.434529\n",
      "iteration 67, current loss: 5401.276536\n",
      "iteration 68, current loss: 5401.118562\n",
      "iteration 69, current loss: 5400.960606\n",
      "iteration 70, current loss: 5400.802669\n",
      "iteration 71, current loss: 5400.644750\n",
      "iteration 72, current loss: 5400.486849\n",
      "iteration 73, current loss: 5400.328967\n",
      "iteration 74, current loss: 5400.171104\n",
      "iteration 75, current loss: 5400.013259\n",
      "iteration 76, current loss: 5399.855432\n",
      "iteration 77, current loss: 5399.697625\n",
      "iteration 78, current loss: 5399.539835\n",
      "iteration 79, current loss: 5399.382065\n",
      "iteration 80, current loss: 5399.224313\n",
      "iteration 81, current loss: 5399.066579\n",
      "iteration 82, current loss: 5398.908865\n",
      "iteration 83, current loss: 5398.751169\n",
      "iteration 84, current loss: 5398.593491\n",
      "iteration 85, current loss: 5398.435833\n",
      "iteration 86, current loss: 5398.278193\n",
      "iteration 87, current loss: 5398.120572\n",
      "iteration 88, current loss: 5397.962970\n",
      "iteration 89, current loss: 5397.805386\n",
      "iteration 90, current loss: 5397.647822\n",
      "iteration 91, current loss: 5397.490276\n",
      "iteration 92, current loss: 5397.332749\n",
      "iteration 93, current loss: 5397.175242\n",
      "iteration 94, current loss: 5397.017753\n",
      "iteration 95, current loss: 5396.860283\n",
      "iteration 96, current loss: 5396.702832\n",
      "iteration 97, current loss: 5396.545399\n",
      "iteration 98, current loss: 5396.387986\n",
      "iteration 99, current loss: 5396.230593\n",
      "iteration 100, current loss: 5396.073218\n",
      "iteration 101, current loss: 5395.915862\n",
      "iteration 102, current loss: 5395.758525\n",
      "iteration 103, current loss: 5395.601207\n",
      "iteration 104, current loss: 5395.443909\n",
      "iteration 105, current loss: 5395.286630\n",
      "iteration 106, current loss: 5395.129370\n",
      "iteration 107, current loss: 5394.972129\n",
      "iteration 108, current loss: 5394.814907\n",
      "iteration 109, current loss: 5394.657705\n",
      "iteration 110, current loss: 5394.500522\n",
      "iteration 111, current loss: 5394.343358\n",
      "iteration 112, current loss: 5394.186214\n",
      "iteration 113, current loss: 5394.029089\n",
      "iteration 114, current loss: 5393.871983\n",
      "iteration 115, current loss: 5393.714897\n",
      "iteration 116, current loss: 5393.557830\n",
      "iteration 117, current loss: 5393.400783\n",
      "iteration 118, current loss: 5393.243755\n",
      "iteration 119, current loss: 5393.086747\n",
      "iteration 120, current loss: 5392.929758\n",
      "iteration 121, current loss: 5392.772789\n",
      "iteration 122, current loss: 5392.615839\n",
      "iteration 123, current loss: 5392.458909\n",
      "iteration 124, current loss: 5392.301999\n",
      "iteration 125, current loss: 5392.145108\n",
      "iteration 126, current loss: 5391.988237\n",
      "iteration 127, current loss: 5391.831385\n",
      "iteration 128, current loss: 5391.674554\n",
      "iteration 129, current loss: 5391.517742\n",
      "iteration 130, current loss: 5391.360949\n",
      "iteration 131, current loss: 5391.204177\n",
      "iteration 132, current loss: 5391.047424\n",
      "iteration 133, current loss: 5390.890692\n",
      "iteration 134, current loss: 5390.733979\n",
      "iteration 135, current loss: 5390.577286\n",
      "iteration 136, current loss: 5390.420613\n",
      "iteration 137, current loss: 5390.263959\n",
      "iteration 138, current loss: 5390.107326\n",
      "iteration 139, current loss: 5389.950713\n",
      "iteration 140, current loss: 5389.794120\n",
      "iteration 141, current loss: 5389.637546\n",
      "iteration 142, current loss: 5389.480993\n",
      "iteration 143, current loss: 5389.324460\n",
      "iteration 144, current loss: 5389.167947\n",
      "iteration 145, current loss: 5389.011454\n",
      "iteration 146, current loss: 5388.854981\n",
      "iteration 147, current loss: 5388.698529\n",
      "iteration 148, current loss: 5388.542096\n",
      "iteration 149, current loss: 5388.385684\n",
      "iteration 150, current loss: 5388.229292\n",
      "iteration 151, current loss: 5388.072921\n",
      "iteration 152, current loss: 5387.916569\n",
      "iteration 153, current loss: 5387.760238\n",
      "iteration 154, current loss: 5387.603927\n",
      "iteration 155, current loss: 5387.447637\n",
      "iteration 156, current loss: 5387.291367\n",
      "iteration 157, current loss: 5387.135117\n",
      "iteration 158, current loss: 5386.978888\n",
      "iteration 159, current loss: 5386.822679\n",
      "iteration 160, current loss: 5386.666491\n",
      "iteration 161, current loss: 5386.510323\n",
      "iteration 162, current loss: 5386.354176\n",
      "iteration 163, current loss: 5386.198049\n",
      "iteration 164, current loss: 5386.041943\n",
      "iteration 165, current loss: 5385.885857\n",
      "iteration 166, current loss: 5385.729792\n",
      "iteration 167, current loss: 5385.573748\n",
      "iteration 168, current loss: 5385.417724\n",
      "iteration 169, current loss: 5385.261721\n",
      "iteration 170, current loss: 5385.105738\n",
      "iteration 171, current loss: 5384.949777\n",
      "iteration 172, current loss: 5384.793836\n",
      "iteration 173, current loss: 5384.637916\n",
      "iteration 174, current loss: 5384.482016\n",
      "iteration 175, current loss: 5384.326137\n",
      "iteration 176, current loss: 5384.170280\n",
      "iteration 177, current loss: 5384.014443\n",
      "iteration 178, current loss: 5383.858626\n",
      "iteration 179, current loss: 5383.702831\n",
      "iteration 180, current loss: 5383.547057\n",
      "iteration 181, current loss: 5383.391303\n",
      "iteration 182, current loss: 5383.235571\n",
      "iteration 183, current loss: 5383.079859\n",
      "iteration 184, current loss: 5382.924169\n",
      "iteration 185, current loss: 5382.768499\n",
      "iteration 186, current loss: 5382.612851\n",
      "iteration 187, current loss: 5382.457223\n",
      "iteration 188, current loss: 5382.301617\n",
      "iteration 189, current loss: 5382.146031\n",
      "iteration 190, current loss: 5381.990467\n",
      "iteration 191, current loss: 5381.834924\n",
      "iteration 192, current loss: 5381.679402\n",
      "iteration 193, current loss: 5381.523901\n",
      "iteration 194, current loss: 5381.368421\n",
      "iteration 195, current loss: 5381.212963\n",
      "iteration 196, current loss: 5381.057525\n",
      "iteration 197, current loss: 5380.902109\n",
      "iteration 198, current loss: 5380.746715\n",
      "iteration 199, current loss: 5380.591341\n",
      "iteration 200, current loss: 5380.435989\n",
      "iteration 201, current loss: 5380.280658\n",
      "iteration 202, current loss: 5380.125348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 203, current loss: 5379.970060\n",
      "iteration 204, current loss: 5379.814793\n",
      "iteration 205, current loss: 5379.659547\n",
      "iteration 206, current loss: 5379.504323\n",
      "iteration 207, current loss: 5379.349120\n",
      "iteration 208, current loss: 5379.193938\n",
      "iteration 209, current loss: 5379.038778\n",
      "iteration 210, current loss: 5378.883640\n",
      "iteration 211, current loss: 5378.728523\n",
      "iteration 212, current loss: 5378.573427\n",
      "iteration 213, current loss: 5378.418353\n",
      "iteration 214, current loss: 5378.263300\n",
      "iteration 215, current loss: 5378.108269\n",
      "iteration 216, current loss: 5377.953259\n",
      "iteration 217, current loss: 5377.798271\n",
      "iteration 218, current loss: 5377.643305\n",
      "iteration 219, current loss: 5377.488360\n",
      "iteration 220, current loss: 5377.333437\n",
      "iteration 221, current loss: 5377.178535\n",
      "iteration 222, current loss: 5377.023655\n",
      "iteration 223, current loss: 5376.868796\n",
      "iteration 224, current loss: 5376.713960\n",
      "iteration 225, current loss: 5376.559145\n",
      "iteration 226, current loss: 5376.404351\n",
      "iteration 227, current loss: 5376.249579\n",
      "iteration 228, current loss: 5376.094829\n",
      "iteration 229, current loss: 5375.940101\n",
      "iteration 230, current loss: 5375.785394\n",
      "iteration 231, current loss: 5375.630709\n",
      "iteration 232, current loss: 5375.476046\n",
      "iteration 233, current loss: 5375.321405\n",
      "iteration 234, current loss: 5375.166785\n",
      "iteration 235, current loss: 5375.012187\n",
      "iteration 236, current loss: 5374.857611\n",
      "iteration 237, current loss: 5374.703056\n",
      "iteration 238, current loss: 5374.548524\n",
      "iteration 239, current loss: 5374.394013\n",
      "iteration 240, current loss: 5374.239524\n",
      "iteration 241, current loss: 5374.085057\n",
      "iteration 242, current loss: 5373.930612\n",
      "iteration 243, current loss: 5373.776189\n",
      "iteration 244, current loss: 5373.621787\n",
      "iteration 245, current loss: 5373.467408\n",
      "iteration 246, current loss: 5373.313050\n",
      "iteration 247, current loss: 5373.158714\n",
      "iteration 248, current loss: 5373.004400\n",
      "iteration 249, current loss: 5372.850108\n",
      "iteration 250, current loss: 5372.695838\n",
      "iteration 251, current loss: 5372.541590\n",
      "iteration 252, current loss: 5372.387364\n",
      "iteration 253, current loss: 5372.233159\n",
      "iteration 254, current loss: 5372.078977\n",
      "iteration 255, current loss: 5371.924817\n",
      "iteration 256, current loss: 5371.770678\n",
      "iteration 257, current loss: 5371.616562\n",
      "iteration 258, current loss: 5371.462467\n",
      "iteration 259, current loss: 5371.308395\n",
      "iteration 260, current loss: 5371.154344\n",
      "iteration 261, current loss: 5371.000315\n",
      "iteration 262, current loss: 5370.846309\n",
      "iteration 263, current loss: 5370.692324\n",
      "iteration 264, current loss: 5370.538362\n",
      "iteration 265, current loss: 5370.384421\n",
      "iteration 266, current loss: 5370.230502\n",
      "iteration 267, current loss: 5370.076606\n",
      "iteration 268, current loss: 5369.922731\n",
      "iteration 269, current loss: 5369.768879\n",
      "iteration 270, current loss: 5369.615048\n",
      "iteration 271, current loss: 5369.461240\n",
      "iteration 272, current loss: 5369.307453\n",
      "iteration 273, current loss: 5369.153689\n",
      "iteration 274, current loss: 5368.999946\n",
      "iteration 275, current loss: 5368.846226\n",
      "iteration 276, current loss: 5368.692528\n",
      "iteration 277, current loss: 5368.538851\n",
      "iteration 278, current loss: 5368.385197\n",
      "iteration 279, current loss: 5368.231565\n",
      "iteration 280, current loss: 5368.077955\n",
      "iteration 281, current loss: 5367.924367\n",
      "iteration 282, current loss: 5367.770801\n",
      "iteration 283, current loss: 5367.617257\n",
      "iteration 284, current loss: 5367.463735\n",
      "iteration 285, current loss: 5367.310235\n",
      "iteration 286, current loss: 5367.156757\n",
      "iteration 287, current loss: 5367.003301\n",
      "iteration 288, current loss: 5366.849868\n",
      "iteration 289, current loss: 5366.696456\n",
      "iteration 290, current loss: 5366.543066\n",
      "iteration 291, current loss: 5366.389699\n",
      "iteration 292, current loss: 5366.236353\n",
      "iteration 293, current loss: 5366.083030\n",
      "iteration 294, current loss: 5365.929728\n",
      "iteration 295, current loss: 5365.776449\n",
      "iteration 296, current loss: 5365.623192\n",
      "iteration 297, current loss: 5365.469957\n",
      "iteration 298, current loss: 5365.316743\n",
      "iteration 299, current loss: 5365.163552\n",
      "iteration 300, current loss: 5365.010383\n",
      "iteration 301, current loss: 5364.857236\n",
      "iteration 302, current loss: 5364.704111\n",
      "iteration 303, current loss: 5364.551008\n",
      "iteration 304, current loss: 5364.397927\n",
      "iteration 305, current loss: 5364.244868\n",
      "iteration 306, current loss: 5364.091831\n",
      "iteration 307, current loss: 5363.938816\n",
      "iteration 308, current loss: 5363.785823\n",
      "iteration 309, current loss: 5363.632852\n",
      "iteration 310, current loss: 5363.479903\n",
      "iteration 311, current loss: 5363.326976\n",
      "iteration 312, current loss: 5363.174071\n",
      "iteration 313, current loss: 5363.021189\n",
      "iteration 314, current loss: 5362.868328\n",
      "iteration 315, current loss: 5362.715489\n",
      "iteration 316, current loss: 5362.562672\n",
      "iteration 317, current loss: 5362.409877\n",
      "iteration 318, current loss: 5362.257104\n",
      "iteration 319, current loss: 5362.104353\n",
      "iteration 320, current loss: 5361.951624\n",
      "iteration 321, current loss: 5361.798916\n",
      "iteration 322, current loss: 5361.646231\n",
      "iteration 323, current loss: 5361.493568\n",
      "iteration 324, current loss: 5361.340927\n",
      "iteration 325, current loss: 5361.188307\n",
      "iteration 326, current loss: 5361.035710\n",
      "iteration 327, current loss: 5360.883134\n",
      "iteration 328, current loss: 5360.730580\n",
      "iteration 329, current loss: 5360.578049\n",
      "iteration 330, current loss: 5360.425539\n",
      "iteration 331, current loss: 5360.273051\n",
      "iteration 332, current loss: 5360.120585\n",
      "iteration 333, current loss: 5359.968140\n",
      "iteration 334, current loss: 5359.815718\n",
      "iteration 335, current loss: 5359.663317\n",
      "iteration 336, current loss: 5359.510939\n",
      "iteration 337, current loss: 5359.358582\n",
      "iteration 338, current loss: 5359.206247\n",
      "iteration 339, current loss: 5359.053933\n",
      "iteration 340, current loss: 5358.901642\n",
      "iteration 341, current loss: 5358.749372\n",
      "iteration 342, current loss: 5358.597124\n",
      "iteration 343, current loss: 5358.444898\n",
      "iteration 344, current loss: 5358.292694\n",
      "iteration 345, current loss: 5358.140512\n",
      "iteration 346, current loss: 5357.988351\n",
      "iteration 347, current loss: 5357.836212\n",
      "iteration 348, current loss: 5357.684095\n",
      "iteration 349, current loss: 5357.531999\n",
      "iteration 350, current loss: 5357.379925\n",
      "iteration 351, current loss: 5357.227873\n",
      "iteration 352, current loss: 5357.075843\n",
      "iteration 353, current loss: 5356.923834\n",
      "iteration 354, current loss: 5356.771847\n",
      "iteration 355, current loss: 5356.619881\n",
      "iteration 356, current loss: 5356.467937\n",
      "iteration 357, current loss: 5356.316015\n",
      "iteration 358, current loss: 5356.164115\n",
      "iteration 359, current loss: 5356.012236\n",
      "iteration 360, current loss: 5355.860379\n",
      "iteration 361, current loss: 5355.708543\n",
      "iteration 362, current loss: 5355.556729\n",
      "iteration 363, current loss: 5355.404936\n",
      "iteration 364, current loss: 5355.253165\n",
      "iteration 365, current loss: 5355.101416\n",
      "iteration 366, current loss: 5354.949688\n",
      "iteration 367, current loss: 5354.797982\n",
      "iteration 368, current loss: 5354.646297\n",
      "iteration 369, current loss: 5354.494633\n",
      "iteration 370, current loss: 5354.342992\n",
      "iteration 371, current loss: 5354.191371\n",
      "iteration 372, current loss: 5354.039772\n",
      "iteration 373, current loss: 5353.888195\n",
      "iteration 374, current loss: 5353.736639\n",
      "iteration 375, current loss: 5353.585104\n",
      "iteration 376, current loss: 5353.433591\n",
      "iteration 377, current loss: 5353.282099\n",
      "iteration 378, current loss: 5353.130629\n",
      "iteration 379, current loss: 5352.979180\n",
      "iteration 380, current loss: 5352.827752\n",
      "iteration 381, current loss: 5352.676346\n",
      "iteration 382, current loss: 5352.524961\n",
      "iteration 383, current loss: 5352.373597\n",
      "iteration 384, current loss: 5352.222255\n",
      "iteration 385, current loss: 5352.070934\n",
      "iteration 386, current loss: 5351.919634\n",
      "iteration 387, current loss: 5351.768355\n",
      "iteration 388, current loss: 5351.617098\n",
      "iteration 389, current loss: 5351.465862\n",
      "iteration 390, current loss: 5351.314647\n",
      "iteration 391, current loss: 5351.163454\n",
      "iteration 392, current loss: 5351.012281\n",
      "iteration 393, current loss: 5350.861130\n",
      "iteration 394, current loss: 5350.710000\n",
      "iteration 395, current loss: 5350.558891\n",
      "iteration 396, current loss: 5350.407803\n",
      "iteration 397, current loss: 5350.256737\n",
      "iteration 398, current loss: 5350.105691\n",
      "iteration 399, current loss: 5349.954667\n",
      "iteration 400, current loss: 5349.803663\n",
      "iteration 401, current loss: 5349.652681\n",
      "iteration 402, current loss: 5349.501720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 403, current loss: 5349.350779\n",
      "iteration 404, current loss: 5349.199860\n",
      "iteration 405, current loss: 5349.048962\n",
      "iteration 406, current loss: 5348.898085\n",
      "iteration 407, current loss: 5348.747228\n",
      "iteration 408, current loss: 5348.596393\n",
      "iteration 409, current loss: 5348.445579\n",
      "iteration 410, current loss: 5348.294785\n",
      "iteration 411, current loss: 5348.144013\n",
      "iteration 412, current loss: 5347.993261\n",
      "iteration 413, current loss: 5347.842530\n",
      "iteration 414, current loss: 5347.691820\n",
      "iteration 415, current loss: 5347.541131\n",
      "iteration 416, current loss: 5347.390463\n",
      "iteration 417, current loss: 5347.239816\n",
      "iteration 418, current loss: 5347.089189\n",
      "iteration 419, current loss: 5346.938583\n",
      "iteration 420, current loss: 5346.787998\n",
      "iteration 421, current loss: 5346.637434\n",
      "iteration 422, current loss: 5346.486890\n",
      "iteration 423, current loss: 5346.336367\n",
      "iteration 424, current loss: 5346.185865\n",
      "iteration 425, current loss: 5346.035383\n",
      "iteration 426, current loss: 5345.884923\n",
      "iteration 427, current loss: 5345.734482\n",
      "iteration 428, current loss: 5345.584063\n",
      "iteration 429, current loss: 5345.433664\n",
      "iteration 430, current loss: 5345.283286\n",
      "iteration 431, current loss: 5345.132928\n",
      "iteration 432, current loss: 5344.982591\n",
      "iteration 433, current loss: 5344.832274\n",
      "iteration 434, current loss: 5344.681978\n",
      "iteration 435, current loss: 5344.531703\n",
      "iteration 436, current loss: 5344.381448\n",
      "iteration 437, current loss: 5344.231213\n",
      "iteration 438, current loss: 5344.080999\n",
      "iteration 439, current loss: 5343.930805\n",
      "iteration 440, current loss: 5343.780632\n",
      "iteration 441, current loss: 5343.630479\n",
      "iteration 442, current loss: 5343.480347\n",
      "iteration 443, current loss: 5343.330235\n",
      "iteration 444, current loss: 5343.180143\n",
      "iteration 445, current loss: 5343.030072\n",
      "iteration 446, current loss: 5342.880021\n",
      "iteration 447, current loss: 5342.729990\n",
      "iteration 448, current loss: 5342.579980\n",
      "iteration 449, current loss: 5342.429990\n",
      "iteration 450, current loss: 5342.280020\n",
      "iteration 451, current loss: 5342.130070\n",
      "iteration 452, current loss: 5341.980141\n",
      "iteration 453, current loss: 5341.830232\n",
      "iteration 454, current loss: 5341.680343\n",
      "iteration 455, current loss: 5341.530474\n",
      "iteration 456, current loss: 5341.380626\n",
      "iteration 457, current loss: 5341.230797\n",
      "iteration 458, current loss: 5341.080989\n",
      "iteration 459, current loss: 5340.931201\n",
      "iteration 460, current loss: 5340.781433\n",
      "iteration 461, current loss: 5340.631685\n",
      "iteration 462, current loss: 5340.481956\n",
      "iteration 463, current loss: 5340.332249\n",
      "iteration 464, current loss: 5340.182561\n",
      "iteration 465, current loss: 5340.032893\n",
      "iteration 466, current loss: 5339.883245\n",
      "iteration 467, current loss: 5339.733617\n",
      "iteration 468, current loss: 5339.584009\n",
      "iteration 469, current loss: 5339.434420\n",
      "iteration 470, current loss: 5339.284852\n",
      "iteration 471, current loss: 5339.135304\n",
      "iteration 472, current loss: 5338.985775\n",
      "iteration 473, current loss: 5338.836267\n",
      "iteration 474, current loss: 5338.686778\n",
      "iteration 475, current loss: 5338.537309\n",
      "iteration 476, current loss: 5338.387860\n",
      "iteration 477, current loss: 5338.238431\n",
      "iteration 478, current loss: 5338.089021\n",
      "iteration 479, current loss: 5337.939631\n",
      "iteration 480, current loss: 5337.790261\n",
      "iteration 481, current loss: 5337.640911\n",
      "iteration 482, current loss: 5337.491580\n",
      "iteration 483, current loss: 5337.342269\n",
      "iteration 484, current loss: 5337.192978\n",
      "iteration 485, current loss: 5337.043706\n",
      "iteration 486, current loss: 5336.894454\n",
      "iteration 487, current loss: 5336.745221\n",
      "iteration 488, current loss: 5336.596008\n",
      "iteration 489, current loss: 5336.446815\n",
      "iteration 490, current loss: 5336.297641\n",
      "iteration 491, current loss: 5336.148487\n",
      "iteration 492, current loss: 5335.999352\n",
      "iteration 493, current loss: 5335.850237\n",
      "iteration 494, current loss: 5335.701141\n",
      "iteration 495, current loss: 5335.552065\n",
      "iteration 496, current loss: 5335.403008\n",
      "iteration 497, current loss: 5335.253971\n",
      "iteration 498, current loss: 5335.104953\n",
      "iteration 499, current loss: 5334.955954\n",
      "iteration 500, current loss: 5334.806975\n",
      "iteration 501, current loss: 5334.658015\n",
      "iteration 502, current loss: 5334.509074\n",
      "iteration 503, current loss: 5334.360153\n",
      "iteration 504, current loss: 5334.211251\n",
      "iteration 505, current loss: 5334.062368\n",
      "iteration 506, current loss: 5333.913505\n",
      "iteration 507, current loss: 5333.764661\n",
      "iteration 508, current loss: 5333.615836\n",
      "iteration 509, current loss: 5333.467030\n",
      "iteration 510, current loss: 5333.318243\n",
      "iteration 511, current loss: 5333.169476\n",
      "iteration 512, current loss: 5333.020728\n",
      "iteration 513, current loss: 5332.871998\n",
      "iteration 514, current loss: 5332.723288\n",
      "iteration 515, current loss: 5332.574597\n",
      "iteration 516, current loss: 5332.425925\n",
      "iteration 517, current loss: 5332.277273\n",
      "iteration 518, current loss: 5332.128639\n",
      "iteration 519, current loss: 5331.980024\n",
      "iteration 520, current loss: 5331.831428\n",
      "iteration 521, current loss: 5331.682852\n",
      "iteration 522, current loss: 5331.534294\n",
      "iteration 523, current loss: 5331.385755\n",
      "iteration 524, current loss: 5331.237235\n",
      "iteration 525, current loss: 5331.088734\n",
      "iteration 526, current loss: 5330.940252\n",
      "iteration 527, current loss: 5330.791788\n",
      "iteration 528, current loss: 5330.643344\n",
      "iteration 529, current loss: 5330.494918\n",
      "iteration 530, current loss: 5330.346512\n",
      "iteration 531, current loss: 5330.198124\n",
      "iteration 532, current loss: 5330.049754\n",
      "iteration 533, current loss: 5329.901404\n",
      "iteration 534, current loss: 5329.753072\n",
      "iteration 535, current loss: 5329.604759\n",
      "iteration 536, current loss: 5329.456465\n",
      "iteration 537, current loss: 5329.308189\n",
      "iteration 538, current loss: 5329.159932\n",
      "iteration 539, current loss: 5329.011694\n",
      "iteration 540, current loss: 5328.863474\n",
      "iteration 541, current loss: 5328.715273\n",
      "iteration 542, current loss: 5328.567091\n",
      "iteration 543, current loss: 5328.418927\n",
      "iteration 544, current loss: 5328.270781\n",
      "iteration 545, current loss: 5328.122654\n",
      "iteration 546, current loss: 5327.974546\n",
      "iteration 547, current loss: 5327.826456\n",
      "iteration 548, current loss: 5327.678385\n",
      "iteration 549, current loss: 5327.530332\n",
      "iteration 550, current loss: 5327.382298\n",
      "iteration 551, current loss: 5327.234282\n",
      "iteration 552, current loss: 5327.086284\n",
      "iteration 553, current loss: 5326.938305\n",
      "iteration 554, current loss: 5326.790344\n",
      "iteration 555, current loss: 5326.642401\n",
      "iteration 556, current loss: 5326.494477\n",
      "iteration 557, current loss: 5326.346571\n",
      "iteration 558, current loss: 5326.198683\n",
      "iteration 559, current loss: 5326.050814\n",
      "iteration 560, current loss: 5325.902963\n",
      "iteration 561, current loss: 5325.755130\n",
      "iteration 562, current loss: 5325.607315\n",
      "iteration 563, current loss: 5325.459519\n",
      "iteration 564, current loss: 5325.311740\n",
      "iteration 565, current loss: 5325.163980\n",
      "iteration 566, current loss: 5325.016238\n",
      "iteration 567, current loss: 5324.868514\n",
      "iteration 568, current loss: 5324.720808\n",
      "iteration 569, current loss: 5324.573120\n",
      "iteration 570, current loss: 5324.425451\n",
      "iteration 571, current loss: 5324.277799\n",
      "iteration 572, current loss: 5324.130165\n",
      "iteration 573, current loss: 5323.982550\n",
      "iteration 574, current loss: 5323.834952\n",
      "iteration 575, current loss: 5323.687372\n",
      "iteration 576, current loss: 5323.539810\n",
      "iteration 577, current loss: 5323.392267\n",
      "iteration 578, current loss: 5323.244741\n",
      "iteration 579, current loss: 5323.097232\n",
      "iteration 580, current loss: 5322.949742\n",
      "iteration 581, current loss: 5322.802270\n",
      "iteration 582, current loss: 5322.654815\n",
      "iteration 583, current loss: 5322.507378\n",
      "iteration 584, current loss: 5322.359959\n",
      "iteration 585, current loss: 5322.212558\n",
      "iteration 586, current loss: 5322.065175\n",
      "iteration 587, current loss: 5321.917809\n",
      "iteration 588, current loss: 5321.770461\n",
      "iteration 589, current loss: 5321.623130\n",
      "iteration 590, current loss: 5321.475818\n",
      "iteration 591, current loss: 5321.328523\n",
      "iteration 592, current loss: 5321.181245\n",
      "iteration 593, current loss: 5321.033985\n",
      "iteration 594, current loss: 5320.886743\n",
      "iteration 595, current loss: 5320.739518\n",
      "iteration 596, current loss: 5320.592311\n",
      "iteration 597, current loss: 5320.445121\n",
      "iteration 598, current loss: 5320.297949\n",
      "iteration 599, current loss: 5320.150795\n",
      "iteration 600, current loss: 5320.003657\n",
      "iteration 601, current loss: 5319.856538\n",
      "iteration 602, current loss: 5319.709435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 603, current loss: 5319.562350\n",
      "iteration 604, current loss: 5319.415283\n",
      "iteration 605, current loss: 5319.268233\n",
      "iteration 606, current loss: 5319.121200\n",
      "iteration 607, current loss: 5318.974184\n",
      "iteration 608, current loss: 5318.827186\n",
      "iteration 609, current loss: 5318.680205\n",
      "iteration 610, current loss: 5318.533242\n",
      "iteration 611, current loss: 5318.386295\n",
      "iteration 612, current loss: 5318.239366\n",
      "iteration 613, current loss: 5318.092454\n",
      "iteration 614, current loss: 5317.945559\n",
      "iteration 615, current loss: 5317.798681\n",
      "iteration 616, current loss: 5317.651821\n",
      "iteration 617, current loss: 5317.504977\n",
      "iteration 618, current loss: 5317.358151\n",
      "iteration 619, current loss: 5317.211342\n",
      "iteration 620, current loss: 5317.064549\n",
      "iteration 621, current loss: 5316.917774\n",
      "iteration 622, current loss: 5316.771016\n",
      "iteration 623, current loss: 5316.624275\n",
      "iteration 624, current loss: 5316.477551\n",
      "iteration 625, current loss: 5316.330843\n",
      "iteration 626, current loss: 5316.184153\n",
      "iteration 627, current loss: 5316.037479\n",
      "iteration 628, current loss: 5315.890823\n",
      "iteration 629, current loss: 5315.744183\n",
      "iteration 630, current loss: 5315.597560\n",
      "iteration 631, current loss: 5315.450954\n",
      "iteration 632, current loss: 5315.304365\n",
      "iteration 633, current loss: 5315.157792\n",
      "iteration 634, current loss: 5315.011237\n",
      "iteration 635, current loss: 5314.864698\n",
      "iteration 636, current loss: 5314.718175\n",
      "iteration 637, current loss: 5314.571670\n",
      "iteration 638, current loss: 5314.425181\n",
      "iteration 639, current loss: 5314.278709\n",
      "iteration 640, current loss: 5314.132253\n",
      "iteration 641, current loss: 5313.985814\n",
      "iteration 642, current loss: 5313.839391\n",
      "iteration 643, current loss: 5313.692985\n",
      "iteration 644, current loss: 5313.546596\n",
      "iteration 645, current loss: 5313.400223\n",
      "iteration 646, current loss: 5313.253867\n",
      "iteration 647, current loss: 5313.107527\n",
      "iteration 648, current loss: 5312.961203\n",
      "iteration 649, current loss: 5312.814896\n",
      "iteration 650, current loss: 5312.668606\n",
      "iteration 651, current loss: 5312.522331\n",
      "iteration 652, current loss: 5312.376073\n",
      "iteration 653, current loss: 5312.229832\n",
      "iteration 654, current loss: 5312.083606\n",
      "iteration 655, current loss: 5311.937397\n",
      "iteration 656, current loss: 5311.791205\n",
      "iteration 657, current loss: 5311.645028\n",
      "iteration 658, current loss: 5311.498868\n",
      "iteration 659, current loss: 5311.352724\n",
      "iteration 660, current loss: 5311.206596\n",
      "iteration 661, current loss: 5311.060484\n",
      "iteration 662, current loss: 5310.914388\n",
      "iteration 663, current loss: 5310.768309\n",
      "iteration 664, current loss: 5310.622245\n",
      "iteration 665, current loss: 5310.476198\n",
      "iteration 666, current loss: 5310.330167\n",
      "iteration 667, current loss: 5310.184151\n",
      "iteration 668, current loss: 5310.038152\n",
      "iteration 669, current loss: 5309.892168\n",
      "iteration 670, current loss: 5309.746201\n",
      "iteration 671, current loss: 5309.600249\n",
      "iteration 672, current loss: 5309.454314\n",
      "iteration 673, current loss: 5309.308394\n",
      "iteration 674, current loss: 5309.162490\n",
      "iteration 675, current loss: 5309.016602\n",
      "iteration 676, current loss: 5308.870729\n",
      "iteration 677, current loss: 5308.724873\n",
      "iteration 678, current loss: 5308.579032\n",
      "iteration 679, current loss: 5308.433206\n",
      "iteration 680, current loss: 5308.287397\n",
      "iteration 681, current loss: 5308.141603\n",
      "iteration 682, current loss: 5307.995825\n",
      "iteration 683, current loss: 5307.850062\n",
      "iteration 684, current loss: 5307.704316\n",
      "iteration 685, current loss: 5307.558584\n",
      "iteration 686, current loss: 5307.412868\n",
      "iteration 687, current loss: 5307.267168\n",
      "iteration 688, current loss: 5307.121483\n",
      "iteration 689, current loss: 5306.975814\n",
      "iteration 690, current loss: 5306.830160\n",
      "iteration 691, current loss: 5306.684521\n",
      "iteration 692, current loss: 5306.538898\n",
      "iteration 693, current loss: 5306.393291\n",
      "iteration 694, current loss: 5306.247698\n",
      "iteration 695, current loss: 5306.102121\n",
      "iteration 696, current loss: 5305.956559\n",
      "iteration 697, current loss: 5305.811013\n",
      "iteration 698, current loss: 5305.665482\n",
      "iteration 699, current loss: 5305.519966\n",
      "iteration 700, current loss: 5305.374465\n",
      "iteration 701, current loss: 5305.228979\n",
      "iteration 702, current loss: 5305.083508\n",
      "iteration 703, current loss: 5304.938053\n",
      "iteration 704, current loss: 5304.792612\n",
      "iteration 705, current loss: 5304.647187\n",
      "iteration 706, current loss: 5304.501777\n",
      "iteration 707, current loss: 5304.356381\n",
      "iteration 708, current loss: 5304.211001\n",
      "iteration 709, current loss: 5304.065636\n",
      "iteration 710, current loss: 5303.920285\n",
      "iteration 711, current loss: 5303.774950\n",
      "iteration 712, current loss: 5303.629629\n",
      "iteration 713, current loss: 5303.484323\n",
      "iteration 714, current loss: 5303.339032\n",
      "iteration 715, current loss: 5303.193756\n",
      "iteration 716, current loss: 5303.048494\n",
      "iteration 717, current loss: 5302.903247\n",
      "iteration 718, current loss: 5302.758015\n",
      "iteration 719, current loss: 5302.612798\n",
      "iteration 720, current loss: 5302.467595\n",
      "iteration 721, current loss: 5302.322407\n",
      "iteration 722, current loss: 5302.177233\n",
      "iteration 723, current loss: 5302.032074\n",
      "iteration 724, current loss: 5301.886930\n",
      "iteration 725, current loss: 5301.741800\n",
      "iteration 726, current loss: 5301.596685\n",
      "iteration 727, current loss: 5301.451584\n",
      "iteration 728, current loss: 5301.306497\n",
      "iteration 729, current loss: 5301.161425\n",
      "iteration 730, current loss: 5301.016367\n",
      "iteration 731, current loss: 5300.871323\n",
      "iteration 732, current loss: 5300.726294\n",
      "iteration 733, current loss: 5300.581279\n",
      "iteration 734, current loss: 5300.436278\n",
      "iteration 735, current loss: 5300.291292\n",
      "iteration 736, current loss: 5300.146320\n",
      "iteration 737, current loss: 5300.001362\n",
      "iteration 738, current loss: 5299.856418\n",
      "iteration 739, current loss: 5299.711488\n",
      "iteration 740, current loss: 5299.566572\n",
      "iteration 741, current loss: 5299.421670\n",
      "iteration 742, current loss: 5299.276782\n",
      "iteration 743, current loss: 5299.131909\n",
      "iteration 744, current loss: 5298.987049\n",
      "iteration 745, current loss: 5298.842203\n",
      "iteration 746, current loss: 5298.697371\n",
      "iteration 747, current loss: 5298.552553\n",
      "iteration 748, current loss: 5298.407748\n",
      "iteration 749, current loss: 5298.262958\n",
      "iteration 750, current loss: 5298.118181\n",
      "iteration 751, current loss: 5297.973418\n",
      "iteration 752, current loss: 5297.828669\n",
      "iteration 753, current loss: 5297.683933\n",
      "iteration 754, current loss: 5297.539211\n",
      "iteration 755, current loss: 5297.394503\n",
      "iteration 756, current loss: 5297.249808\n",
      "iteration 757, current loss: 5297.105127\n",
      "iteration 758, current loss: 5296.960459\n",
      "iteration 759, current loss: 5296.815805\n",
      "iteration 760, current loss: 5296.671164\n",
      "iteration 761, current loss: 5296.526537\n",
      "iteration 762, current loss: 5296.381923\n",
      "iteration 763, current loss: 5296.237323\n",
      "iteration 764, current loss: 5296.092736\n",
      "iteration 765, current loss: 5295.948162\n",
      "iteration 766, current loss: 5295.803601\n",
      "iteration 767, current loss: 5295.659054\n",
      "iteration 768, current loss: 5295.514520\n",
      "iteration 769, current loss: 5295.369999\n",
      "iteration 770, current loss: 5295.225491\n",
      "iteration 771, current loss: 5295.080996\n",
      "iteration 772, current loss: 5294.936514\n",
      "iteration 773, current loss: 5294.792046\n",
      "iteration 774, current loss: 5294.647590\n",
      "iteration 775, current loss: 5294.503148\n",
      "iteration 776, current loss: 5294.358718\n",
      "iteration 777, current loss: 5294.214301\n",
      "iteration 778, current loss: 5294.069897\n",
      "iteration 779, current loss: 5293.925506\n",
      "iteration 780, current loss: 5293.781128\n",
      "iteration 781, current loss: 5293.636763\n",
      "iteration 782, current loss: 5293.492410\n",
      "iteration 783, current loss: 5293.348070\n",
      "iteration 784, current loss: 5293.203743\n",
      "iteration 785, current loss: 5293.059428\n",
      "iteration 786, current loss: 5292.915126\n",
      "iteration 787, current loss: 5292.770837\n",
      "iteration 788, current loss: 5292.626560\n",
      "iteration 789, current loss: 5292.482296\n",
      "iteration 790, current loss: 5292.338044\n",
      "iteration 791, current loss: 5292.193805\n",
      "iteration 792, current loss: 5292.049578\n",
      "iteration 793, current loss: 5291.905363\n",
      "iteration 794, current loss: 5291.761161\n",
      "iteration 795, current loss: 5291.616971\n",
      "iteration 796, current loss: 5291.472793\n",
      "iteration 797, current loss: 5291.328628\n",
      "iteration 798, current loss: 5291.184475\n",
      "iteration 799, current loss: 5291.040334\n",
      "iteration 800, current loss: 5290.896205\n",
      "iteration 801, current loss: 5290.752088\n",
      "iteration 802, current loss: 5290.607983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 803, current loss: 5290.463891\n",
      "iteration 804, current loss: 5290.319810\n",
      "iteration 805, current loss: 5290.175741\n",
      "iteration 806, current loss: 5290.031685\n",
      "iteration 807, current loss: 5289.887640\n",
      "iteration 808, current loss: 5289.743607\n",
      "iteration 809, current loss: 5289.599585\n",
      "iteration 810, current loss: 5289.455576\n",
      "iteration 811, current loss: 5289.311578\n",
      "iteration 812, current loss: 5289.167592\n",
      "iteration 813, current loss: 5289.023618\n",
      "iteration 814, current loss: 5288.879656\n",
      "iteration 815, current loss: 5288.735705\n",
      "iteration 816, current loss: 5288.591765\n",
      "iteration 817, current loss: 5288.447837\n",
      "iteration 818, current loss: 5288.303921\n",
      "iteration 819, current loss: 5288.160016\n",
      "iteration 820, current loss: 5288.016123\n",
      "iteration 821, current loss: 5287.872240\n",
      "iteration 822, current loss: 5287.728370\n",
      "iteration 823, current loss: 5287.584510\n",
      "iteration 824, current loss: 5287.440662\n",
      "iteration 825, current loss: 5287.296825\n",
      "iteration 826, current loss: 5287.153000\n",
      "iteration 827, current loss: 5287.009185\n",
      "iteration 828, current loss: 5286.865382\n",
      "iteration 829, current loss: 5286.721590\n",
      "iteration 830, current loss: 5286.577808\n",
      "iteration 831, current loss: 5286.434038\n",
      "iteration 832, current loss: 5286.290279\n",
      "iteration 833, current loss: 5286.146531\n",
      "iteration 834, current loss: 5286.002794\n",
      "iteration 835, current loss: 5285.859067\n",
      "iteration 836, current loss: 5285.715352\n",
      "iteration 837, current loss: 5285.571647\n",
      "iteration 838, current loss: 5285.427953\n",
      "iteration 839, current loss: 5285.284270\n",
      "iteration 840, current loss: 5285.140597\n",
      "iteration 841, current loss: 5284.996935\n",
      "iteration 842, current loss: 5284.853284\n",
      "iteration 843, current loss: 5284.709643\n",
      "iteration 844, current loss: 5284.566013\n",
      "iteration 845, current loss: 5284.422394\n",
      "iteration 846, current loss: 5284.278784\n",
      "iteration 847, current loss: 5284.135186\n",
      "iteration 848, current loss: 5283.991597\n",
      "iteration 849, current loss: 5283.848019\n",
      "iteration 850, current loss: 5283.704452\n",
      "iteration 851, current loss: 5283.560894\n",
      "iteration 852, current loss: 5283.417347\n",
      "iteration 853, current loss: 5283.273810\n",
      "iteration 854, current loss: 5283.130283\n",
      "iteration 855, current loss: 5282.986767\n",
      "iteration 856, current loss: 5282.843260\n",
      "iteration 857, current loss: 5282.699764\n",
      "iteration 858, current loss: 5282.556277\n",
      "iteration 859, current loss: 5282.412801\n",
      "iteration 860, current loss: 5282.269334\n",
      "iteration 861, current loss: 5282.125877\n",
      "iteration 862, current loss: 5281.982430\n",
      "iteration 863, current loss: 5281.838993\n",
      "iteration 864, current loss: 5281.695566\n",
      "iteration 865, current loss: 5281.552149\n",
      "iteration 866, current loss: 5281.408741\n",
      "iteration 867, current loss: 5281.265343\n",
      "iteration 868, current loss: 5281.121954\n",
      "iteration 869, current loss: 5280.978575\n",
      "iteration 870, current loss: 5280.835206\n",
      "iteration 871, current loss: 5280.691846\n",
      "iteration 872, current loss: 5280.548496\n",
      "iteration 873, current loss: 5280.405155\n",
      "iteration 874, current loss: 5280.261823\n",
      "iteration 875, current loss: 5280.118501\n",
      "iteration 876, current loss: 5279.975188\n",
      "iteration 877, current loss: 5279.831884\n",
      "iteration 878, current loss: 5279.688590\n",
      "iteration 879, current loss: 5279.545305\n",
      "iteration 880, current loss: 5279.402029\n",
      "iteration 881, current loss: 5279.258762\n",
      "iteration 882, current loss: 5279.115504\n",
      "iteration 883, current loss: 5278.972255\n",
      "iteration 884, current loss: 5278.829016\n",
      "iteration 885, current loss: 5278.685785\n",
      "iteration 886, current loss: 5278.542563\n",
      "iteration 887, current loss: 5278.399350\n",
      "iteration 888, current loss: 5278.256146\n",
      "iteration 889, current loss: 5278.112951\n",
      "iteration 890, current loss: 5277.969764\n",
      "iteration 891, current loss: 5277.826586\n",
      "iteration 892, current loss: 5277.683417\n",
      "iteration 893, current loss: 5277.540257\n",
      "iteration 894, current loss: 5277.397105\n",
      "iteration 895, current loss: 5277.253961\n",
      "iteration 896, current loss: 5277.110827\n",
      "iteration 897, current loss: 5276.967700\n",
      "iteration 898, current loss: 5276.824582\n",
      "iteration 899, current loss: 5276.681473\n",
      "iteration 900, current loss: 5276.538372\n",
      "iteration 901, current loss: 5276.395279\n",
      "iteration 902, current loss: 5276.252195\n",
      "iteration 903, current loss: 5276.109119\n",
      "iteration 904, current loss: 5275.966051\n",
      "iteration 905, current loss: 5275.822991\n",
      "iteration 906, current loss: 5275.679939\n",
      "iteration 907, current loss: 5275.536896\n",
      "iteration 908, current loss: 5275.393860\n",
      "iteration 909, current loss: 5275.250833\n",
      "iteration 910, current loss: 5275.107814\n",
      "iteration 911, current loss: 5274.964802\n",
      "iteration 912, current loss: 5274.821798\n",
      "iteration 913, current loss: 5274.678803\n",
      "iteration 914, current loss: 5274.535815\n",
      "iteration 915, current loss: 5274.392835\n",
      "iteration 916, current loss: 5274.249862\n",
      "iteration 917, current loss: 5274.106898\n",
      "iteration 918, current loss: 5273.963941\n",
      "iteration 919, current loss: 5273.820991\n",
      "iteration 920, current loss: 5273.678049\n",
      "iteration 921, current loss: 5273.535115\n",
      "iteration 922, current loss: 5273.392188\n",
      "iteration 923, current loss: 5273.249269\n",
      "iteration 924, current loss: 5273.106357\n",
      "iteration 925, current loss: 5272.963453\n",
      "iteration 926, current loss: 5272.820556\n",
      "iteration 927, current loss: 5272.677666\n",
      "iteration 928, current loss: 5272.534784\n",
      "iteration 929, current loss: 5272.391908\n",
      "iteration 930, current loss: 5272.249040\n",
      "iteration 931, current loss: 5272.106179\n",
      "iteration 932, current loss: 5271.963326\n",
      "iteration 933, current loss: 5271.820479\n",
      "iteration 934, current loss: 5271.677639\n",
      "iteration 935, current loss: 5271.534807\n",
      "iteration 936, current loss: 5271.391981\n",
      "iteration 937, current loss: 5271.249162\n",
      "iteration 938, current loss: 5271.106350\n",
      "iteration 939, current loss: 5270.963545\n",
      "iteration 940, current loss: 5270.820747\n",
      "iteration 941, current loss: 5270.677956\n",
      "iteration 942, current loss: 5270.535171\n",
      "iteration 943, current loss: 5270.392393\n",
      "iteration 944, current loss: 5270.249622\n",
      "iteration 945, current loss: 5270.106857\n",
      "iteration 946, current loss: 5269.964099\n",
      "iteration 947, current loss: 5269.821347\n",
      "iteration 948, current loss: 5269.678602\n",
      "iteration 949, current loss: 5269.535863\n",
      "iteration 950, current loss: 5269.393131\n",
      "iteration 951, current loss: 5269.250405\n",
      "iteration 952, current loss: 5269.107686\n",
      "iteration 953, current loss: 5268.964973\n",
      "iteration 954, current loss: 5268.822266\n",
      "iteration 955, current loss: 5268.679565\n",
      "iteration 956, current loss: 5268.536871\n",
      "iteration 957, current loss: 5268.394182\n",
      "iteration 958, current loss: 5268.251500\n",
      "iteration 959, current loss: 5268.108824\n",
      "iteration 960, current loss: 5267.966154\n",
      "iteration 961, current loss: 5267.823490\n",
      "iteration 962, current loss: 5267.680832\n",
      "iteration 963, current loss: 5267.538180\n",
      "iteration 964, current loss: 5267.395534\n",
      "iteration 965, current loss: 5267.252894\n",
      "iteration 966, current loss: 5267.110259\n",
      "iteration 967, current loss: 5266.967630\n",
      "iteration 968, current loss: 5266.825007\n",
      "iteration 969, current loss: 5266.682390\n",
      "iteration 970, current loss: 5266.539779\n",
      "iteration 971, current loss: 5266.397173\n",
      "iteration 972, current loss: 5266.254573\n",
      "iteration 973, current loss: 5266.111978\n",
      "iteration 974, current loss: 5265.969389\n",
      "iteration 975, current loss: 5265.826805\n",
      "iteration 976, current loss: 5265.684227\n",
      "iteration 977, current loss: 5265.541654\n",
      "iteration 978, current loss: 5265.399087\n",
      "iteration 979, current loss: 5265.256525\n",
      "iteration 980, current loss: 5265.113968\n",
      "iteration 981, current loss: 5264.971416\n",
      "iteration 982, current loss: 5264.828870\n",
      "iteration 983, current loss: 5264.686329\n",
      "iteration 984, current loss: 5264.543794\n",
      "iteration 985, current loss: 5264.401263\n",
      "iteration 986, current loss: 5264.258737\n",
      "iteration 987, current loss: 5264.116217\n",
      "iteration 988, current loss: 5263.973702\n",
      "iteration 989, current loss: 5263.831191\n",
      "iteration 990, current loss: 5263.688686\n",
      "iteration 991, current loss: 5263.546185\n",
      "iteration 992, current loss: 5263.403690\n",
      "iteration 993, current loss: 5263.261199\n",
      "iteration 994, current loss: 5263.118713\n",
      "iteration 995, current loss: 5262.976232\n",
      "iteration 996, current loss: 5262.833756\n",
      "iteration 997, current loss: 5262.691284\n",
      "iteration 998, current loss: 5262.548817\n",
      "iteration 999, current loss: 5262.406355\n",
      "iteration 1000, current loss: 5262.263897\n",
      "iteration 1001, current loss: 5262.121444\n",
      "iteration 1002, current loss: 5261.978996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1003, current loss: 5261.836552\n",
      "iteration 1004, current loss: 5261.694113\n",
      "iteration 1005, current loss: 5261.551678\n",
      "iteration 1006, current loss: 5261.409248\n",
      "iteration 1007, current loss: 5261.266821\n",
      "iteration 1008, current loss: 5261.124400\n",
      "iteration 1009, current loss: 5260.981982\n",
      "iteration 1010, current loss: 5260.839569\n",
      "iteration 1011, current loss: 5260.697161\n",
      "iteration 1012, current loss: 5260.554756\n",
      "iteration 1013, current loss: 5260.412356\n",
      "iteration 1014, current loss: 5260.269960\n",
      "iteration 1015, current loss: 5260.127568\n",
      "iteration 1016, current loss: 5259.985180\n",
      "iteration 1017, current loss: 5259.842796\n",
      "iteration 1018, current loss: 5259.700417\n",
      "iteration 1019, current loss: 5259.558041\n",
      "iteration 1020, current loss: 5259.415670\n",
      "iteration 1021, current loss: 5259.273302\n",
      "iteration 1022, current loss: 5259.130938\n",
      "iteration 1023, current loss: 5258.988578\n",
      "iteration 1024, current loss: 5258.846223\n",
      "iteration 1025, current loss: 5258.703871\n",
      "iteration 1026, current loss: 5258.561522\n",
      "iteration 1027, current loss: 5258.419178\n",
      "iteration 1028, current loss: 5258.276837\n",
      "iteration 1029, current loss: 5258.134500\n",
      "iteration 1030, current loss: 5257.992167\n",
      "iteration 1031, current loss: 5257.849838\n",
      "iteration 1032, current loss: 5257.707512\n",
      "iteration 1033, current loss: 5257.565190\n",
      "iteration 1034, current loss: 5257.422871\n",
      "iteration 1035, current loss: 5257.280556\n",
      "iteration 1036, current loss: 5257.138245\n",
      "iteration 1037, current loss: 5256.995937\n",
      "iteration 1038, current loss: 5256.853632\n",
      "iteration 1039, current loss: 5256.711331\n",
      "iteration 1040, current loss: 5256.569034\n",
      "iteration 1041, current loss: 5256.426739\n",
      "iteration 1042, current loss: 5256.284449\n",
      "iteration 1043, current loss: 5256.142161\n",
      "iteration 1044, current loss: 5255.999877\n",
      "iteration 1045, current loss: 5255.857597\n",
      "iteration 1046, current loss: 5255.715319\n",
      "iteration 1047, current loss: 5255.573045\n",
      "iteration 1048, current loss: 5255.430774\n",
      "iteration 1049, current loss: 5255.288506\n",
      "iteration 1050, current loss: 5255.146242\n",
      "iteration 1051, current loss: 5255.003980\n",
      "iteration 1052, current loss: 5254.861722\n",
      "iteration 1053, current loss: 5254.719467\n",
      "iteration 1054, current loss: 5254.577215\n",
      "iteration 1055, current loss: 5254.434966\n",
      "iteration 1056, current loss: 5254.292720\n",
      "iteration 1057, current loss: 5254.150477\n",
      "iteration 1058, current loss: 5254.008238\n",
      "iteration 1059, current loss: 5253.866001\n",
      "iteration 1060, current loss: 5253.723767\n",
      "iteration 1061, current loss: 5253.581536\n",
      "iteration 1062, current loss: 5253.439308\n",
      "iteration 1063, current loss: 5253.297083\n",
      "iteration 1064, current loss: 5253.154860\n",
      "iteration 1065, current loss: 5253.012641\n",
      "iteration 1066, current loss: 5252.870424\n",
      "iteration 1067, current loss: 5252.728211\n",
      "iteration 1068, current loss: 5252.586000\n",
      "iteration 1069, current loss: 5252.443792\n",
      "iteration 1070, current loss: 5252.301586\n",
      "iteration 1071, current loss: 5252.159384\n",
      "iteration 1072, current loss: 5252.017184\n",
      "iteration 1073, current loss: 5251.874986\n",
      "iteration 1074, current loss: 5251.732792\n",
      "iteration 1075, current loss: 5251.590600\n",
      "iteration 1076, current loss: 5251.448411\n",
      "iteration 1077, current loss: 5251.306224\n",
      "iteration 1078, current loss: 5251.164040\n",
      "iteration 1079, current loss: 5251.021859\n",
      "iteration 1080, current loss: 5250.879680\n",
      "iteration 1081, current loss: 5250.737503\n",
      "iteration 1082, current loss: 5250.595330\n",
      "iteration 1083, current loss: 5250.453158\n",
      "iteration 1084, current loss: 5250.310990\n",
      "iteration 1085, current loss: 5250.168824\n",
      "iteration 1086, current loss: 5250.026660\n",
      "iteration 1087, current loss: 5249.884499\n",
      "iteration 1088, current loss: 5249.742340\n",
      "iteration 1089, current loss: 5249.600183\n",
      "iteration 1090, current loss: 5249.458029\n",
      "iteration 1091, current loss: 5249.315878\n",
      "iteration 1092, current loss: 5249.173729\n",
      "iteration 1093, current loss: 5249.031582\n",
      "iteration 1094, current loss: 5248.889438\n",
      "iteration 1095, current loss: 5248.747296\n",
      "iteration 1096, current loss: 5248.605156\n",
      "iteration 1097, current loss: 5248.463019\n",
      "iteration 1098, current loss: 5248.320884\n",
      "iteration 1099, current loss: 5248.178751\n",
      "iteration 1100, current loss: 5248.036620\n",
      "iteration 1101, current loss: 5247.894492\n",
      "iteration 1102, current loss: 5247.752367\n",
      "iteration 1103, current loss: 5247.610243\n",
      "iteration 1104, current loss: 5247.468122\n",
      "iteration 1105, current loss: 5247.326002\n",
      "iteration 1106, current loss: 5247.183886\n",
      "iteration 1107, current loss: 5247.041771\n",
      "iteration 1108, current loss: 5246.899659\n",
      "iteration 1109, current loss: 5246.757548\n",
      "iteration 1110, current loss: 5246.615440\n",
      "iteration 1111, current loss: 5246.473334\n",
      "iteration 1112, current loss: 5246.331231\n",
      "iteration 1113, current loss: 5246.189129\n",
      "iteration 1114, current loss: 5246.047030\n",
      "iteration 1115, current loss: 5245.904933\n",
      "iteration 1116, current loss: 5245.762838\n",
      "iteration 1117, current loss: 5245.620745\n",
      "iteration 1118, current loss: 5245.478654\n",
      "iteration 1119, current loss: 5245.336566\n",
      "iteration 1120, current loss: 5245.194479\n",
      "iteration 1121, current loss: 5245.052395\n",
      "iteration 1122, current loss: 5244.910313\n",
      "iteration 1123, current loss: 5244.768232\n",
      "iteration 1124, current loss: 5244.626154\n",
      "iteration 1125, current loss: 5244.484078\n",
      "iteration 1126, current loss: 5244.342005\n",
      "iteration 1127, current loss: 5244.199933\n",
      "iteration 1128, current loss: 5244.057863\n",
      "iteration 1129, current loss: 5243.915796\n",
      "iteration 1130, current loss: 5243.773730\n",
      "iteration 1131, current loss: 5243.631667\n",
      "iteration 1132, current loss: 5243.489605\n",
      "iteration 1133, current loss: 5243.347546\n",
      "iteration 1134, current loss: 5243.205489\n",
      "iteration 1135, current loss: 5243.063434\n",
      "iteration 1136, current loss: 5242.921381\n",
      "iteration 1137, current loss: 5242.779330\n",
      "iteration 1138, current loss: 5242.637281\n",
      "iteration 1139, current loss: 5242.495234\n",
      "iteration 1140, current loss: 5242.353189\n",
      "iteration 1141, current loss: 5242.211147\n",
      "iteration 1142, current loss: 5242.069106\n",
      "iteration 1143, current loss: 5241.927067\n",
      "iteration 1144, current loss: 5241.785031\n",
      "iteration 1145, current loss: 5241.642996\n",
      "iteration 1146, current loss: 5241.500964\n",
      "iteration 1147, current loss: 5241.358934\n",
      "iteration 1148, current loss: 5241.216905\n",
      "iteration 1149, current loss: 5241.074879\n",
      "iteration 1150, current loss: 5240.932855\n",
      "iteration 1151, current loss: 5240.790833\n",
      "iteration 1152, current loss: 5240.648813\n",
      "iteration 1153, current loss: 5240.506795\n",
      "iteration 1154, current loss: 5240.364780\n",
      "iteration 1155, current loss: 5240.222766\n",
      "iteration 1156, current loss: 5240.080754\n",
      "iteration 1157, current loss: 5239.938745\n",
      "iteration 1158, current loss: 5239.796738\n",
      "iteration 1159, current loss: 5239.654732\n",
      "iteration 1160, current loss: 5239.512729\n",
      "iteration 1161, current loss: 5239.370728\n",
      "iteration 1162, current loss: 5239.228729\n",
      "iteration 1163, current loss: 5239.086733\n",
      "iteration 1164, current loss: 5238.944738\n",
      "iteration 1165, current loss: 5238.802745\n",
      "iteration 1166, current loss: 5238.660755\n",
      "iteration 1167, current loss: 5238.518767\n",
      "iteration 1168, current loss: 5238.376781\n",
      "iteration 1169, current loss: 5238.234797\n",
      "iteration 1170, current loss: 5238.092815\n",
      "iteration 1171, current loss: 5237.950836\n",
      "iteration 1172, current loss: 5237.808859\n",
      "iteration 1173, current loss: 5237.666884\n",
      "iteration 1174, current loss: 5237.524911\n",
      "iteration 1175, current loss: 5237.382940\n",
      "iteration 1176, current loss: 5237.240972\n",
      "iteration 1177, current loss: 5237.099005\n",
      "iteration 1178, current loss: 5236.957041\n",
      "iteration 1179, current loss: 5236.815080\n",
      "iteration 1180, current loss: 5236.673120\n",
      "iteration 1181, current loss: 5236.531163\n",
      "iteration 1182, current loss: 5236.389208\n",
      "iteration 1183, current loss: 5236.247256\n",
      "iteration 1184, current loss: 5236.105305\n",
      "iteration 1185, current loss: 5235.963357\n",
      "iteration 1186, current loss: 5235.821412\n",
      "iteration 1187, current loss: 5235.679468\n",
      "iteration 1188, current loss: 5235.537527\n",
      "iteration 1189, current loss: 5235.395589\n",
      "iteration 1190, current loss: 5235.253653\n",
      "iteration 1191, current loss: 5235.111719\n",
      "iteration 1192, current loss: 5234.969787\n",
      "iteration 1193, current loss: 5234.827858\n",
      "iteration 1194, current loss: 5234.685932\n",
      "iteration 1195, current loss: 5234.544007\n",
      "iteration 1196, current loss: 5234.402085\n",
      "iteration 1197, current loss: 5234.260166\n",
      "iteration 1198, current loss: 5234.118249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1199, current loss: 5233.976335\n",
      "iteration 1200, current loss: 5233.834423\n",
      "iteration 1201, current loss: 5233.692514\n",
      "iteration 1202, current loss: 5233.550607\n",
      "iteration 1203, current loss: 5233.408702\n",
      "iteration 1204, current loss: 5233.266801\n",
      "iteration 1205, current loss: 5233.124901\n",
      "iteration 1206, current loss: 5232.983005\n",
      "iteration 1207, current loss: 5232.841111\n",
      "iteration 1208, current loss: 5232.699219\n",
      "iteration 1209, current loss: 5232.557330\n",
      "iteration 1210, current loss: 5232.415444\n",
      "iteration 1211, current loss: 5232.273561\n",
      "iteration 1212, current loss: 5232.131680\n",
      "iteration 1213, current loss: 5231.989802\n",
      "iteration 1214, current loss: 5231.847926\n",
      "iteration 1215, current loss: 5231.706053\n",
      "iteration 1216, current loss: 5231.564183\n",
      "iteration 1217, current loss: 5231.422316\n",
      "iteration 1218, current loss: 5231.280451\n",
      "iteration 1219, current loss: 5231.138590\n",
      "iteration 1220, current loss: 5230.996731\n",
      "iteration 1221, current loss: 5230.854874\n",
      "iteration 1222, current loss: 5230.713021\n",
      "iteration 1223, current loss: 5230.571170\n",
      "iteration 1224, current loss: 5230.429323\n",
      "iteration 1225, current loss: 5230.287478\n",
      "iteration 1226, current loss: 5230.145636\n",
      "iteration 1227, current loss: 5230.003797\n",
      "iteration 1228, current loss: 5229.861961\n",
      "iteration 1229, current loss: 5229.720127\n",
      "iteration 1230, current loss: 5229.578297\n",
      "iteration 1231, current loss: 5229.436470\n",
      "iteration 1232, current loss: 5229.294646\n",
      "iteration 1233, current loss: 5229.152824\n",
      "iteration 1234, current loss: 5229.011006\n",
      "iteration 1235, current loss: 5228.869191\n",
      "iteration 1236, current loss: 5228.727379\n",
      "iteration 1237, current loss: 5228.585570\n",
      "iteration 1238, current loss: 5228.443764\n",
      "iteration 1239, current loss: 5228.301961\n",
      "iteration 1240, current loss: 5228.160161\n",
      "iteration 1241, current loss: 5228.018364\n",
      "iteration 1242, current loss: 5227.876571\n",
      "iteration 1243, current loss: 5227.734780\n",
      "iteration 1244, current loss: 5227.592993\n",
      "iteration 1245, current loss: 5227.451209\n",
      "iteration 1246, current loss: 5227.309428\n",
      "iteration 1247, current loss: 5227.167651\n",
      "iteration 1248, current loss: 5227.025877\n",
      "iteration 1249, current loss: 5226.884106\n",
      "iteration 1250, current loss: 5226.742338\n",
      "iteration 1251, current loss: 5226.600574\n",
      "iteration 1252, current loss: 5226.458813\n",
      "iteration 1253, current loss: 5226.317055\n",
      "iteration 1254, current loss: 5226.175301\n",
      "iteration 1255, current loss: 5226.033550\n",
      "iteration 1256, current loss: 5225.891802\n",
      "iteration 1257, current loss: 5225.750058\n",
      "iteration 1258, current loss: 5225.608318\n",
      "iteration 1259, current loss: 5225.466581\n",
      "iteration 1260, current loss: 5225.324847\n",
      "iteration 1261, current loss: 5225.183116\n",
      "iteration 1262, current loss: 5225.041390\n",
      "iteration 1263, current loss: 5224.899666\n",
      "iteration 1264, current loss: 5224.757947\n",
      "iteration 1265, current loss: 5224.616231\n",
      "iteration 1266, current loss: 5224.474518\n",
      "iteration 1267, current loss: 5224.332809\n",
      "iteration 1268, current loss: 5224.191104\n",
      "iteration 1269, current loss: 5224.049402\n",
      "iteration 1270, current loss: 5223.907704\n",
      "iteration 1271, current loss: 5223.766009\n",
      "iteration 1272, current loss: 5223.624319\n",
      "iteration 1273, current loss: 5223.482631\n",
      "iteration 1274, current loss: 5223.340948\n",
      "iteration 1275, current loss: 5223.199268\n",
      "iteration 1276, current loss: 5223.057592\n",
      "iteration 1277, current loss: 5222.915920\n",
      "iteration 1278, current loss: 5222.774252\n",
      "iteration 1279, current loss: 5222.632587\n",
      "iteration 1280, current loss: 5222.490926\n",
      "iteration 1281, current loss: 5222.349269\n",
      "iteration 1282, current loss: 5222.207616\n",
      "iteration 1283, current loss: 5222.065967\n",
      "iteration 1284, current loss: 5221.924321\n",
      "iteration 1285, current loss: 5221.782680\n",
      "iteration 1286, current loss: 5221.641042\n",
      "iteration 1287, current loss: 5221.499408\n",
      "iteration 1288, current loss: 5221.357779\n",
      "iteration 1289, current loss: 5221.216153\n",
      "iteration 1290, current loss: 5221.074531\n",
      "iteration 1291, current loss: 5220.932913\n",
      "iteration 1292, current loss: 5220.791299\n",
      "iteration 1293, current loss: 5220.649689\n",
      "iteration 1294, current loss: 5220.508083\n",
      "iteration 1295, current loss: 5220.366481\n",
      "iteration 1296, current loss: 5220.224884\n",
      "iteration 1297, current loss: 5220.083290\n",
      "iteration 1298, current loss: 5219.941700\n",
      "iteration 1299, current loss: 5219.800115\n",
      "iteration 1300, current loss: 5219.658533\n",
      "iteration 1301, current loss: 5219.516956\n",
      "iteration 1302, current loss: 5219.375383\n",
      "iteration 1303, current loss: 5219.233814\n",
      "iteration 1304, current loss: 5219.092249\n",
      "iteration 1305, current loss: 5218.950688\n",
      "iteration 1306, current loss: 5218.809132\n",
      "iteration 1307, current loss: 5218.667580\n",
      "iteration 1308, current loss: 5218.526032\n",
      "iteration 1309, current loss: 5218.384488\n",
      "iteration 1310, current loss: 5218.242949\n",
      "iteration 1311, current loss: 5218.101413\n",
      "iteration 1312, current loss: 5217.959882\n",
      "iteration 1313, current loss: 5217.818356\n",
      "iteration 1314, current loss: 5217.676833\n",
      "iteration 1315, current loss: 5217.535315\n",
      "iteration 1316, current loss: 5217.393802\n",
      "iteration 1317, current loss: 5217.252292\n",
      "iteration 1318, current loss: 5217.110787\n",
      "iteration 1319, current loss: 5216.969287\n",
      "iteration 1320, current loss: 5216.827790\n",
      "iteration 1321, current loss: 5216.686299\n",
      "iteration 1322, current loss: 5216.544811\n",
      "iteration 1323, current loss: 5216.403328\n",
      "iteration 1324, current loss: 5216.261850\n",
      "iteration 1325, current loss: 5216.120376\n",
      "iteration 1326, current loss: 5215.978906\n",
      "iteration 1327, current loss: 5215.837441\n",
      "iteration 1328, current loss: 5215.695980\n",
      "iteration 1329, current loss: 5215.554524\n",
      "iteration 1330, current loss: 5215.413072\n",
      "iteration 1331, current loss: 5215.271625\n",
      "iteration 1332, current loss: 5215.130182\n",
      "iteration 1333, current loss: 5214.988744\n",
      "iteration 1334, current loss: 5214.847311\n",
      "iteration 1335, current loss: 5214.705882\n",
      "iteration 1336, current loss: 5214.564458\n",
      "iteration 1337, current loss: 5214.423038\n",
      "iteration 1338, current loss: 5214.281623\n",
      "iteration 1339, current loss: 5214.140212\n",
      "iteration 1340, current loss: 5213.998806\n",
      "iteration 1341, current loss: 5213.857405\n",
      "iteration 1342, current loss: 5213.716008\n",
      "iteration 1343, current loss: 5213.574616\n",
      "iteration 1344, current loss: 5213.433229\n",
      "iteration 1345, current loss: 5213.291846\n",
      "iteration 1346, current loss: 5213.150468\n",
      "iteration 1347, current loss: 5213.009095\n",
      "iteration 1348, current loss: 5212.867726\n",
      "iteration 1349, current loss: 5212.726363\n",
      "iteration 1350, current loss: 5212.585004\n",
      "iteration 1351, current loss: 5212.443649\n",
      "iteration 1352, current loss: 5212.302300\n",
      "iteration 1353, current loss: 5212.160955\n",
      "iteration 1354, current loss: 5212.019615\n",
      "iteration 1355, current loss: 5211.878280\n",
      "iteration 1356, current loss: 5211.736949\n",
      "iteration 1357, current loss: 5211.595623\n",
      "iteration 1358, current loss: 5211.454303\n",
      "iteration 1359, current loss: 5211.312987\n",
      "iteration 1360, current loss: 5211.171675\n",
      "iteration 1361, current loss: 5211.030369\n",
      "iteration 1362, current loss: 5210.889068\n",
      "iteration 1363, current loss: 5210.747771\n",
      "iteration 1364, current loss: 5210.606480\n",
      "iteration 1365, current loss: 5210.465193\n",
      "iteration 1366, current loss: 5210.323911\n",
      "iteration 1367, current loss: 5210.182634\n",
      "iteration 1368, current loss: 5210.041362\n",
      "iteration 1369, current loss: 5209.900095\n",
      "iteration 1370, current loss: 5209.758833\n",
      "iteration 1371, current loss: 5209.617576\n",
      "iteration 1372, current loss: 5209.476323\n",
      "iteration 1373, current loss: 5209.335076\n",
      "iteration 1374, current loss: 5209.193834\n",
      "iteration 1375, current loss: 5209.052597\n",
      "iteration 1376, current loss: 5208.911365\n",
      "iteration 1377, current loss: 5208.770138\n",
      "iteration 1378, current loss: 5208.628915\n",
      "iteration 1379, current loss: 5208.487698\n",
      "iteration 1380, current loss: 5208.346486\n",
      "iteration 1381, current loss: 5208.205280\n",
      "iteration 1382, current loss: 5208.064078\n",
      "iteration 1383, current loss: 5207.922881\n",
      "iteration 1384, current loss: 5207.781689\n",
      "iteration 1385, current loss: 5207.640503\n",
      "iteration 1386, current loss: 5207.499322\n",
      "iteration 1387, current loss: 5207.358146\n",
      "iteration 1388, current loss: 5207.216975\n",
      "iteration 1389, current loss: 5207.075809\n",
      "iteration 1390, current loss: 5206.934648\n",
      "iteration 1391, current loss: 5206.793493\n",
      "iteration 1392, current loss: 5206.652342\n",
      "iteration 1393, current loss: 5206.511198\n",
      "iteration 1394, current loss: 5206.370058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1395, current loss: 5206.228923\n",
      "iteration 1396, current loss: 5206.087794\n",
      "iteration 1397, current loss: 5205.946670\n",
      "iteration 1398, current loss: 5205.805552\n",
      "iteration 1399, current loss: 5205.664438\n",
      "iteration 1400, current loss: 5205.523330\n",
      "iteration 1401, current loss: 5205.382228\n",
      "iteration 1402, current loss: 5205.241131\n",
      "iteration 1403, current loss: 5205.100039\n",
      "iteration 1404, current loss: 5204.958952\n",
      "iteration 1405, current loss: 5204.817871\n",
      "iteration 1406, current loss: 5204.676796\n",
      "iteration 1407, current loss: 5204.535725\n",
      "iteration 1408, current loss: 5204.394661\n",
      "iteration 1409, current loss: 5204.253601\n",
      "iteration 1410, current loss: 5204.112548\n",
      "iteration 1411, current loss: 5203.971499\n",
      "iteration 1412, current loss: 5203.830457\n",
      "iteration 1413, current loss: 5203.689420\n",
      "iteration 1414, current loss: 5203.548388\n",
      "iteration 1415, current loss: 5203.407362\n",
      "iteration 1416, current loss: 5203.266342\n",
      "iteration 1417, current loss: 5203.125327\n",
      "iteration 1418, current loss: 5202.984318\n",
      "iteration 1419, current loss: 5202.843314\n",
      "iteration 1420, current loss: 5202.702316\n",
      "iteration 1421, current loss: 5202.561324\n",
      "iteration 1422, current loss: 5202.420338\n",
      "iteration 1423, current loss: 5202.279357\n",
      "iteration 1424, current loss: 5202.138382\n",
      "iteration 1425, current loss: 5201.997413\n",
      "iteration 1426, current loss: 5201.856450\n",
      "iteration 1427, current loss: 5201.715492\n",
      "iteration 1428, current loss: 5201.574541\n",
      "iteration 1429, current loss: 5201.433595\n",
      "iteration 1430, current loss: 5201.292655\n",
      "iteration 1431, current loss: 5201.151721\n",
      "iteration 1432, current loss: 5201.010793\n",
      "iteration 1433, current loss: 5200.869871\n",
      "iteration 1434, current loss: 5200.728955\n",
      "iteration 1435, current loss: 5200.588045\n",
      "iteration 1436, current loss: 5200.447141\n",
      "iteration 1437, current loss: 5200.306243\n",
      "iteration 1438, current loss: 5200.165351\n",
      "iteration 1439, current loss: 5200.024465\n",
      "iteration 1440, current loss: 5199.883586\n",
      "iteration 1441, current loss: 5199.742712\n",
      "iteration 1442, current loss: 5199.601845\n",
      "iteration 1443, current loss: 5199.460984\n",
      "iteration 1444, current loss: 5199.320129\n",
      "iteration 1445, current loss: 5199.179281\n",
      "iteration 1446, current loss: 5199.038439\n",
      "iteration 1447, current loss: 5198.897603\n",
      "iteration 1448, current loss: 5198.756773\n",
      "iteration 1449, current loss: 5198.615950\n",
      "iteration 1450, current loss: 5198.475134\n",
      "iteration 1451, current loss: 5198.334323\n",
      "iteration 1452, current loss: 5198.193520\n",
      "iteration 1453, current loss: 5198.052723\n",
      "iteration 1454, current loss: 5197.911932\n",
      "iteration 1455, current loss: 5197.771148\n",
      "iteration 1456, current loss: 5197.630370\n",
      "iteration 1457, current loss: 5197.489600\n",
      "iteration 1458, current loss: 5197.348836\n",
      "iteration 1459, current loss: 5197.208078\n",
      "iteration 1460, current loss: 5197.067328\n",
      "iteration 1461, current loss: 5196.926584\n",
      "iteration 1462, current loss: 5196.785847\n",
      "iteration 1463, current loss: 5196.645117\n",
      "iteration 1464, current loss: 5196.504393\n",
      "iteration 1465, current loss: 5196.363677\n",
      "iteration 1466, current loss: 5196.222968\n",
      "iteration 1467, current loss: 5196.082265\n",
      "iteration 1468, current loss: 5195.941570\n",
      "iteration 1469, current loss: 5195.800882\n",
      "iteration 1470, current loss: 5195.660200\n",
      "iteration 1471, current loss: 5195.519526\n",
      "iteration 1472, current loss: 5195.378860\n",
      "iteration 1473, current loss: 5195.238200\n",
      "iteration 1474, current loss: 5195.097548\n",
      "iteration 1475, current loss: 5194.956903\n",
      "iteration 1476, current loss: 5194.816265\n",
      "iteration 1477, current loss: 5194.675635\n",
      "iteration 1478, current loss: 5194.535012\n",
      "iteration 1479, current loss: 5194.394396\n",
      "iteration 1480, current loss: 5194.253788\n",
      "iteration 1481, current loss: 5194.113188\n",
      "iteration 1482, current loss: 5193.972595\n",
      "iteration 1483, current loss: 5193.832010\n",
      "iteration 1484, current loss: 5193.691433\n",
      "iteration 1485, current loss: 5193.550863\n",
      "iteration 1486, current loss: 5193.410301\n",
      "iteration 1487, current loss: 5193.269747\n",
      "iteration 1488, current loss: 5193.129201\n",
      "iteration 1489, current loss: 5192.988662\n",
      "iteration 1490, current loss: 5192.848132\n",
      "iteration 1491, current loss: 5192.707610\n",
      "iteration 1492, current loss: 5192.567095\n",
      "iteration 1493, current loss: 5192.426589\n",
      "iteration 1494, current loss: 5192.286091\n",
      "iteration 1495, current loss: 5192.145601\n",
      "iteration 1496, current loss: 5192.005119\n",
      "iteration 1497, current loss: 5191.864645\n",
      "iteration 1498, current loss: 5191.724180\n",
      "iteration 1499, current loss: 5191.583723\n",
      "iteration 1500, current loss: 5191.443275\n",
      "iteration 1501, current loss: 5191.302835\n",
      "iteration 1502, current loss: 5191.162403\n",
      "iteration 1503, current loss: 5191.021980\n",
      "iteration 1504, current loss: 5190.881566\n",
      "iteration 1505, current loss: 5190.741160\n",
      "iteration 1506, current loss: 5190.600763\n",
      "iteration 1507, current loss: 5190.460375\n",
      "iteration 1508, current loss: 5190.319996\n",
      "iteration 1509, current loss: 5190.179625\n",
      "iteration 1510, current loss: 5190.039264\n",
      "iteration 1511, current loss: 5189.898911\n",
      "iteration 1512, current loss: 5189.758567\n",
      "iteration 1513, current loss: 5189.618233\n",
      "iteration 1514, current loss: 5189.477907\n",
      "iteration 1515, current loss: 5189.337591\n",
      "iteration 1516, current loss: 5189.197283\n",
      "iteration 1517, current loss: 5189.056985\n",
      "iteration 1518, current loss: 5188.916697\n",
      "iteration 1519, current loss: 5188.776418\n",
      "iteration 1520, current loss: 5188.636148\n",
      "iteration 1521, current loss: 5188.495887\n",
      "iteration 1522, current loss: 5188.355636\n",
      "iteration 1523, current loss: 5188.215395\n",
      "iteration 1524, current loss: 5188.075163\n",
      "iteration 1525, current loss: 5187.934941\n",
      "iteration 1526, current loss: 5187.794729\n",
      "iteration 1527, current loss: 5187.654527\n",
      "iteration 1528, current loss: 5187.514334\n",
      "iteration 1529, current loss: 5187.374151\n",
      "iteration 1530, current loss: 5187.233978\n",
      "iteration 1531, current loss: 5187.093815\n",
      "iteration 1532, current loss: 5186.953662\n",
      "iteration 1533, current loss: 5186.813519\n",
      "iteration 1534, current loss: 5186.673387\n",
      "iteration 1535, current loss: 5186.533264\n",
      "iteration 1536, current loss: 5186.393152\n",
      "iteration 1537, current loss: 5186.253050\n",
      "iteration 1538, current loss: 5186.112959\n",
      "iteration 1539, current loss: 5185.972877\n",
      "iteration 1540, current loss: 5185.832807\n",
      "iteration 1541, current loss: 5185.692746\n",
      "iteration 1542, current loss: 5185.552697\n",
      "iteration 1543, current loss: 5185.412657\n",
      "iteration 1544, current loss: 5185.272629\n",
      "iteration 1545, current loss: 5185.132611\n",
      "iteration 1546, current loss: 5184.992604\n",
      "iteration 1547, current loss: 5184.852608\n",
      "iteration 1548, current loss: 5184.712623\n",
      "iteration 1549, current loss: 5184.572648\n",
      "iteration 1550, current loss: 5184.432685\n",
      "iteration 1551, current loss: 5184.292733\n",
      "iteration 1552, current loss: 5184.152791\n",
      "iteration 1553, current loss: 5184.012861\n",
      "iteration 1554, current loss: 5183.872942\n",
      "iteration 1555, current loss: 5183.733034\n",
      "iteration 1556, current loss: 5183.593137\n",
      "iteration 1557, current loss: 5183.453252\n",
      "iteration 1558, current loss: 5183.313378\n",
      "iteration 1559, current loss: 5183.173515\n",
      "iteration 1560, current loss: 5183.033664\n",
      "iteration 1561, current loss: 5182.893825\n",
      "iteration 1562, current loss: 5182.753997\n",
      "iteration 1563, current loss: 5182.614180\n",
      "iteration 1564, current loss: 5182.474375\n",
      "iteration 1565, current loss: 5182.334582\n",
      "iteration 1566, current loss: 5182.194801\n",
      "iteration 1567, current loss: 5182.055031\n",
      "iteration 1568, current loss: 5181.915273\n",
      "iteration 1569, current loss: 5181.775527\n",
      "iteration 1570, current loss: 5181.635793\n",
      "iteration 1571, current loss: 5181.496071\n",
      "iteration 1572, current loss: 5181.356361\n",
      "iteration 1573, current loss: 5181.216663\n",
      "iteration 1574, current loss: 5181.076977\n",
      "iteration 1575, current loss: 5180.937303\n",
      "iteration 1576, current loss: 5180.797642\n",
      "iteration 1577, current loss: 5180.657992\n",
      "iteration 1578, current loss: 5180.518355\n",
      "iteration 1579, current loss: 5180.378730\n",
      "iteration 1580, current loss: 5180.239118\n",
      "iteration 1581, current loss: 5180.099518\n",
      "iteration 1582, current loss: 5179.959930\n",
      "iteration 1583, current loss: 5179.820355\n",
      "iteration 1584, current loss: 5179.680792\n",
      "iteration 1585, current loss: 5179.541242\n",
      "iteration 1586, current loss: 5179.401704\n",
      "iteration 1587, current loss: 5179.262179\n",
      "iteration 1588, current loss: 5179.122667\n",
      "iteration 1589, current loss: 5178.983167\n",
      "iteration 1590, current loss: 5178.843680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1591, current loss: 5178.704206\n",
      "iteration 1592, current loss: 5178.564744\n",
      "iteration 1593, current loss: 5178.425296\n",
      "iteration 1594, current loss: 5178.285860\n",
      "iteration 1595, current loss: 5178.146437\n",
      "iteration 1596, current loss: 5178.007027\n",
      "iteration 1597, current loss: 5177.867630\n",
      "iteration 1598, current loss: 5177.728246\n",
      "iteration 1599, current loss: 5177.588875\n",
      "iteration 1600, current loss: 5177.449517\n",
      "iteration 1601, current loss: 5177.310172\n",
      "iteration 1602, current loss: 5177.170840\n",
      "iteration 1603, current loss: 5177.031521\n",
      "iteration 1604, current loss: 5176.892216\n",
      "iteration 1605, current loss: 5176.752924\n",
      "iteration 1606, current loss: 5176.613644\n",
      "iteration 1607, current loss: 5176.474379\n",
      "iteration 1608, current loss: 5176.335126\n",
      "iteration 1609, current loss: 5176.195886\n",
      "iteration 1610, current loss: 5176.056660\n",
      "iteration 1611, current loss: 5175.917448\n",
      "iteration 1612, current loss: 5175.778248\n",
      "iteration 1613, current loss: 5175.639062\n",
      "iteration 1614, current loss: 5175.499890\n",
      "iteration 1615, current loss: 5175.360731\n",
      "iteration 1616, current loss: 5175.221585\n",
      "iteration 1617, current loss: 5175.082453\n",
      "iteration 1618, current loss: 5174.943334\n",
      "iteration 1619, current loss: 5174.804228\n",
      "iteration 1620, current loss: 5174.665137\n",
      "iteration 1621, current loss: 5174.526058\n",
      "iteration 1622, current loss: 5174.386994\n",
      "iteration 1623, current loss: 5174.247943\n",
      "iteration 1624, current loss: 5174.108905\n",
      "iteration 1625, current loss: 5173.969881\n",
      "iteration 1626, current loss: 5173.830871\n",
      "iteration 1627, current loss: 5173.691874\n",
      "iteration 1628, current loss: 5173.552891\n",
      "iteration 1629, current loss: 5173.413921\n",
      "iteration 1630, current loss: 5173.274965\n",
      "iteration 1631, current loss: 5173.136023\n",
      "iteration 1632, current loss: 5172.997095\n",
      "iteration 1633, current loss: 5172.858180\n",
      "iteration 1634, current loss: 5172.719279\n",
      "iteration 1635, current loss: 5172.580391\n",
      "iteration 1636, current loss: 5172.441518\n",
      "iteration 1637, current loss: 5172.302658\n",
      "iteration 1638, current loss: 5172.163811\n",
      "iteration 1639, current loss: 5172.024979\n",
      "iteration 1640, current loss: 5171.886160\n",
      "iteration 1641, current loss: 5171.747355\n",
      "iteration 1642, current loss: 5171.608563\n",
      "iteration 1643, current loss: 5171.469786\n",
      "iteration 1644, current loss: 5171.331022\n",
      "iteration 1645, current loss: 5171.192272\n",
      "iteration 1646, current loss: 5171.053535\n",
      "iteration 1647, current loss: 5170.914813\n",
      "iteration 1648, current loss: 5170.776104\n",
      "iteration 1649, current loss: 5170.637409\n",
      "iteration 1650, current loss: 5170.498727\n",
      "iteration 1651, current loss: 5170.360059\n",
      "iteration 1652, current loss: 5170.221405\n",
      "iteration 1653, current loss: 5170.082765\n",
      "iteration 1654, current loss: 5169.944139\n",
      "iteration 1655, current loss: 5169.805526\n",
      "iteration 1656, current loss: 5169.666927\n",
      "iteration 1657, current loss: 5169.528342\n",
      "iteration 1658, current loss: 5169.389770\n",
      "iteration 1659, current loss: 5169.251212\n",
      "iteration 1660, current loss: 5169.112668\n",
      "iteration 1661, current loss: 5168.974137\n",
      "iteration 1662, current loss: 5168.835620\n",
      "iteration 1663, current loss: 5168.697117\n",
      "iteration 1664, current loss: 5168.558628\n",
      "iteration 1665, current loss: 5168.420152\n",
      "iteration 1666, current loss: 5168.281690\n",
      "iteration 1667, current loss: 5168.143241\n",
      "iteration 1668, current loss: 5168.004806\n",
      "iteration 1669, current loss: 5167.866385\n",
      "iteration 1670, current loss: 5167.727977\n",
      "iteration 1671, current loss: 5167.589583\n",
      "iteration 1672, current loss: 5167.451202\n",
      "iteration 1673, current loss: 5167.312835\n",
      "iteration 1674, current loss: 5167.174481\n",
      "iteration 1675, current loss: 5167.036141\n",
      "iteration 1676, current loss: 5166.897814\n",
      "iteration 1677, current loss: 5166.759501\n",
      "iteration 1678, current loss: 5166.621202\n",
      "iteration 1679, current loss: 5166.482916\n",
      "iteration 1680, current loss: 5166.344643\n",
      "iteration 1681, current loss: 5166.206384\n",
      "iteration 1682, current loss: 5166.068138\n",
      "iteration 1683, current loss: 5165.929905\n",
      "iteration 1684, current loss: 5165.791686\n",
      "iteration 1685, current loss: 5165.653480\n",
      "iteration 1686, current loss: 5165.515288\n",
      "iteration 1687, current loss: 5165.377108\n",
      "iteration 1688, current loss: 5165.238942\n",
      "iteration 1689, current loss: 5165.100790\n",
      "iteration 1690, current loss: 5164.962650\n",
      "iteration 1691, current loss: 5164.824524\n",
      "iteration 1692, current loss: 5164.686411\n",
      "iteration 1693, current loss: 5164.548311\n",
      "iteration 1694, current loss: 5164.410224\n",
      "iteration 1695, current loss: 5164.272150\n",
      "iteration 1696, current loss: 5164.134090\n",
      "iteration 1697, current loss: 5163.996042\n",
      "iteration 1698, current loss: 5163.858008\n",
      "iteration 1699, current loss: 5163.719986\n",
      "iteration 1700, current loss: 5163.581978\n",
      "iteration 1701, current loss: 5163.443982\n",
      "iteration 1702, current loss: 5163.306000\n",
      "iteration 1703, current loss: 5163.168030\n",
      "iteration 1704, current loss: 5163.030073\n",
      "iteration 1705, current loss: 5162.892129\n",
      "iteration 1706, current loss: 5162.754198\n",
      "iteration 1707, current loss: 5162.616280\n",
      "iteration 1708, current loss: 5162.478374\n",
      "iteration 1709, current loss: 5162.340482\n",
      "iteration 1710, current loss: 5162.202602\n",
      "iteration 1711, current loss: 5162.064734\n",
      "iteration 1712, current loss: 5161.926879\n",
      "iteration 1713, current loss: 5161.789037\n",
      "iteration 1714, current loss: 5161.651208\n",
      "iteration 1715, current loss: 5161.513391\n",
      "iteration 1716, current loss: 5161.375586\n",
      "iteration 1717, current loss: 5161.237794\n",
      "iteration 1718, current loss: 5161.100015\n",
      "iteration 1719, current loss: 5160.962248\n",
      "iteration 1720, current loss: 5160.824493\n",
      "iteration 1721, current loss: 5160.686751\n",
      "iteration 1722, current loss: 5160.549021\n",
      "iteration 1723, current loss: 5160.411303\n",
      "iteration 1724, current loss: 5160.273598\n",
      "iteration 1725, current loss: 5160.135904\n",
      "iteration 1726, current loss: 5159.998223\n",
      "iteration 1727, current loss: 5159.860555\n",
      "iteration 1728, current loss: 5159.722898\n",
      "iteration 1729, current loss: 5159.585253\n",
      "iteration 1730, current loss: 5159.447621\n",
      "iteration 1731, current loss: 5159.310000\n",
      "iteration 1732, current loss: 5159.172392\n",
      "iteration 1733, current loss: 5159.034795\n",
      "iteration 1734, current loss: 5158.897211\n",
      "iteration 1735, current loss: 5158.759638\n",
      "iteration 1736, current loss: 5158.622077\n",
      "iteration 1737, current loss: 5158.484528\n",
      "iteration 1738, current loss: 5158.346991\n",
      "iteration 1739, current loss: 5158.209466\n",
      "iteration 1740, current loss: 5158.071952\n",
      "iteration 1741, current loss: 5157.934450\n",
      "iteration 1742, current loss: 5157.796959\n",
      "iteration 1743, current loss: 5157.659481\n",
      "iteration 1744, current loss: 5157.522013\n",
      "iteration 1745, current loss: 5157.384558\n",
      "iteration 1746, current loss: 5157.247114\n",
      "iteration 1747, current loss: 5157.109681\n",
      "iteration 1748, current loss: 5156.972259\n",
      "iteration 1749, current loss: 5156.834850\n",
      "iteration 1750, current loss: 5156.697451\n",
      "iteration 1751, current loss: 5156.560064\n",
      "iteration 1752, current loss: 5156.422688\n",
      "iteration 1753, current loss: 5156.285323\n",
      "iteration 1754, current loss: 5156.147969\n",
      "iteration 1755, current loss: 5156.010627\n",
      "iteration 1756, current loss: 5155.873296\n",
      "iteration 1757, current loss: 5155.735975\n",
      "iteration 1758, current loss: 5155.598666\n",
      "iteration 1759, current loss: 5155.461368\n",
      "iteration 1760, current loss: 5155.324081\n",
      "iteration 1761, current loss: 5155.186805\n",
      "iteration 1762, current loss: 5155.049539\n",
      "iteration 1763, current loss: 5154.912285\n",
      "iteration 1764, current loss: 5154.775041\n",
      "iteration 1765, current loss: 5154.637808\n",
      "iteration 1766, current loss: 5154.500586\n",
      "iteration 1767, current loss: 5154.363374\n",
      "iteration 1768, current loss: 5154.226174\n",
      "iteration 1769, current loss: 5154.088983\n",
      "iteration 1770, current loss: 5153.951804\n",
      "iteration 1771, current loss: 5153.814635\n",
      "iteration 1772, current loss: 5153.677476\n",
      "iteration 1773, current loss: 5153.540328\n",
      "iteration 1774, current loss: 5153.403190\n",
      "iteration 1775, current loss: 5153.266063\n",
      "iteration 1776, current loss: 5153.128946\n",
      "iteration 1777, current loss: 5152.991840\n",
      "iteration 1778, current loss: 5152.854743\n",
      "iteration 1779, current loss: 5152.717657\n",
      "iteration 1780, current loss: 5152.580581\n",
      "iteration 1781, current loss: 5152.443515\n",
      "iteration 1782, current loss: 5152.306460\n",
      "iteration 1783, current loss: 5152.169414\n",
      "iteration 1784, current loss: 5152.032379\n",
      "iteration 1785, current loss: 5151.895353\n",
      "iteration 1786, current loss: 5151.758337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1787, current loss: 5151.621332\n",
      "iteration 1788, current loss: 5151.484336\n",
      "iteration 1789, current loss: 5151.347350\n",
      "iteration 1790, current loss: 5151.210374\n",
      "iteration 1791, current loss: 5151.073407\n",
      "iteration 1792, current loss: 5150.936451\n",
      "iteration 1793, current loss: 5150.799504\n",
      "iteration 1794, current loss: 5150.662566\n",
      "iteration 1795, current loss: 5150.525638\n",
      "iteration 1796, current loss: 5150.388720\n",
      "iteration 1797, current loss: 5150.251811\n",
      "iteration 1798, current loss: 5150.114912\n",
      "iteration 1799, current loss: 5149.978022\n",
      "iteration 1800, current loss: 5149.841142\n",
      "iteration 1801, current loss: 5149.704271\n",
      "iteration 1802, current loss: 5149.567409\n",
      "iteration 1803, current loss: 5149.430557\n",
      "iteration 1804, current loss: 5149.293713\n",
      "iteration 1805, current loss: 5149.156879\n",
      "iteration 1806, current loss: 5149.020054\n",
      "iteration 1807, current loss: 5148.883239\n",
      "iteration 1808, current loss: 5148.746432\n",
      "iteration 1809, current loss: 5148.609634\n",
      "iteration 1810, current loss: 5148.472846\n",
      "iteration 1811, current loss: 5148.336066\n",
      "iteration 1812, current loss: 5148.199295\n",
      "iteration 1813, current loss: 5148.062533\n",
      "iteration 1814, current loss: 5147.925780\n",
      "iteration 1815, current loss: 5147.789036\n",
      "iteration 1816, current loss: 5147.652300\n",
      "iteration 1817, current loss: 5147.515574\n",
      "iteration 1818, current loss: 5147.378855\n",
      "iteration 1819, current loss: 5147.242146\n",
      "iteration 1820, current loss: 5147.105445\n",
      "iteration 1821, current loss: 5146.968753\n",
      "iteration 1822, current loss: 5146.832069\n",
      "iteration 1823, current loss: 5146.695394\n",
      "iteration 1824, current loss: 5146.558727\n",
      "iteration 1825, current loss: 5146.422068\n",
      "iteration 1826, current loss: 5146.285418\n",
      "iteration 1827, current loss: 5146.148776\n",
      "iteration 1828, current loss: 5146.012143\n",
      "iteration 1829, current loss: 5145.875517\n",
      "iteration 1830, current loss: 5145.738900\n",
      "iteration 1831, current loss: 5145.602291\n",
      "iteration 1832, current loss: 5145.465690\n",
      "iteration 1833, current loss: 5145.329098\n",
      "iteration 1834, current loss: 5145.192513\n",
      "iteration 1835, current loss: 5145.055936\n",
      "iteration 1836, current loss: 5144.919367\n",
      "iteration 1837, current loss: 5144.782807\n",
      "iteration 1838, current loss: 5144.646254\n",
      "iteration 1839, current loss: 5144.509708\n",
      "iteration 1840, current loss: 5144.373171\n",
      "iteration 1841, current loss: 5144.236641\n",
      "iteration 1842, current loss: 5144.100119\n",
      "iteration 1843, current loss: 5143.963605\n",
      "iteration 1844, current loss: 5143.827099\n",
      "iteration 1845, current loss: 5143.690600\n",
      "iteration 1846, current loss: 5143.554108\n",
      "iteration 1847, current loss: 5143.417624\n",
      "iteration 1848, current loss: 5143.281148\n",
      "iteration 1849, current loss: 5143.144678\n",
      "iteration 1850, current loss: 5143.008217\n",
      "iteration 1851, current loss: 5142.871762\n",
      "iteration 1852, current loss: 5142.735315\n",
      "iteration 1853, current loss: 5142.598876\n",
      "iteration 1854, current loss: 5142.462443\n",
      "iteration 1855, current loss: 5142.326018\n",
      "iteration 1856, current loss: 5142.189599\n",
      "iteration 1857, current loss: 5142.053188\n",
      "iteration 1858, current loss: 5141.916784\n",
      "iteration 1859, current loss: 5141.780387\n",
      "iteration 1860, current loss: 5141.643997\n",
      "iteration 1861, current loss: 5141.507614\n",
      "iteration 1862, current loss: 5141.371238\n",
      "iteration 1863, current loss: 5141.234868\n",
      "iteration 1864, current loss: 5141.098506\n",
      "iteration 1865, current loss: 5140.962150\n",
      "iteration 1866, current loss: 5140.825801\n",
      "iteration 1867, current loss: 5140.689458\n",
      "iteration 1868, current loss: 5140.553123\n",
      "iteration 1869, current loss: 5140.416794\n",
      "iteration 1870, current loss: 5140.280471\n",
      "iteration 1871, current loss: 5140.144155\n",
      "iteration 1872, current loss: 5140.007845\n",
      "iteration 1873, current loss: 5139.871542\n",
      "iteration 1874, current loss: 5139.735246\n",
      "iteration 1875, current loss: 5139.598955\n",
      "iteration 1876, current loss: 5139.462671\n",
      "iteration 1877, current loss: 5139.326393\n",
      "iteration 1878, current loss: 5139.190122\n",
      "iteration 1879, current loss: 5139.053856\n",
      "iteration 1880, current loss: 5138.917597\n",
      "iteration 1881, current loss: 5138.781344\n",
      "iteration 1882, current loss: 5138.645097\n",
      "iteration 1883, current loss: 5138.508856\n",
      "iteration 1884, current loss: 5138.372621\n",
      "iteration 1885, current loss: 5138.236392\n",
      "iteration 1886, current loss: 5138.100169\n",
      "iteration 1887, current loss: 5137.963951\n",
      "iteration 1888, current loss: 5137.827740\n",
      "iteration 1889, current loss: 5137.691534\n",
      "iteration 1890, current loss: 5137.555334\n",
      "iteration 1891, current loss: 5137.419139\n",
      "iteration 1892, current loss: 5137.282951\n",
      "iteration 1893, current loss: 5137.146767\n",
      "iteration 1894, current loss: 5137.010590\n",
      "iteration 1895, current loss: 5136.874418\n",
      "iteration 1896, current loss: 5136.738251\n",
      "iteration 1897, current loss: 5136.602090\n",
      "iteration 1898, current loss: 5136.465934\n",
      "iteration 1899, current loss: 5136.329783\n",
      "iteration 1900, current loss: 5136.193638\n",
      "iteration 1901, current loss: 5136.057498\n",
      "iteration 1902, current loss: 5135.921363\n",
      "iteration 1903, current loss: 5135.785234\n",
      "iteration 1904, current loss: 5135.649109\n",
      "iteration 1905, current loss: 5135.512990\n",
      "iteration 1906, current loss: 5135.376875\n",
      "iteration 1907, current loss: 5135.240766\n",
      "iteration 1908, current loss: 5135.104661\n",
      "iteration 1909, current loss: 5134.968562\n",
      "iteration 1910, current loss: 5134.832467\n",
      "iteration 1911, current loss: 5134.696377\n",
      "iteration 1912, current loss: 5134.560292\n",
      "iteration 1913, current loss: 5134.424211\n",
      "iteration 1914, current loss: 5134.288135\n",
      "iteration 1915, current loss: 5134.152064\n",
      "iteration 1916, current loss: 5134.015998\n",
      "iteration 1917, current loss: 5133.879936\n",
      "iteration 1918, current loss: 5133.743878\n",
      "iteration 1919, current loss: 5133.607825\n",
      "iteration 1920, current loss: 5133.471776\n",
      "iteration 1921, current loss: 5133.335732\n",
      "iteration 1922, current loss: 5133.199691\n",
      "iteration 1923, current loss: 5133.063656\n",
      "iteration 1924, current loss: 5132.927624\n",
      "iteration 1925, current loss: 5132.791597\n",
      "iteration 1926, current loss: 5132.655573\n",
      "iteration 1927, current loss: 5132.519554\n",
      "iteration 1928, current loss: 5132.383539\n",
      "iteration 1929, current loss: 5132.247528\n",
      "iteration 1930, current loss: 5132.111520\n",
      "iteration 1931, current loss: 5131.975517\n",
      "iteration 1932, current loss: 5131.839517\n",
      "iteration 1933, current loss: 5131.703521\n",
      "iteration 1934, current loss: 5131.567529\n",
      "iteration 1935, current loss: 5131.431541\n",
      "iteration 1936, current loss: 5131.295557\n",
      "iteration 1937, current loss: 5131.159575\n",
      "iteration 1938, current loss: 5131.023598\n",
      "iteration 1939, current loss: 5130.887624\n",
      "iteration 1940, current loss: 5130.751654\n",
      "iteration 1941, current loss: 5130.615686\n",
      "iteration 1942, current loss: 5130.479723\n",
      "iteration 1943, current loss: 5130.343762\n",
      "iteration 1944, current loss: 5130.207805\n",
      "iteration 1945, current loss: 5130.071851\n",
      "iteration 1946, current loss: 5129.935901\n",
      "iteration 1947, current loss: 5129.799953\n",
      "iteration 1948, current loss: 5129.664009\n",
      "iteration 1949, current loss: 5129.528067\n",
      "iteration 1950, current loss: 5129.392129\n",
      "iteration 1951, current loss: 5129.256193\n",
      "iteration 1952, current loss: 5129.120261\n",
      "iteration 1953, current loss: 5128.984331\n",
      "iteration 1954, current loss: 5128.848404\n",
      "iteration 1955, current loss: 5128.712480\n",
      "iteration 1956, current loss: 5128.576558\n",
      "iteration 1957, current loss: 5128.440639\n",
      "iteration 1958, current loss: 5128.304723\n",
      "iteration 1959, current loss: 5128.168809\n",
      "iteration 1960, current loss: 5128.032898\n",
      "iteration 1961, current loss: 5127.896989\n",
      "iteration 1962, current loss: 5127.761082\n",
      "iteration 1963, current loss: 5127.625178\n",
      "iteration 1964, current loss: 5127.489276\n",
      "iteration 1965, current loss: 5127.353377\n",
      "iteration 1966, current loss: 5127.217480\n",
      "iteration 1967, current loss: 5127.081584\n",
      "iteration 1968, current loss: 5126.945691\n",
      "iteration 1969, current loss: 5126.809800\n",
      "iteration 1970, current loss: 5126.673911\n",
      "iteration 1971, current loss: 5126.538024\n",
      "iteration 1972, current loss: 5126.402139\n",
      "iteration 1973, current loss: 5126.266255\n",
      "iteration 1974, current loss: 5126.130374\n",
      "iteration 1975, current loss: 5125.994494\n",
      "iteration 1976, current loss: 5125.858616\n",
      "iteration 1977, current loss: 5125.722739\n",
      "iteration 1978, current loss: 5125.586864\n",
      "iteration 1979, current loss: 5125.450991\n",
      "iteration 1980, current loss: 5125.315119\n",
      "iteration 1981, current loss: 5125.179249\n",
      "iteration 1982, current loss: 5125.043380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1983, current loss: 5124.907512\n",
      "iteration 1984, current loss: 5124.771646\n",
      "iteration 1985, current loss: 5124.635780\n",
      "iteration 1986, current loss: 5124.499916\n",
      "iteration 1987, current loss: 5124.364053\n",
      "iteration 1988, current loss: 5124.228192\n",
      "iteration 1989, current loss: 5124.092331\n",
      "iteration 1990, current loss: 5123.956471\n",
      "iteration 1991, current loss: 5123.820612\n",
      "iteration 1992, current loss: 5123.684754\n",
      "iteration 1993, current loss: 5123.548897\n",
      "iteration 1994, current loss: 5123.413041\n",
      "iteration 1995, current loss: 5123.277185\n",
      "iteration 1996, current loss: 5123.141330\n",
      "iteration 1997, current loss: 5123.005475\n",
      "iteration 1998, current loss: 5122.869622\n",
      "iteration 1999, current loss: 5122.733768\n",
      "iteration 2000, current loss: 5122.597915\n",
      "iteration 2001, current loss: 5122.462063\n",
      "iteration 2002, current loss: 5122.326211\n",
      "iteration 2003, current loss: 5122.190359\n",
      "iteration 2004, current loss: 5122.054507\n",
      "iteration 2005, current loss: 5121.918656\n",
      "iteration 2006, current loss: 5121.782805\n",
      "iteration 2007, current loss: 5121.646954\n",
      "iteration 2008, current loss: 5121.511102\n",
      "iteration 2009, current loss: 5121.375251\n",
      "iteration 2010, current loss: 5121.239400\n",
      "iteration 2011, current loss: 5121.103549\n",
      "iteration 2012, current loss: 5120.967697\n",
      "iteration 2013, current loss: 5120.831846\n",
      "iteration 2014, current loss: 5120.695994\n",
      "iteration 2015, current loss: 5120.560142\n",
      "iteration 2016, current loss: 5120.424289\n",
      "iteration 2017, current loss: 5120.288436\n",
      "iteration 2018, current loss: 5120.152583\n",
      "iteration 2019, current loss: 5120.016729\n",
      "iteration 2020, current loss: 5119.880874\n",
      "iteration 2021, current loss: 5119.745019\n",
      "iteration 2022, current loss: 5119.609163\n",
      "iteration 2023, current loss: 5119.473306\n",
      "iteration 2024, current loss: 5119.337449\n",
      "iteration 2025, current loss: 5119.201591\n",
      "iteration 2026, current loss: 5119.065732\n",
      "iteration 2027, current loss: 5118.929872\n",
      "iteration 2028, current loss: 5118.794011\n",
      "iteration 2029, current loss: 5118.658148\n",
      "iteration 2030, current loss: 5118.522285\n",
      "iteration 2031, current loss: 5118.386421\n",
      "iteration 2032, current loss: 5118.250556\n",
      "iteration 2033, current loss: 5118.114689\n",
      "iteration 2034, current loss: 5117.978821\n",
      "iteration 2035, current loss: 5117.842952\n",
      "iteration 2036, current loss: 5117.707081\n",
      "iteration 2037, current loss: 5117.571209\n",
      "iteration 2038, current loss: 5117.435335\n",
      "iteration 2039, current loss: 5117.299460\n",
      "iteration 2040, current loss: 5117.163584\n",
      "iteration 2041, current loss: 5117.027705\n",
      "iteration 2042, current loss: 5116.891825\n",
      "iteration 2043, current loss: 5116.755944\n",
      "iteration 2044, current loss: 5116.620060\n",
      "iteration 2045, current loss: 5116.484175\n",
      "iteration 2046, current loss: 5116.348288\n",
      "iteration 2047, current loss: 5116.212399\n",
      "iteration 2048, current loss: 5116.076508\n",
      "iteration 2049, current loss: 5115.940614\n",
      "iteration 2050, current loss: 5115.804719\n",
      "iteration 2051, current loss: 5115.668822\n",
      "iteration 2052, current loss: 5115.532923\n",
      "iteration 2053, current loss: 5115.397021\n",
      "iteration 2054, current loss: 5115.261118\n",
      "iteration 2055, current loss: 5115.125211\n",
      "iteration 2056, current loss: 5114.989303\n",
      "iteration 2057, current loss: 5114.853392\n",
      "iteration 2058, current loss: 5114.717479\n",
      "iteration 2059, current loss: 5114.581563\n",
      "iteration 2060, current loss: 5114.445645\n",
      "iteration 2061, current loss: 5114.309725\n",
      "iteration 2062, current loss: 5114.173801\n",
      "iteration 2063, current loss: 5114.037875\n",
      "iteration 2064, current loss: 5113.901947\n",
      "iteration 2065, current loss: 5113.766015\n",
      "iteration 2066, current loss: 5113.630081\n",
      "iteration 2067, current loss: 5113.494144\n",
      "iteration 2068, current loss: 5113.358205\n",
      "iteration 2069, current loss: 5113.222262\n",
      "iteration 2070, current loss: 5113.086317\n",
      "iteration 2071, current loss: 5112.950368\n",
      "iteration 2072, current loss: 5112.814417\n",
      "iteration 2073, current loss: 5112.678462\n",
      "iteration 2074, current loss: 5112.542504\n",
      "iteration 2075, current loss: 5112.406544\n",
      "iteration 2076, current loss: 5112.270580\n",
      "iteration 2077, current loss: 5112.134612\n",
      "iteration 2078, current loss: 5111.998642\n",
      "iteration 2079, current loss: 5111.862668\n",
      "iteration 2080, current loss: 5111.726692\n",
      "iteration 2081, current loss: 5111.590711\n",
      "iteration 2082, current loss: 5111.454728\n",
      "iteration 2083, current loss: 5111.318741\n",
      "iteration 2084, current loss: 5111.182750\n",
      "iteration 2085, current loss: 5111.046756\n",
      "iteration 2086, current loss: 5110.910759\n",
      "iteration 2087, current loss: 5110.774758\n",
      "iteration 2088, current loss: 5110.638753\n",
      "iteration 2089, current loss: 5110.502745\n",
      "iteration 2090, current loss: 5110.366733\n",
      "iteration 2091, current loss: 5110.230717\n",
      "iteration 2092, current loss: 5110.094698\n",
      "iteration 2093, current loss: 5109.958675\n",
      "iteration 2094, current loss: 5109.822649\n",
      "iteration 2095, current loss: 5109.686618\n",
      "iteration 2096, current loss: 5109.550584\n",
      "iteration 2097, current loss: 5109.414546\n",
      "iteration 2098, current loss: 5109.278504\n",
      "iteration 2099, current loss: 5109.142458\n",
      "iteration 2100, current loss: 5109.006408\n",
      "iteration 2101, current loss: 5108.870354\n",
      "iteration 2102, current loss: 5108.734297\n",
      "iteration 2103, current loss: 5108.598235\n",
      "iteration 2104, current loss: 5108.462170\n",
      "iteration 2105, current loss: 5108.326100\n",
      "iteration 2106, current loss: 5108.190026\n",
      "iteration 2107, current loss: 5108.053949\n",
      "iteration 2108, current loss: 5107.917867\n",
      "iteration 2109, current loss: 5107.781781\n",
      "iteration 2110, current loss: 5107.645691\n",
      "iteration 2111, current loss: 5107.509597\n",
      "iteration 2112, current loss: 5107.373499\n",
      "iteration 2113, current loss: 5107.237396\n",
      "iteration 2114, current loss: 5107.101290\n",
      "iteration 2115, current loss: 5106.965179\n",
      "iteration 2116, current loss: 5106.829064\n",
      "iteration 2117, current loss: 5106.692945\n",
      "iteration 2118, current loss: 5106.556822\n",
      "iteration 2119, current loss: 5106.420694\n",
      "iteration 2120, current loss: 5106.284562\n",
      "iteration 2121, current loss: 5106.148426\n",
      "iteration 2122, current loss: 5106.012286\n",
      "iteration 2123, current loss: 5105.876141\n",
      "iteration 2124, current loss: 5105.739993\n",
      "iteration 2125, current loss: 5105.603839\n",
      "iteration 2126, current loss: 5105.467682\n",
      "iteration 2127, current loss: 5105.331521\n",
      "iteration 2128, current loss: 5105.195355\n",
      "iteration 2129, current loss: 5105.059185\n",
      "iteration 2130, current loss: 5104.923010\n",
      "iteration 2131, current loss: 5104.786832\n",
      "iteration 2132, current loss: 5104.650649\n",
      "iteration 2133, current loss: 5104.514462\n",
      "iteration 2134, current loss: 5104.378270\n",
      "iteration 2135, current loss: 5104.242075\n",
      "iteration 2136, current loss: 5104.105875\n",
      "iteration 2137, current loss: 5103.969671\n",
      "iteration 2138, current loss: 5103.833462\n",
      "iteration 2139, current loss: 5103.697250\n",
      "iteration 2140, current loss: 5103.561033\n",
      "iteration 2141, current loss: 5103.424812\n",
      "iteration 2142, current loss: 5103.288587\n",
      "iteration 2143, current loss: 5103.152358\n",
      "iteration 2144, current loss: 5103.016125\n",
      "iteration 2145, current loss: 5102.879887\n",
      "iteration 2146, current loss: 5102.743646\n",
      "iteration 2147, current loss: 5102.607400\n",
      "iteration 2148, current loss: 5102.471150\n",
      "iteration 2149, current loss: 5102.334897\n",
      "iteration 2150, current loss: 5102.198639\n",
      "iteration 2151, current loss: 5102.062377\n",
      "iteration 2152, current loss: 5101.926111\n",
      "iteration 2153, current loss: 5101.789842\n",
      "iteration 2154, current loss: 5101.653568\n",
      "iteration 2155, current loss: 5101.517291\n",
      "iteration 2156, current loss: 5101.381009\n",
      "iteration 2157, current loss: 5101.244724\n",
      "iteration 2158, current loss: 5101.108435\n",
      "iteration 2159, current loss: 5100.972143\n",
      "iteration 2160, current loss: 5100.835846\n",
      "iteration 2161, current loss: 5100.699546\n",
      "iteration 2162, current loss: 5100.563242\n",
      "iteration 2163, current loss: 5100.426935\n",
      "iteration 2164, current loss: 5100.290624\n",
      "iteration 2165, current loss: 5100.154310\n",
      "iteration 2166, current loss: 5100.017992\n",
      "iteration 2167, current loss: 5099.881670\n",
      "iteration 2168, current loss: 5099.745346\n",
      "iteration 2169, current loss: 5099.609017\n",
      "iteration 2170, current loss: 5099.472686\n",
      "iteration 2171, current loss: 5099.336351\n",
      "iteration 2172, current loss: 5099.200014\n",
      "iteration 2173, current loss: 5099.063673\n",
      "iteration 2174, current loss: 5098.927328\n",
      "iteration 2175, current loss: 5098.790981\n",
      "iteration 2176, current loss: 5098.654631\n",
      "iteration 2177, current loss: 5098.518278\n",
      "iteration 2178, current loss: 5098.381922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2179, current loss: 5098.245564\n",
      "iteration 2180, current loss: 5098.109202\n",
      "iteration 2181, current loss: 5097.972838\n",
      "iteration 2182, current loss: 5097.836471\n",
      "iteration 2183, current loss: 5097.700102\n",
      "iteration 2184, current loss: 5097.563730\n",
      "iteration 2185, current loss: 5097.427356\n",
      "iteration 2186, current loss: 5097.290979\n",
      "iteration 2187, current loss: 5097.154600\n",
      "iteration 2188, current loss: 5097.018219\n",
      "iteration 2189, current loss: 5096.881836\n",
      "iteration 2190, current loss: 5096.745451\n",
      "iteration 2191, current loss: 5096.609064\n",
      "iteration 2192, current loss: 5096.472675\n",
      "iteration 2193, current loss: 5096.336284\n",
      "iteration 2194, current loss: 5096.199891\n",
      "iteration 2195, current loss: 5096.063497\n",
      "iteration 2196, current loss: 5095.927101\n",
      "iteration 2197, current loss: 5095.790703\n",
      "iteration 2198, current loss: 5095.654304\n",
      "iteration 2199, current loss: 5095.517904\n",
      "iteration 2200, current loss: 5095.381503\n",
      "iteration 2201, current loss: 5095.245100\n",
      "iteration 2202, current loss: 5095.108697\n",
      "iteration 2203, current loss: 5094.972292\n",
      "iteration 2204, current loss: 5094.835887\n",
      "iteration 2205, current loss: 5094.699481\n",
      "iteration 2206, current loss: 5094.563074\n",
      "iteration 2207, current loss: 5094.426666\n",
      "iteration 2208, current loss: 5094.290258\n",
      "iteration 2209, current loss: 5094.153850\n",
      "iteration 2210, current loss: 5094.017441\n",
      "iteration 2211, current loss: 5093.881033\n",
      "iteration 2212, current loss: 5093.744624\n",
      "iteration 2213, current loss: 5093.608215\n",
      "iteration 2214, current loss: 5093.471806\n",
      "iteration 2215, current loss: 5093.335398\n",
      "iteration 2216, current loss: 5093.198990\n",
      "iteration 2217, current loss: 5093.062582\n",
      "iteration 2218, current loss: 5092.926175\n",
      "iteration 2219, current loss: 5092.789769\n",
      "iteration 2220, current loss: 5092.653363\n",
      "iteration 2221, current loss: 5092.516958\n",
      "iteration 2222, current loss: 5092.380555\n",
      "iteration 2223, current loss: 5092.244152\n",
      "iteration 2224, current loss: 5092.107751\n",
      "iteration 2225, current loss: 5091.971351\n",
      "iteration 2226, current loss: 5091.834953\n",
      "iteration 2227, current loss: 5091.698556\n",
      "iteration 2228, current loss: 5091.562161\n",
      "iteration 2229, current loss: 5091.425768\n",
      "iteration 2230, current loss: 5091.289377\n",
      "iteration 2231, current loss: 5091.152988\n",
      "iteration 2232, current loss: 5091.016601\n",
      "iteration 2233, current loss: 5090.880217\n",
      "iteration 2234, current loss: 5090.743835\n",
      "iteration 2235, current loss: 5090.607456\n",
      "iteration 2236, current loss: 5090.471079\n",
      "iteration 2237, current loss: 5090.334706\n",
      "iteration 2238, current loss: 5090.198335\n",
      "iteration 2239, current loss: 5090.061968\n",
      "iteration 2240, current loss: 5089.925604\n",
      "iteration 2241, current loss: 5089.789243\n",
      "iteration 2242, current loss: 5089.652886\n",
      "iteration 2243, current loss: 5089.516532\n",
      "iteration 2244, current loss: 5089.380183\n",
      "iteration 2245, current loss: 5089.243837\n",
      "iteration 2246, current loss: 5089.107495\n",
      "iteration 2247, current loss: 5088.971158\n",
      "iteration 2248, current loss: 5088.834825\n",
      "iteration 2249, current loss: 5088.698497\n",
      "iteration 2250, current loss: 5088.562173\n",
      "iteration 2251, current loss: 5088.425854\n",
      "iteration 2252, current loss: 5088.289540\n",
      "iteration 2253, current loss: 5088.153232\n",
      "iteration 2254, current loss: 5088.016928\n",
      "iteration 2255, current loss: 5087.880630\n",
      "iteration 2256, current loss: 5087.744337\n",
      "iteration 2257, current loss: 5087.608050\n",
      "iteration 2258, current loss: 5087.471769\n",
      "iteration 2259, current loss: 5087.335494\n",
      "iteration 2260, current loss: 5087.199225\n",
      "iteration 2261, current loss: 5087.062962\n",
      "iteration 2262, current loss: 5086.926706\n",
      "iteration 2263, current loss: 5086.790456\n",
      "iteration 2264, current loss: 5086.654213\n",
      "iteration 2265, current loss: 5086.517976\n",
      "iteration 2266, current loss: 5086.381747\n",
      "iteration 2267, current loss: 5086.245525\n",
      "iteration 2268, current loss: 5086.109310\n",
      "iteration 2269, current loss: 5085.973103\n",
      "iteration 2270, current loss: 5085.836903\n",
      "iteration 2271, current loss: 5085.700711\n",
      "iteration 2272, current loss: 5085.564527\n",
      "iteration 2273, current loss: 5085.428351\n",
      "iteration 2274, current loss: 5085.292183\n",
      "iteration 2275, current loss: 5085.156024\n",
      "iteration 2276, current loss: 5085.019873\n",
      "iteration 2277, current loss: 5084.883730\n",
      "iteration 2278, current loss: 5084.747597\n",
      "iteration 2279, current loss: 5084.611472\n",
      "iteration 2280, current loss: 5084.475357\n",
      "iteration 2281, current loss: 5084.339251\n",
      "iteration 2282, current loss: 5084.203154\n",
      "iteration 2283, current loss: 5084.067067\n",
      "iteration 2284, current loss: 5083.930989\n",
      "iteration 2285, current loss: 5083.794922\n",
      "iteration 2286, current loss: 5083.658864\n",
      "iteration 2287, current loss: 5083.522817\n",
      "iteration 2288, current loss: 5083.386780\n",
      "iteration 2289, current loss: 5083.250753\n",
      "iteration 2290, current loss: 5083.114737\n",
      "iteration 2291, current loss: 5082.978732\n",
      "iteration 2292, current loss: 5082.842738\n",
      "iteration 2293, current loss: 5082.706755\n",
      "iteration 2294, current loss: 5082.570783\n",
      "iteration 2295, current loss: 5082.434822\n",
      "iteration 2296, current loss: 5082.298873\n",
      "iteration 2297, current loss: 5082.162935\n",
      "iteration 2298, current loss: 5082.027010\n",
      "iteration 2299, current loss: 5081.891096\n",
      "iteration 2300, current loss: 5081.755194\n",
      "iteration 2301, current loss: 5081.619305\n",
      "iteration 2302, current loss: 5081.483428\n",
      "iteration 2303, current loss: 5081.347563\n",
      "iteration 2304, current loss: 5081.211712\n",
      "iteration 2305, current loss: 5081.075873\n",
      "iteration 2306, current loss: 5080.940047\n",
      "iteration 2307, current loss: 5080.804234\n",
      "iteration 2308, current loss: 5080.668434\n",
      "iteration 2309, current loss: 5080.532648\n",
      "iteration 2310, current loss: 5080.396875\n",
      "iteration 2311, current loss: 5080.261116\n",
      "iteration 2312, current loss: 5080.125370\n",
      "iteration 2313, current loss: 5079.989639\n",
      "iteration 2314, current loss: 5079.853922\n",
      "iteration 2315, current loss: 5079.718218\n",
      "iteration 2316, current loss: 5079.582530\n",
      "iteration 2317, current loss: 5079.446855\n",
      "iteration 2318, current loss: 5079.311196\n",
      "iteration 2319, current loss: 5079.175551\n",
      "iteration 2320, current loss: 5079.039921\n",
      "iteration 2321, current loss: 5078.904305\n",
      "iteration 2322, current loss: 5078.768705\n",
      "iteration 2323, current loss: 5078.633121\n",
      "iteration 2324, current loss: 5078.497551\n",
      "iteration 2325, current loss: 5078.361998\n",
      "iteration 2326, current loss: 5078.226460\n",
      "iteration 2327, current loss: 5078.090937\n",
      "iteration 2328, current loss: 5077.955431\n",
      "iteration 2329, current loss: 5077.819940\n",
      "iteration 2330, current loss: 5077.684466\n",
      "iteration 2331, current loss: 5077.549008\n",
      "iteration 2332, current loss: 5077.413566\n",
      "iteration 2333, current loss: 5077.278141\n",
      "iteration 2334, current loss: 5077.142732\n",
      "iteration 2335, current loss: 5077.007341\n",
      "iteration 2336, current loss: 5076.871966\n",
      "iteration 2337, current loss: 5076.736608\n",
      "iteration 2338, current loss: 5076.601267\n",
      "iteration 2339, current loss: 5076.465943\n",
      "iteration 2340, current loss: 5076.330636\n",
      "iteration 2341, current loss: 5076.195347\n",
      "iteration 2342, current loss: 5076.060076\n",
      "iteration 2343, current loss: 5075.924822\n",
      "iteration 2344, current loss: 5075.789586\n",
      "iteration 2345, current loss: 5075.654367\n",
      "iteration 2346, current loss: 5075.519167\n",
      "iteration 2347, current loss: 5075.383985\n",
      "iteration 2348, current loss: 5075.248821\n",
      "iteration 2349, current loss: 5075.113675\n",
      "iteration 2350, current loss: 5074.978547\n",
      "iteration 2351, current loss: 5074.843438\n",
      "iteration 2352, current loss: 5074.708347\n",
      "iteration 2353, current loss: 5074.573275\n",
      "iteration 2354, current loss: 5074.438222\n",
      "iteration 2355, current loss: 5074.303188\n",
      "iteration 2356, current loss: 5074.168172\n",
      "iteration 2357, current loss: 5074.033176\n",
      "iteration 2358, current loss: 5073.898198\n",
      "iteration 2359, current loss: 5073.763240\n",
      "iteration 2360, current loss: 5073.628301\n",
      "iteration 2361, current loss: 5073.493382\n",
      "iteration 2362, current loss: 5073.358482\n",
      "iteration 2363, current loss: 5073.223601\n",
      "iteration 2364, current loss: 5073.088740\n",
      "iteration 2365, current loss: 5072.953899\n",
      "iteration 2366, current loss: 5072.819077\n",
      "iteration 2367, current loss: 5072.684276\n",
      "iteration 2368, current loss: 5072.549494\n",
      "iteration 2369, current loss: 5072.414732\n",
      "iteration 2370, current loss: 5072.279991\n",
      "iteration 2371, current loss: 5072.145269\n",
      "iteration 2372, current loss: 5072.010568\n",
      "iteration 2373, current loss: 5071.875887\n",
      "iteration 2374, current loss: 5071.741226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2375, current loss: 5071.606586\n",
      "iteration 2376, current loss: 5071.471967\n",
      "iteration 2377, current loss: 5071.337367\n",
      "iteration 2378, current loss: 5071.202789\n",
      "iteration 2379, current loss: 5071.068231\n",
      "iteration 2380, current loss: 5070.933694\n",
      "iteration 2381, current loss: 5070.799178\n",
      "iteration 2382, current loss: 5070.664683\n",
      "iteration 2383, current loss: 5070.530208\n",
      "iteration 2384, current loss: 5070.395755\n",
      "iteration 2385, current loss: 5070.261322\n",
      "iteration 2386, current loss: 5070.126911\n",
      "iteration 2387, current loss: 5069.992521\n",
      "iteration 2388, current loss: 5069.858152\n",
      "iteration 2389, current loss: 5069.723804\n",
      "iteration 2390, current loss: 5069.589478\n",
      "iteration 2391, current loss: 5069.455173\n",
      "iteration 2392, current loss: 5069.320890\n",
      "iteration 2393, current loss: 5069.186627\n",
      "iteration 2394, current loss: 5069.052387\n",
      "iteration 2395, current loss: 5068.918168\n",
      "iteration 2396, current loss: 5068.783970\n",
      "iteration 2397, current loss: 5068.649794\n",
      "iteration 2398, current loss: 5068.515640\n",
      "iteration 2399, current loss: 5068.381508\n",
      "iteration 2400, current loss: 5068.247397\n",
      "iteration 2401, current loss: 5068.113308\n",
      "iteration 2402, current loss: 5067.979241\n",
      "iteration 2403, current loss: 5067.845196\n",
      "iteration 2404, current loss: 5067.711172\n",
      "iteration 2405, current loss: 5067.577171\n",
      "iteration 2406, current loss: 5067.443191\n",
      "iteration 2407, current loss: 5067.309234\n",
      "iteration 2408, current loss: 5067.175298\n",
      "iteration 2409, current loss: 5067.041385\n",
      "iteration 2410, current loss: 5066.907493\n",
      "iteration 2411, current loss: 5066.773624\n",
      "iteration 2412, current loss: 5066.639777\n",
      "iteration 2413, current loss: 5066.505952\n",
      "iteration 2414, current loss: 5066.372149\n",
      "iteration 2415, current loss: 5066.238368\n",
      "iteration 2416, current loss: 5066.104610\n",
      "iteration 2417, current loss: 5065.970874\n",
      "iteration 2418, current loss: 5065.837160\n",
      "iteration 2419, current loss: 5065.703468\n",
      "iteration 2420, current loss: 5065.569799\n",
      "iteration 2421, current loss: 5065.436152\n",
      "iteration 2422, current loss: 5065.302527\n",
      "iteration 2423, current loss: 5065.168925\n",
      "iteration 2424, current loss: 5065.035345\n",
      "iteration 2425, current loss: 5064.901787\n",
      "iteration 2426, current loss: 5064.768252\n",
      "iteration 2427, current loss: 5064.634739\n",
      "iteration 2428, current loss: 5064.501249\n",
      "iteration 2429, current loss: 5064.367781\n",
      "iteration 2430, current loss: 5064.234335\n",
      "iteration 2431, current loss: 5064.100912\n",
      "iteration 2432, current loss: 5063.967511\n",
      "iteration 2433, current loss: 5063.834133\n",
      "iteration 2434, current loss: 5063.700778\n",
      "iteration 2435, current loss: 5063.567444\n",
      "iteration 2436, current loss: 5063.434133\n",
      "iteration 2437, current loss: 5063.300845\n",
      "iteration 2438, current loss: 5063.167579\n",
      "iteration 2439, current loss: 5063.034336\n",
      "iteration 2440, current loss: 5062.901115\n",
      "iteration 2441, current loss: 5062.767917\n",
      "iteration 2442, current loss: 5062.634741\n",
      "iteration 2443, current loss: 5062.501588\n",
      "iteration 2444, current loss: 5062.368457\n",
      "iteration 2445, current loss: 5062.235348\n",
      "iteration 2446, current loss: 5062.102262\n",
      "iteration 2447, current loss: 5061.969199\n",
      "iteration 2448, current loss: 5061.836158\n",
      "iteration 2449, current loss: 5061.703139\n",
      "iteration 2450, current loss: 5061.570143\n",
      "iteration 2451, current loss: 5061.437170\n",
      "iteration 2452, current loss: 5061.304218\n",
      "iteration 2453, current loss: 5061.171290\n",
      "iteration 2454, current loss: 5061.038384\n",
      "iteration 2455, current loss: 5060.905500\n",
      "iteration 2456, current loss: 5060.772638\n",
      "iteration 2457, current loss: 5060.639799\n",
      "iteration 2458, current loss: 5060.506983\n",
      "iteration 2459, current loss: 5060.374189\n",
      "iteration 2460, current loss: 5060.241417\n",
      "iteration 2461, current loss: 5060.108667\n",
      "iteration 2462, current loss: 5059.975940\n",
      "iteration 2463, current loss: 5059.843236\n",
      "iteration 2464, current loss: 5059.710553\n",
      "iteration 2465, current loss: 5059.577893\n",
      "iteration 2466, current loss: 5059.445256\n",
      "iteration 2467, current loss: 5059.312640\n",
      "iteration 2468, current loss: 5059.180047\n",
      "iteration 2469, current loss: 5059.047476\n",
      "iteration 2470, current loss: 5058.914928\n",
      "iteration 2471, current loss: 5058.782401\n",
      "iteration 2472, current loss: 5058.649897\n",
      "iteration 2473, current loss: 5058.517415\n",
      "iteration 2474, current loss: 5058.384956\n",
      "iteration 2475, current loss: 5058.252518\n",
      "iteration 2476, current loss: 5058.120103\n",
      "iteration 2477, current loss: 5057.987709\n",
      "iteration 2478, current loss: 5057.855338\n",
      "iteration 2479, current loss: 5057.722989\n",
      "iteration 2480, current loss: 5057.590662\n",
      "iteration 2481, current loss: 5057.458357\n",
      "iteration 2482, current loss: 5057.326075\n",
      "iteration 2483, current loss: 5057.193814\n",
      "iteration 2484, current loss: 5057.061575\n",
      "iteration 2485, current loss: 5056.929358\n",
      "iteration 2486, current loss: 5056.797163\n",
      "iteration 2487, current loss: 5056.664991\n",
      "iteration 2488, current loss: 5056.532840\n",
      "iteration 2489, current loss: 5056.400710\n",
      "iteration 2490, current loss: 5056.268603\n",
      "iteration 2491, current loss: 5056.136518\n",
      "iteration 2492, current loss: 5056.004455\n",
      "iteration 2493, current loss: 5055.872413\n",
      "iteration 2494, current loss: 5055.740393\n",
      "iteration 2495, current loss: 5055.608395\n",
      "iteration 2496, current loss: 5055.476419\n",
      "iteration 2497, current loss: 5055.344464\n",
      "iteration 2498, current loss: 5055.212531\n",
      "iteration 2499, current loss: 5055.080620\n",
      "iteration 2500, current loss: 5054.948730\n",
      "iteration 2501, current loss: 5054.816862\n",
      "iteration 2502, current loss: 5054.685016\n",
      "iteration 2503, current loss: 5054.553191\n",
      "iteration 2504, current loss: 5054.421388\n",
      "iteration 2505, current loss: 5054.289606\n",
      "iteration 2506, current loss: 5054.157846\n",
      "iteration 2507, current loss: 5054.026107\n",
      "iteration 2508, current loss: 5053.894390\n",
      "iteration 2509, current loss: 5053.762694\n",
      "iteration 2510, current loss: 5053.631020\n",
      "iteration 2511, current loss: 5053.499367\n",
      "iteration 2512, current loss: 5053.367736\n",
      "iteration 2513, current loss: 5053.236125\n",
      "iteration 2514, current loss: 5053.104536\n",
      "iteration 2515, current loss: 5052.972969\n",
      "iteration 2516, current loss: 5052.841422\n",
      "iteration 2517, current loss: 5052.709897\n",
      "iteration 2518, current loss: 5052.578393\n",
      "iteration 2519, current loss: 5052.446911\n",
      "iteration 2520, current loss: 5052.315449\n",
      "iteration 2521, current loss: 5052.184009\n",
      "iteration 2522, current loss: 5052.052589\n",
      "iteration 2523, current loss: 5051.921191\n",
      "iteration 2524, current loss: 5051.789814\n",
      "iteration 2525, current loss: 5051.658458\n",
      "iteration 2526, current loss: 5051.527123\n",
      "iteration 2527, current loss: 5051.395809\n",
      "iteration 2528, current loss: 5051.264515\n",
      "iteration 2529, current loss: 5051.133243\n",
      "iteration 2530, current loss: 5051.001992\n",
      "iteration 2531, current loss: 5050.870762\n",
      "iteration 2532, current loss: 5050.739552\n",
      "iteration 2533, current loss: 5050.608363\n",
      "iteration 2534, current loss: 5050.477195\n",
      "iteration 2535, current loss: 5050.346048\n",
      "iteration 2536, current loss: 5050.214922\n",
      "iteration 2537, current loss: 5050.083816\n",
      "iteration 2538, current loss: 5049.952731\n",
      "iteration 2539, current loss: 5049.821667\n",
      "iteration 2540, current loss: 5049.690624\n",
      "iteration 2541, current loss: 5049.559601\n",
      "iteration 2542, current loss: 5049.428599\n",
      "iteration 2543, current loss: 5049.297617\n",
      "iteration 2544, current loss: 5049.166656\n",
      "iteration 2545, current loss: 5049.035715\n",
      "iteration 2546, current loss: 5048.904795\n",
      "iteration 2547, current loss: 5048.773896\n",
      "iteration 2548, current loss: 5048.643017\n",
      "iteration 2549, current loss: 5048.512158\n",
      "iteration 2550, current loss: 5048.381320\n",
      "iteration 2551, current loss: 5048.250502\n",
      "iteration 2552, current loss: 5048.119705\n",
      "iteration 2553, current loss: 5047.988928\n",
      "iteration 2554, current loss: 5047.858171\n",
      "iteration 2555, current loss: 5047.727435\n",
      "iteration 2556, current loss: 5047.596719\n",
      "iteration 2557, current loss: 5047.466023\n",
      "iteration 2558, current loss: 5047.335347\n",
      "iteration 2559, current loss: 5047.204692\n",
      "iteration 2560, current loss: 5047.074057\n",
      "iteration 2561, current loss: 5046.943442\n",
      "iteration 2562, current loss: 5046.812847\n",
      "iteration 2563, current loss: 5046.682273\n",
      "iteration 2564, current loss: 5046.551718\n",
      "iteration 2565, current loss: 5046.421184\n",
      "iteration 2566, current loss: 5046.290670\n",
      "iteration 2567, current loss: 5046.160175\n",
      "iteration 2568, current loss: 5046.029701\n",
      "iteration 2569, current loss: 5045.899247\n",
      "iteration 2570, current loss: 5045.768813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2571, current loss: 5045.638399\n",
      "iteration 2572, current loss: 5045.508004\n",
      "iteration 2573, current loss: 5045.377630\n",
      "iteration 2574, current loss: 5045.247276\n",
      "iteration 2575, current loss: 5045.116941\n",
      "iteration 2576, current loss: 5044.986627\n",
      "iteration 2577, current loss: 5044.856332\n",
      "iteration 2578, current loss: 5044.726057\n",
      "iteration 2579, current loss: 5044.595802\n",
      "iteration 2580, current loss: 5044.465567\n",
      "iteration 2581, current loss: 5044.335352\n",
      "iteration 2582, current loss: 5044.205156\n",
      "iteration 2583, current loss: 5044.074981\n",
      "iteration 2584, current loss: 5043.944825\n",
      "iteration 2585, current loss: 5043.814688\n",
      "iteration 2586, current loss: 5043.684572\n",
      "iteration 2587, current loss: 5043.554475\n",
      "iteration 2588, current loss: 5043.424398\n",
      "iteration 2589, current loss: 5043.294340\n",
      "iteration 2590, current loss: 5043.164303\n",
      "iteration 2591, current loss: 5043.034284\n",
      "iteration 2592, current loss: 5042.904286\n",
      "iteration 2593, current loss: 5042.774307\n",
      "iteration 2594, current loss: 5042.644348\n",
      "iteration 2595, current loss: 5042.514408\n",
      "iteration 2596, current loss: 5042.384488\n",
      "iteration 2597, current loss: 5042.254588\n",
      "iteration 2598, current loss: 5042.124707\n",
      "iteration 2599, current loss: 5041.994845\n",
      "iteration 2600, current loss: 5041.865003\n",
      "iteration 2601, current loss: 5041.735181\n",
      "iteration 2602, current loss: 5041.605378\n",
      "iteration 2603, current loss: 5041.475595\n",
      "iteration 2604, current loss: 5041.345831\n",
      "iteration 2605, current loss: 5041.216087\n",
      "iteration 2606, current loss: 5041.086362\n",
      "iteration 2607, current loss: 5040.956657\n",
      "iteration 2608, current loss: 5040.826971\n",
      "iteration 2609, current loss: 5040.697304\n",
      "iteration 2610, current loss: 5040.567657\n",
      "iteration 2611, current loss: 5040.438030\n",
      "iteration 2612, current loss: 5040.308422\n",
      "iteration 2613, current loss: 5040.178833\n",
      "iteration 2614, current loss: 5040.049264\n",
      "iteration 2615, current loss: 5039.919714\n",
      "iteration 2616, current loss: 5039.790183\n",
      "iteration 2617, current loss: 5039.660672\n",
      "iteration 2618, current loss: 5039.531180\n",
      "iteration 2619, current loss: 5039.401708\n",
      "iteration 2620, current loss: 5039.272255\n",
      "iteration 2621, current loss: 5039.142821\n",
      "iteration 2622, current loss: 5039.013407\n",
      "iteration 2623, current loss: 5038.884012\n",
      "iteration 2624, current loss: 5038.754637\n",
      "iteration 2625, current loss: 5038.625280\n",
      "iteration 2626, current loss: 5038.495944\n",
      "iteration 2627, current loss: 5038.366626\n",
      "iteration 2628, current loss: 5038.237328\n",
      "iteration 2629, current loss: 5038.108049\n",
      "iteration 2630, current loss: 5037.978790\n",
      "iteration 2631, current loss: 5037.849550\n",
      "iteration 2632, current loss: 5037.720329\n",
      "iteration 2633, current loss: 5037.591128\n",
      "iteration 2634, current loss: 5037.461945\n",
      "iteration 2635, current loss: 5037.332783\n",
      "iteration 2636, current loss: 5037.203639\n",
      "iteration 2637, current loss: 5037.074515\n",
      "iteration 2638, current loss: 5036.945411\n",
      "iteration 2639, current loss: 5036.816325\n",
      "iteration 2640, current loss: 5036.687259\n",
      "iteration 2641, current loss: 5036.558212\n",
      "iteration 2642, current loss: 5036.429185\n",
      "iteration 2643, current loss: 5036.300177\n",
      "iteration 2644, current loss: 5036.171188\n",
      "iteration 2645, current loss: 5036.042219\n",
      "iteration 2646, current loss: 5035.913269\n",
      "iteration 2647, current loss: 5035.784339\n",
      "iteration 2648, current loss: 5035.655427\n",
      "iteration 2649, current loss: 5035.526536\n",
      "iteration 2650, current loss: 5035.397663\n",
      "iteration 2651, current loss: 5035.268810\n",
      "iteration 2652, current loss: 5035.139976\n",
      "iteration 2653, current loss: 5035.011162\n",
      "iteration 2654, current loss: 5034.882367\n",
      "iteration 2655, current loss: 5034.753591\n",
      "iteration 2656, current loss: 5034.624835\n",
      "iteration 2657, current loss: 5034.496098\n",
      "iteration 2658, current loss: 5034.367381\n",
      "iteration 2659, current loss: 5034.238683\n",
      "iteration 2660, current loss: 5034.110005\n",
      "iteration 2661, current loss: 5033.981346\n",
      "iteration 2662, current loss: 5033.852706\n",
      "iteration 2663, current loss: 5033.724086\n",
      "iteration 2664, current loss: 5033.595485\n",
      "iteration 2665, current loss: 5033.466904\n",
      "iteration 2666, current loss: 5033.338342\n",
      "iteration 2667, current loss: 5033.209800\n",
      "iteration 2668, current loss: 5033.081278\n",
      "iteration 2669, current loss: 5032.952774\n",
      "iteration 2670, current loss: 5032.824291\n",
      "iteration 2671, current loss: 5032.695826\n",
      "iteration 2672, current loss: 5032.567382\n",
      "iteration 2673, current loss: 5032.438957\n",
      "iteration 2674, current loss: 5032.310551\n",
      "iteration 2675, current loss: 5032.182165\n",
      "iteration 2676, current loss: 5032.053799\n",
      "iteration 2677, current loss: 5031.925452\n",
      "iteration 2678, current loss: 5031.797125\n",
      "iteration 2679, current loss: 5031.668817\n",
      "iteration 2680, current loss: 5031.540530\n",
      "iteration 2681, current loss: 5031.412261\n",
      "iteration 2682, current loss: 5031.284013\n",
      "iteration 2683, current loss: 5031.155784\n",
      "iteration 2684, current loss: 5031.027575\n",
      "iteration 2685, current loss: 5030.899385\n",
      "iteration 2686, current loss: 5030.771215\n",
      "iteration 2687, current loss: 5030.643065\n",
      "iteration 2688, current loss: 5030.514935\n",
      "iteration 2689, current loss: 5030.386824\n",
      "iteration 2690, current loss: 5030.258733\n",
      "iteration 2691, current loss: 5030.130662\n",
      "iteration 2692, current loss: 5030.002611\n",
      "iteration 2693, current loss: 5029.874579\n",
      "iteration 2694, current loss: 5029.746567\n",
      "iteration 2695, current loss: 5029.618576\n",
      "iteration 2696, current loss: 5029.490604\n",
      "iteration 2697, current loss: 5029.362651\n",
      "iteration 2698, current loss: 5029.234719\n",
      "iteration 2699, current loss: 5029.106807\n",
      "iteration 2700, current loss: 5028.978914\n",
      "iteration 2701, current loss: 5028.851042\n",
      "iteration 2702, current loss: 5028.723189\n",
      "iteration 2703, current loss: 5028.595357\n",
      "iteration 2704, current loss: 5028.467544\n",
      "iteration 2705, current loss: 5028.339751\n",
      "iteration 2706, current loss: 5028.211979\n",
      "iteration 2707, current loss: 5028.084226\n",
      "iteration 2708, current loss: 5027.956493\n",
      "iteration 2709, current loss: 5027.828781\n",
      "iteration 2710, current loss: 5027.701088\n",
      "iteration 2711, current loss: 5027.573416\n",
      "iteration 2712, current loss: 5027.445764\n",
      "iteration 2713, current loss: 5027.318132\n",
      "iteration 2714, current loss: 5027.190519\n",
      "iteration 2715, current loss: 5027.062928\n",
      "iteration 2716, current loss: 5026.935356\n",
      "iteration 2717, current loss: 5026.807804\n",
      "iteration 2718, current loss: 5026.680273\n",
      "iteration 2719, current loss: 5026.552762\n",
      "iteration 2720, current loss: 5026.425271\n",
      "iteration 2721, current loss: 5026.297800\n",
      "iteration 2722, current loss: 5026.170350\n",
      "iteration 2723, current loss: 5026.042920\n",
      "iteration 2724, current loss: 5025.915510\n",
      "iteration 2725, current loss: 5025.788120\n",
      "iteration 2726, current loss: 5025.660751\n",
      "iteration 2727, current loss: 5025.533402\n",
      "iteration 2728, current loss: 5025.406074\n",
      "iteration 2729, current loss: 5025.278766\n",
      "iteration 2730, current loss: 5025.151478\n",
      "iteration 2731, current loss: 5025.024211\n",
      "iteration 2732, current loss: 5024.896964\n",
      "iteration 2733, current loss: 5024.769737\n",
      "iteration 2734, current loss: 5024.642531\n",
      "iteration 2735, current loss: 5024.515346\n",
      "iteration 2736, current loss: 5024.388180\n",
      "iteration 2737, current loss: 5024.261036\n",
      "iteration 2738, current loss: 5024.133912\n",
      "iteration 2739, current loss: 5024.006808\n",
      "iteration 2740, current loss: 5023.879725\n",
      "iteration 2741, current loss: 5023.752663\n",
      "iteration 2742, current loss: 5023.625621\n",
      "iteration 2743, current loss: 5023.498599\n",
      "iteration 2744, current loss: 5023.371599\n",
      "iteration 2745, current loss: 5023.244618\n",
      "iteration 2746, current loss: 5023.117659\n",
      "iteration 2747, current loss: 5022.990720\n",
      "iteration 2748, current loss: 5022.863802\n",
      "iteration 2749, current loss: 5022.736904\n",
      "iteration 2750, current loss: 5022.610027\n",
      "iteration 2751, current loss: 5022.483171\n",
      "iteration 2752, current loss: 5022.356335\n",
      "iteration 2753, current loss: 5022.229520\n",
      "iteration 2754, current loss: 5022.102726\n",
      "iteration 2755, current loss: 5021.975953\n",
      "iteration 2756, current loss: 5021.849200\n",
      "iteration 2757, current loss: 5021.722468\n",
      "iteration 2758, current loss: 5021.595757\n",
      "iteration 2759, current loss: 5021.469067\n",
      "iteration 2760, current loss: 5021.342397\n",
      "iteration 2761, current loss: 5021.215748\n",
      "iteration 2762, current loss: 5021.089120\n",
      "iteration 2763, current loss: 5020.962513\n",
      "iteration 2764, current loss: 5020.835926\n",
      "iteration 2765, current loss: 5020.709361\n",
      "iteration 2766, current loss: 5020.582816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2767, current loss: 5020.456292\n",
      "iteration 2768, current loss: 5020.329789\n",
      "iteration 2769, current loss: 5020.203306\n",
      "iteration 2770, current loss: 5020.076845\n",
      "iteration 2771, current loss: 5019.950404\n",
      "iteration 2772, current loss: 5019.823985\n",
      "iteration 2773, current loss: 5019.697586\n",
      "iteration 2774, current loss: 5019.571208\n",
      "iteration 2775, current loss: 5019.444851\n",
      "iteration 2776, current loss: 5019.318514\n",
      "iteration 2777, current loss: 5019.192199\n",
      "iteration 2778, current loss: 5019.065905\n",
      "iteration 2779, current loss: 5018.939631\n",
      "iteration 2780, current loss: 5018.813378\n",
      "iteration 2781, current loss: 5018.687146\n",
      "iteration 2782, current loss: 5018.560936\n",
      "iteration 2783, current loss: 5018.434746\n",
      "iteration 2784, current loss: 5018.308576\n",
      "iteration 2785, current loss: 5018.182428\n",
      "iteration 2786, current loss: 5018.056301\n",
      "iteration 2787, current loss: 5017.930195\n",
      "iteration 2788, current loss: 5017.804109\n",
      "iteration 2789, current loss: 5017.678044\n",
      "iteration 2790, current loss: 5017.552001\n",
      "iteration 2791, current loss: 5017.425978\n",
      "iteration 2792, current loss: 5017.299976\n",
      "iteration 2793, current loss: 5017.173995\n",
      "iteration 2794, current loss: 5017.048035\n",
      "iteration 2795, current loss: 5016.922095\n",
      "iteration 2796, current loss: 5016.796177\n",
      "iteration 2797, current loss: 5016.670279\n",
      "iteration 2798, current loss: 5016.544402\n",
      "iteration 2799, current loss: 5016.418546\n",
      "iteration 2800, current loss: 5016.292711\n",
      "iteration 2801, current loss: 5016.166897\n",
      "iteration 2802, current loss: 5016.041104\n",
      "iteration 2803, current loss: 5015.915331\n",
      "iteration 2804, current loss: 5015.789579\n",
      "iteration 2805, current loss: 5015.663848\n",
      "iteration 2806, current loss: 5015.538138\n",
      "iteration 2807, current loss: 5015.412448\n",
      "iteration 2808, current loss: 5015.286780\n",
      "iteration 2809, current loss: 5015.161132\n",
      "iteration 2810, current loss: 5015.035504\n",
      "iteration 2811, current loss: 5014.909898\n",
      "iteration 2812, current loss: 5014.784312\n",
      "iteration 2813, current loss: 5014.658747\n",
      "iteration 2814, current loss: 5014.533203\n",
      "iteration 2815, current loss: 5014.407679\n",
      "iteration 2816, current loss: 5014.282176\n",
      "iteration 2817, current loss: 5014.156693\n",
      "iteration 2818, current loss: 5014.031232\n",
      "iteration 2819, current loss: 5013.905790\n",
      "iteration 2820, current loss: 5013.780370\n",
      "iteration 2821, current loss: 5013.654969\n",
      "iteration 2822, current loss: 5013.529590\n",
      "iteration 2823, current loss: 5013.404231\n",
      "iteration 2824, current loss: 5013.278892\n",
      "iteration 2825, current loss: 5013.153574\n",
      "iteration 2826, current loss: 5013.028277\n",
      "iteration 2827, current loss: 5012.903000\n",
      "iteration 2828, current loss: 5012.777743\n",
      "iteration 2829, current loss: 5012.652506\n",
      "iteration 2830, current loss: 5012.527290\n",
      "iteration 2831, current loss: 5012.402095\n",
      "iteration 2832, current loss: 5012.276919\n",
      "iteration 2833, current loss: 5012.151764\n",
      "iteration 2834, current loss: 5012.026630\n",
      "iteration 2835, current loss: 5011.901515\n",
      "iteration 2836, current loss: 5011.776421\n",
      "iteration 2837, current loss: 5011.651347\n",
      "iteration 2838, current loss: 5011.526293\n",
      "iteration 2839, current loss: 5011.401259\n",
      "iteration 2840, current loss: 5011.276245\n",
      "iteration 2841, current loss: 5011.151251\n",
      "iteration 2842, current loss: 5011.026278\n",
      "iteration 2843, current loss: 5010.901324\n",
      "iteration 2844, current loss: 5010.776390\n",
      "iteration 2845, current loss: 5010.651477\n",
      "iteration 2846, current loss: 5010.526583\n",
      "iteration 2847, current loss: 5010.401709\n",
      "iteration 2848, current loss: 5010.276855\n",
      "iteration 2849, current loss: 5010.152020\n",
      "iteration 2850, current loss: 5010.027206\n",
      "iteration 2851, current loss: 5009.902411\n",
      "iteration 2852, current loss: 5009.777636\n",
      "iteration 2853, current loss: 5009.652880\n",
      "iteration 2854, current loss: 5009.528144\n",
      "iteration 2855, current loss: 5009.403428\n",
      "iteration 2856, current loss: 5009.278731\n",
      "iteration 2857, current loss: 5009.154054\n",
      "iteration 2858, current loss: 5009.029396\n",
      "iteration 2859, current loss: 5008.904758\n",
      "iteration 2860, current loss: 5008.780139\n",
      "iteration 2861, current loss: 5008.655539\n",
      "iteration 2862, current loss: 5008.530959\n",
      "iteration 2863, current loss: 5008.406398\n",
      "iteration 2864, current loss: 5008.281856\n",
      "iteration 2865, current loss: 5008.157333\n",
      "iteration 2866, current loss: 5008.032830\n",
      "iteration 2867, current loss: 5007.908345\n",
      "iteration 2868, current loss: 5007.783880\n",
      "iteration 2869, current loss: 5007.659433\n",
      "iteration 2870, current loss: 5007.535006\n",
      "iteration 2871, current loss: 5007.410597\n",
      "iteration 2872, current loss: 5007.286207\n",
      "iteration 2873, current loss: 5007.161836\n",
      "iteration 2874, current loss: 5007.037484\n",
      "iteration 2875, current loss: 5006.913150\n",
      "iteration 2876, current loss: 5006.788835\n",
      "iteration 2877, current loss: 5006.664539\n",
      "iteration 2878, current loss: 5006.540261\n",
      "iteration 2879, current loss: 5006.416002\n",
      "iteration 2880, current loss: 5006.291761\n",
      "iteration 2881, current loss: 5006.167539\n",
      "iteration 2882, current loss: 5006.043335\n",
      "iteration 2883, current loss: 5005.919149\n",
      "iteration 2884, current loss: 5005.794981\n",
      "iteration 2885, current loss: 5005.670832\n",
      "iteration 2886, current loss: 5005.546700\n",
      "iteration 2887, current loss: 5005.422587\n",
      "iteration 2888, current loss: 5005.298492\n",
      "iteration 2889, current loss: 5005.174415\n",
      "iteration 2890, current loss: 5005.050355\n",
      "iteration 2891, current loss: 5004.926314\n",
      "iteration 2892, current loss: 5004.802290\n",
      "iteration 2893, current loss: 5004.678284\n",
      "iteration 2894, current loss: 5004.554295\n",
      "iteration 2895, current loss: 5004.430325\n",
      "iteration 2896, current loss: 5004.306371\n",
      "iteration 2897, current loss: 5004.182436\n",
      "iteration 2898, current loss: 5004.058517\n",
      "iteration 2899, current loss: 5003.934616\n",
      "iteration 2900, current loss: 5003.810733\n",
      "iteration 2901, current loss: 5003.686866\n",
      "iteration 2902, current loss: 5003.563017\n",
      "iteration 2903, current loss: 5003.439185\n",
      "iteration 2904, current loss: 5003.315370\n",
      "iteration 2905, current loss: 5003.191572\n",
      "iteration 2906, current loss: 5003.067791\n",
      "iteration 2907, current loss: 5002.944026\n",
      "iteration 2908, current loss: 5002.820279\n",
      "iteration 2909, current loss: 5002.696548\n",
      "iteration 2910, current loss: 5002.572834\n",
      "iteration 2911, current loss: 5002.449137\n",
      "iteration 2912, current loss: 5002.325456\n",
      "iteration 2913, current loss: 5002.201791\n",
      "iteration 2914, current loss: 5002.078143\n",
      "iteration 2915, current loss: 5001.954511\n",
      "iteration 2916, current loss: 5001.830896\n",
      "iteration 2917, current loss: 5001.707296\n",
      "iteration 2918, current loss: 5001.583713\n",
      "iteration 2919, current loss: 5001.460146\n",
      "iteration 2920, current loss: 5001.336595\n",
      "iteration 2921, current loss: 5001.213060\n",
      "iteration 2922, current loss: 5001.089540\n",
      "iteration 2923, current loss: 5000.966037\n",
      "iteration 2924, current loss: 5000.842549\n",
      "iteration 2925, current loss: 5000.719077\n",
      "iteration 2926, current loss: 5000.595620\n",
      "iteration 2927, current loss: 5000.472179\n",
      "iteration 2928, current loss: 5000.348753\n",
      "iteration 2929, current loss: 5000.225343\n",
      "iteration 2930, current loss: 5000.101947\n",
      "iteration 2931, current loss: 4999.978567\n",
      "iteration 2932, current loss: 4999.855203\n",
      "iteration 2933, current loss: 4999.731853\n",
      "iteration 2934, current loss: 4999.608518\n",
      "iteration 2935, current loss: 4999.485198\n",
      "iteration 2936, current loss: 4999.361893\n",
      "iteration 2937, current loss: 4999.238603\n",
      "iteration 2938, current loss: 4999.115327\n",
      "iteration 2939, current loss: 4998.992066\n",
      "iteration 2940, current loss: 4998.868820\n",
      "iteration 2941, current loss: 4998.745588\n",
      "iteration 2942, current loss: 4998.622370\n",
      "iteration 2943, current loss: 4998.499167\n",
      "iteration 2944, current loss: 4998.375978\n",
      "iteration 2945, current loss: 4998.252803\n",
      "iteration 2946, current loss: 4998.129642\n",
      "iteration 2947, current loss: 4998.006495\n",
      "iteration 2948, current loss: 4997.883362\n",
      "iteration 2949, current loss: 4997.760243\n",
      "iteration 2950, current loss: 4997.637138\n",
      "iteration 2951, current loss: 4997.514046\n",
      "iteration 2952, current loss: 4997.390968\n",
      "iteration 2953, current loss: 4997.267903\n",
      "iteration 2954, current loss: 4997.144852\n",
      "iteration 2955, current loss: 4997.021815\n",
      "iteration 2956, current loss: 4996.898790\n",
      "iteration 2957, current loss: 4996.775779\n",
      "iteration 2958, current loss: 4996.652781\n",
      "iteration 2959, current loss: 4996.529796\n",
      "iteration 2960, current loss: 4996.406824\n",
      "iteration 2961, current loss: 4996.283865\n",
      "iteration 2962, current loss: 4996.160918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2963, current loss: 4996.037985\n",
      "iteration 2964, current loss: 4995.915064\n",
      "iteration 2965, current loss: 4995.792155\n",
      "iteration 2966, current loss: 4995.669259\n",
      "iteration 2967, current loss: 4995.546376\n",
      "iteration 2968, current loss: 4995.423505\n",
      "iteration 2969, current loss: 4995.300646\n",
      "iteration 2970, current loss: 4995.177799\n",
      "iteration 2971, current loss: 4995.054964\n",
      "iteration 2972, current loss: 4994.932141\n",
      "iteration 2973, current loss: 4994.809331\n",
      "iteration 2974, current loss: 4994.686532\n",
      "iteration 2975, current loss: 4994.563744\n",
      "iteration 2976, current loss: 4994.440969\n",
      "iteration 2977, current loss: 4994.318205\n",
      "iteration 2978, current loss: 4994.195453\n",
      "iteration 2979, current loss: 4994.072711\n",
      "iteration 2980, current loss: 4993.949982\n",
      "iteration 2981, current loss: 4993.827263\n",
      "iteration 2982, current loss: 4993.704556\n",
      "iteration 2983, current loss: 4993.581860\n",
      "iteration 2984, current loss: 4993.459175\n",
      "iteration 2985, current loss: 4993.336501\n",
      "iteration 2986, current loss: 4993.213837\n",
      "iteration 2987, current loss: 4993.091185\n",
      "iteration 2988, current loss: 4992.968543\n",
      "iteration 2989, current loss: 4992.845911\n",
      "iteration 2990, current loss: 4992.723291\n",
      "iteration 2991, current loss: 4992.600680\n",
      "iteration 2992, current loss: 4992.478080\n",
      "iteration 2993, current loss: 4992.355491\n",
      "iteration 2994, current loss: 4992.232911\n",
      "iteration 2995, current loss: 4992.110341\n",
      "iteration 2996, current loss: 4991.987782\n",
      "iteration 2997, current loss: 4991.865232\n",
      "iteration 2998, current loss: 4991.742693\n",
      "iteration 2999, current loss: 4991.620163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1933ab40208>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(3000, model)\n",
    "costs = model['costs']\n",
    "plt.plot(costs[-3000:])\n",
    "#print(W[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1da3fa6ef98>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs[-3000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3, Inspect Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pred, real):\n",
    "    right = np.sum(pred==real)\n",
    "    acc = right/real.shape[1]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy: 0.965026\n",
      "test set accuracy: 0.921429\n"
     ]
    }
   ],
   "source": [
    "predict = model['predict']\n",
    "pred = predict(model, train_X)\n",
    "pred = Y_to_labels(pred)\n",
    "acc = get_accuracy(pred, train_labels)\n",
    "print(\"training set accuracy: %f\" % acc)\n",
    "pred = predict(model, test_X)\n",
    "pred = Y_to_labels(pred)\n",
    "acc = get_accuracy(pred, test_labels)\n",
    "print(\"test set accuracy: %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"W-four-layer\", *W)\n",
    "np.savez(\"b-four-layer\", *b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 28000)\n"
     ]
    }
   ],
   "source": [
    "test_X = np.genfromtxt(\"test.csv\", delimiter=',', skip_header=1)\n",
    "test_X = test_X.T\n",
    "test_X = test_X/255\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28000)\n"
     ]
    }
   ],
   "source": [
    "test_Y = predict(model, test_X)\n",
    "test_labels = Y_to_labels(test_Y)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.zeros((test_labels.shape[1], 2), dtype=int)\n",
    "for i in range(output.shape[0]):\n",
    "    output[i, 0] = i+1\n",
    "    output[i, 1] = test_labels[0, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"4layer-submission.csv\", output, fmt=\"%d\", delimiter=',', header='ImageId,Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19357d075c0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_images = X_to_images(test_X)\n",
    "plt.imshow(test_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
